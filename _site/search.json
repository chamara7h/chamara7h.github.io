[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harsha Halgamuwe Hewage",
    "section": "",
    "text": "I am a PhD student at the Data Lab for Social Good research group at Cardiff University, UK. My research focuses on improving forecast methodologies for family planning supply chains in developing countries, with the goal of enhancing their efficiency and accessibility. In addition to my PhD work, I serve as the Coordinator at the Data Lab for Social Good.\nI have also led the research network at Forecasting for Social Good (F4SG), an official section of the International Institute of Forecasters. As part of this role, I managed the Learning Labs workshop series, providing free training on forecasting methodologies using R and Python. I am passionate about leveraging data science and forecasting techniques to address global challenges.\nI collaborate closely with organizations such as USAID, the Ethiopian Pharmaceutical Supply Service, and the HISP Centre at the University of Oslo, translating research into operational tools that improve public health outcomes.\nExplore my full list of publications on Google Scholar.\n\n  Core Skills\n  \n     Time Series Forecasting\n     R\n     Python\n     Quarto\n     Data Visualization\n     Creative Designing\n  \n\n\n  \n     Download CV"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "PROJECTS",
    "section": "",
    "text": "Creative Gallery\n\n\nSomething is cooking\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Projects\n\n\nSomething is cooking\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Projects\n\n\nMy academic publications, along with their source code and data.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/first-post/index.html",
    "href": "projects/first-post/index.html",
    "title": "Journal Publication",
    "section": "",
    "text": "Our research focuses on judgmental forecast adjustments during pre-promotional (normal), promotional, and post-promotional periods through behavioral experiments. The findings highlight the importance of providing forecasters with structured support to make effective adjustments, particularly during post-promotional periods. Additionally, offering quantitative information can help mitigate harmful judgmental adjustments in these periods. All materials related to the laboratory experiments will soon be available on my GitHub repository."
  },
  {
    "objectID": "projects/research/index.html",
    "href": "projects/research/index.html",
    "title": "Publications",
    "section": "",
    "text": "Predicting House Prices with Machine Learning\n\n\n\n\n\n\nForecasting\n\n\nRetail\n\n\nJudgmental adjustments\n\n\nR\n\n\nShiny\n\n\n\nOur research focuses on judgmental forecast adjustments during pre-promotional (normal), promotional, and post-promotional periods through behavioral experiments. The findings highlight the importance of providing forecasters with structured support to make effective adjustments, particularly during post-promotional periods. Additionally, offering quantitative information can help mitigate harmful judgmental adjustments in these periods. All materials related to the laboratory experiments will soon be available on my GitHub repository. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation Using Clustering Techniques\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nClustering\n\n\nStatistical Modelling\n\n\n\nThis project focuses on segmenting customers into different groups based on their purchasing behavior and demographics. It uses clustering algorithms like K-means and hierarchical clustering to identify distinct customer segments. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Global CO2 Emissions\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nEnvironmental Science\n\n\n\nThis project involves creating visualizations to show trends in global CO2 emissions over time. It includes data extraction from public databases, data cleaning, and using visualization libraries to create interactive charts and graphs. \n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/ds_projects/first-post/index.html",
    "href": "projects/ds_projects/first-post/index.html",
    "title": "Journal Publication",
    "section": "",
    "text": "Our research focuses on judgmental forecast adjustments during pre-promotional (normal), promotional, and post-promotional periods through behavioral experiments. The findings highlight the importance of providing forecasters with structured support to make effective adjustments, particularly during post-promotional periods. Additionally, offering quantitative information can help mitigate harmful judgmental adjustments in these periods. All materials related to the laboratory experiments will soon be available on my GitHub repository."
  },
  {
    "objectID": "projects/ds_projects/index.html",
    "href": "projects/ds_projects/index.html",
    "title": "Publications",
    "section": "",
    "text": "Journal Publication\n\n\nForecast adjustments during post-promotional periods\n\n\n\nHarsha Chamara Hewage, H. Niles Perera, Shari De Baets\n\n\nJul 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\nPost description for second post\n\n\n\nAlicia\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThird Blog Post\n\n\nAn example post where the source document is a Jupyter Notebook\n\n\n\nAlicia\n\n\nMay 24, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/publications/first-post/index.html",
    "href": "projects/publications/first-post/index.html",
    "title": "Forecast adjustments during post-promotional periods",
    "section": "",
    "text": "Our research focuses on judgmental forecast adjustments during pre-promotional (normal), promotional, and post-promotional periods through behavioral experiments. The findings highlight the importance of providing forecasters with structured support to make effective adjustments, particularly during post-promotional periods. Additionally, offering quantitative information can help mitigate harmful judgmental adjustments in these periods. All materials related to the laboratory experiments will soon be available on my GitHub repository.\n\n\n\n Journal article\n\n\n GitHub repo"
  },
  {
    "objectID": "projects/publications/second-post/index.html",
    "href": "projects/publications/second-post/index.html",
    "title": "A hybrid predictive and prescriptive modelling framework for long-term mental healthcare workforce planning",
    "section": "",
    "text": "We introduce a novel framework that predicts long-term mental healthcare workforce needs using real NHS data. This framework captures the dynamics of both patient needs and nurse availability, enabling effective long-term planning. It also tests policies and identifies strategies to address future staff shortages, providing valuable insights for decision-makers to develop resilient mental healthcare workforce policies.\n\n\n\n Pre-print\n\n\n GitHub repo"
  },
  {
    "objectID": "projects/projects/index.html",
    "href": "projects/projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting House Prices with Machine Learning\n\n\n\n\n\n\nForecasting\n\n\nRetail\n\n\nJudgmental adjustments\n\n\nR\n\n\nShiny\n\n\n\nOur research focuses on judgmental forecast adjustments during pre-promotional (normal), promotional, and post-promotional periods through behavioral experiments. The findings highlight the importance of providing forecasters with structured support to make effective adjustments, particularly during post-promotional periods. Additionally, offering quantitative information can help mitigate harmful judgmental adjustments in these periods. All materials related to the laboratory experiments will soon be available on my GitHub repository. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation Using Clustering Techniques\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nClustering\n\n\nStatistical Modelling\n\n\n\nThis project focuses on segmenting customers into different groups based on their purchasing behavior and demographics. It uses clustering algorithms like K-means and hierarchical clustering to identify distinct customer segments. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Global CO2 Emissions\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nEnvironmental Science\n\n\n\nThis project involves creating visualizations to show trends in global CO2 emissions over time. It includes data extraction from public databases, data cleaning, and using visualization libraries to create interactive charts and graphs. \n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/publications/index.html",
    "href": "projects/publications/index.html",
    "title": "Research Projects",
    "section": "",
    "text": "A hybrid predictive and prescriptive modelling framework for long-term mental healthcare workforce planning\n\n\n\nWorkforce planning\n\n\nHealthcare\n\n\nMachine Learning\n\n\nSimulation\n\n\nR\n\n\nPython\n\n\n\nPre-print (under review)\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecast adjustments during post-promotional periods\n\n\n\nForecasting\n\n\nRetail\n\n\nJudgmental adjustments\n\n\nR\n\n\nShiny\n\n\n\nJournal Publication\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Home",
    "section": "",
    "text": "From Cases to Consumption: Evaluating Forecasting Model Portability for Public-Health Supply Chains\n\n\n\nForecasting\n\n\nInventory\n\n\nClimate Health\n\n\nDHIS2\n\n\nCHAP\n\n\n\nConference: LOM Section Annual Conference (LOMSAC) 2026, Cardiff Business School.\n\n\n\nHarsha Halgamuwe Hewage\n\n\nJan 15, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Cases to Consumption: Evaluating CHAP/DHIS2 Model Portability for Health Supply Chains\n\n\n\nForecasting\n\n\nInventory\n\n\nClimate Health\n\n\nDHIS2\n\n\nCHAP\n\n\n\nWebinar: DHIS2 Climate and Health Tools for Planning and Monitoring Immunization Programmes.\n\n\n\nHarsha Halgamuwe Hewage\n\n\nOct 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWHAT WAS LOST: Estimating censored demand in family planning supply chains with the Truncated Conformal Kalman Filter\n\n\n\nForecasting\n\n\nInventory\n\n\nLoss Sales\n\n\nR\n\n\nTobit Kalman Filter\n\n\nConformal Prediction\n\n\n\n5th Welsh Postgraduate Research Cluster Workshop in Economy, Enterprise and Productivity\n\n\n\nHarsha Halgamuwe Hewage\n\n\nSep 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWHAT WAS LOST: tracing unmet demand in contraceptive supply chains\n\n\n\nForecasting\n\n\nInventory\n\n\nLoss Sales\n\n\nR\n\n\nTobit Kalman Filter\n\n\nConformal Prediction\n\n\n\nEURO 2025 Leeds, UK\n\n\n\nHarsha Halgamuwe Hewage\n\n\nJun 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWHAT WAS LOST: tracing unmet demand in contraceptive supply chains\n\n\n\nForecasting\n\n\nInventory\n\n\nLoss Sales\n\n\nR\n\n\nTobit Kalman Filter\n\n\nConformal Prediction\n\n\n\nIIF UK Chapter: Quarterly Forecasting Forum\n\n\n\nHarsha Halgamuwe Hewage\n\n\nMay 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWHAT WAS LOST: tracing unmet demand in contraceptive supply chains\n\n\n\nForecasting\n\n\nInventory\n\n\nLoss Sales\n\n\nR\n\n\nTobit Kalman Filter\n\n\nConformal Prediction\n\n\n\nData Lab for Social Good Seminar\n\n\n\nHarsha Halgamuwe Hewage\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/third-post/index.html",
    "href": "blog/third-post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "The source for any page in your website could also be a Jupyter Notebook. This one is third-post/index.ipynb.\nHere’s an example I borrowed from the Seaborn docs:\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Plot the distribution of clarity ratings, conditional on carat\nsns.displot(\n    data=diamonds,\n    x=\"carat\", hue=\"cut\",\n    kind=\"kde\", height=4, aspect=1.5,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\",   \n)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 22, 2021\n\n\nFirst Post\n\n\nAlicia \n\n\n\n\nMay 23, 2021\n\n\nSecond Post\n\n\nAlicia \n\n\n\n\nMay 24, 2021\n\n\nThird Blog Post\n\n\nAlicia \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/first-post/index.html",
    "href": "blog/first-post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Sed risus ultricies tristique nulla aliquet. Neque volutpat ac tincidunt vitae semper quis lectus nulla.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "blog/second-post/index.html",
    "href": "blog/second-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "projects/publications/third-post/index.html",
    "href": "projects/publications/third-post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "The source for any page in your website could also be a Jupyter Notebook. This one is third-post/index.ipynb.\nHere’s an example I borrowed from the Seaborn docs:\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Plot the distribution of clarity ratings, conditional on carat\nsns.displot(\n    data=diamonds,\n    x=\"carat\", hue=\"cut\",\n    kind=\"kde\", height=4, aspect=1.5,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\",   \n)"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "CV",
    "section": "",
    "text": "{{&lt; embed-pdf \"resume_hasrha.pdf\" \"100%\" \"100%\" \"min-height:100vh;\" &gt;}}"
  },
  {
    "objectID": "phd/projects.html",
    "href": "phd/projects.html",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "",
    "text": "My research focuses on addressing inefficiencies in supply chain management for contraceptive products in developing countries. These inefficiencies often lead to shortages, limiting women’s reproductive autonomy and exacerbating societal challenges. By integrating probabilistic forecasting with inventory optimization, I aim to create a novel approach that accounts for uncertainties, poor data quality, and local demand variations. This interdisciplinary project involves collaboration with scholars from Cardiff Business School, the School of Computer Science & Informatics, and the United States Agency for International Development (USAID). The goal is to improve access to contraceptives, reduce unintended pregnancies, and prevent unsafe abortions, ultimately benefiting women, healthcare providers, governments, and donor organizations."
  },
  {
    "objectID": "phd/projects.html#supervision-team",
    "href": "phd/projects.html#supervision-team",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "Supervision team",
    "text": "Supervision team\n\nLead: Prof Bahman Rostami-Tabar, Data Lab for Social Good Research Group, Cardiff Business School, UK\nCo: Prof Aris Syntetos, Cardiff Business School, UK\nCo: Dr Federico Liberatore, School of Computer Science and Informatics, UK\nCollaborator: Glenn Milano, Global Health Supply Chain, Bureau for Global Health, United States Agency for International Development (USAID)"
  },
  {
    "objectID": "phd/projects.html#funder",
    "href": "phd/projects.html#funder",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "Funder",
    "text": "Funder\nWelsh Graduate School for the Social Sciences (WGSSS)"
  },
  {
    "objectID": "phd/index.html",
    "href": "phd/index.html",
    "title": "Home",
    "section": "",
    "text": "Forecasting for Health Supply Chains\n        My PhD research project is focused on improving demand forecasting for family planning to ensure contraceptives are always available when and where they are needed.\n    \n\n    \n        The Core Challenge: What Was Lost?\n        \n            In public health, \"zero demand\" doesn't mean \"zero need\"—it often just means the shelves were empty. When health facilities run out of stock, the true demand from the community goes unrecorded. This \"censored\" or \"lost\" demand data leads to flawed forecasts, which in turn causes chronic under-stocking and reinforces a cycle of supply failure. My research focuses on breaking this cycle.\n        \n    \n    \n    \n        Key Outputs & Publications\n        \n        \n            A Novel Hybrid Approach to Contraceptive Demand Forecasting\n            Published in International Journal of Production Research\n            \n                 Read Paper\n                 View Code\n            \n        \n\n        \n            Estimating censored demand in family planning supply chains: forecast accuracy, inventory implications, and public health outcomes\n            Preprint - Submitted for review\n            \n                 Read Paper\n                 View Code\n            \n        \n    \n\n    \n         Impact in Practice: EPSS Training\n        Research is most valuable when it's put into practice. As part of my PhD, I developed and delivered a two-day training workshop for practitioners at the Ethiopian Pharmaceutical Supply Service (EPSS), equipping them with modern forecasting skills using R and Python.\n        \n             Training Materials\n             Code & Labs\n        \n    \n    \n    \n        Supervision & Collaboration Team\n        \n            \n                \n                Prof. Bahman Rostami-Tabar\n                Lead Supervisor, Cardiff University\n            \n            \n                \n                Prof. Aris Syntetos\n                Co-supervisor, Cardiff University\n            \n            \n                \n                Dr. Federico Liberatore\n                Co-supervisor, Cardiff University\n            \n            \n                \n                Glenn Milano\n                Industry Collaborator, USAID\n            \n        \n    \n    \n    \n        Funding & Partners"
  },
  {
    "objectID": "phd/index.html#supervision-team",
    "href": "phd/index.html#supervision-team",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "Supervision team",
    "text": "Supervision team\n\nLead: Prof Bahman Rostami-Tabar, Data Lab for Social Good Research Group, Cardiff Business School, UK\nCo: Prof Aris Syntetos, Cardiff Business School, UK\nCo: Dr Federico Liberatore, School of Computer Science and Informatics, UK\nCollaborator: Glenn Milano, Global Health Supply Chain, Bureau for Global Health, United States Agency for International Development (USAID)"
  },
  {
    "objectID": "phd/index.html#funder",
    "href": "phd/index.html#funder",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "Funder",
    "text": "Funder\nWelsh Graduate School for the Social Sciences (WGSSS)"
  },
  {
    "objectID": "cv/phd.html",
    "href": "cv/phd.html",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "",
    "text": "My research focuses on addressing inefficiencies in supply chain management for contraceptive products in developing countries. These inefficiencies often lead to shortages, limiting women’s reproductive autonomy and exacerbating societal challenges. By integrating probabilistic forecasting with inventory optimization, I aim to create a novel approach that accounts for uncertainties, poor data quality, and local demand variations. This interdisciplinary project involves collaboration with scholars from Cardiff Business School, the School of Computer Science & Informatics, and the United States Agency for International Development (USAID). The goal is to improve access to contraceptives, reduce unintended pregnancies, and prevent unsafe abortions, ultimately benefiting women, healthcare providers, governments, and donor organizations."
  },
  {
    "objectID": "cv/phd.html#supervision-team",
    "href": "cv/phd.html#supervision-team",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "Supervision team",
    "text": "Supervision team\n\nLead: Prof Bahman Rostami-Tabar, Data Lab for Social Good Research Group, Cardiff Business School, UK\nCo: Prof Aris Syntetos, Cardiff Business School, UK\nCo: Dr Federico Liberatore, School of Computer Science and Informatics, UK\nCollaborator: Glenn Milano, Global Health Supply Chain, Bureau for Global Health, United States Agency for International Development (USAID)"
  },
  {
    "objectID": "cv/phd.html#funder",
    "href": "cv/phd.html#funder",
    "title": "Forecasting improvements for better reproductive health and family planning operations in global health supply chains",
    "section": "Funder",
    "text": "Funder\nWelsh Graduate School for the Social Sciences (WGSSS)"
  },
  {
    "objectID": "lab/forecasting/first-post/index.html",
    "href": "lab/forecasting/first-post/index.html",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain: An introduction to time series forecasting",
    "section": "",
    "text": "Slides\n\n\n GitHub repo\n\n\nContext\nAccurate demand forecasting is essential for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory management, and distribution. This course aims to equip pharmaceutical officers at healthcare site levels with the knowledge and tools to adopt modern forecasting methods using R and Python.\n\n\nTarget Audience\n\nPharmaceutical officers at healthcare site levels responsible for demand planning\n\nAnyone interested in forecasting in the context of contraceptive supply chains\n\n\n\nPrerequisites\n\nBasic knowledge of R and Python\n\nBasic understanding of statistics concepts\n\n\n\nLearning Outcomes\n\nDay 1:\n\nFamiliarize with RStudio and R Notebooks.\nInstall and load required packages in R.\nLearn data wrangling and feature engineering.\nUnderstand time series graphics.\nExplore forecasting models: sNAIVE, Moving Average, ARIMA, ETS, and Demographic Forecasting.\nEvaluate model performance using time series cross-validation.\n\n\n\nDay 2:\n\nFamiliarize with Google Colab and Python Notebooks.\nInstall and load required Python packages.\nImport data into Python.\nImplement advanced forecasting models: LightGBM, XGBoost, and TimeGPT.\nEvaluate model performance using time series cross-validation.\nExplore additional learning resources.\n\n\n\n\nPreparation\nThe workshop will provide a quick-start overview of exploring time series data and producing forecasts. No prior experience in time series is required. However, familiarity with:\n\nWriting R code and using tidyverse packages (dplyr, ggplot2) is recommended. Learn R here.\nWriting Python code is beneficial. Learn Python here.\nBasic statistical concepts such as mean, variance, quantiles, and regression would be helpful.\n\n\n\nRequired Equipment\nPlease have a laptop capable of running both R and Python."
  },
  {
    "objectID": "lab/forecasting/index.html",
    "href": "lab/forecasting/index.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Demand Forecasting Models for Contraceptive Supply Chain: An introduction to time series forecasting\n\n\n\nForecasting\n\n\nHealthcare\n\n\nR\n\n\nPython\n\n\nMachine Learning\n\n\n\nWorkshop\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lab/dashboards/index.html",
    "href": "lab/dashboards/index.html",
    "title": "Research Projects",
    "section": "",
    "text": "Data illustrations by Storyset\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lab/index.html",
    "href": "lab/index.html",
    "title": "Home",
    "section": "",
    "text": "Quarto Websites: Create and Publish Your First Website\n\n\n\nR\n\n\nQuarto\n\n\nGithub\n\n\n\nA hands-on workshop for researchers, students, and practitioners.\n\n\n\nHarsha Chamara Hewage\n\n\nNov 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Markov Processes in Healthcare Supply Chains\n\n\n\nStochastic modelling\n\n\nHealthcare\n\n\nR\n\n\nPython\n\n\n\nA hands-on workshop on stochastic modelling for logistical systems.\n\n\n\nHarsha Chamara Hewage\n\n\nMay 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethodological Skills for Research\n\n\n\nResearch\n\n\nMethodology\n\n\nPhilosophy\n\n\n\nLecture - University of Moratuwa Sri Lanka\n\n\n\nHarsha Halgamuwe Hewage\n\n\nApr 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDemand Forecasting Models for Contraceptive Supply Chain\n\n\n\nForecasting\n\n\nHealthcare\n\n\nR\n\n\nPython\n\n\nMachine Learning\n\n\n\nAn introduction to time series forecasting\n\n\n\nHarsha Chamara Hewage\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Probabilistic Forecasts with Energy Score\n\n\n\nForecasting\n\n\nScoring Rules\n\n\nR\n\n\n\nTutorial\n\n\n\nHarsha Chamara Hewage\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA basic introduction to Python\n\n\n\nPython\n\n\nJupyter Lab\n\n\nData analytics\n\n\n\nTutorial\n\n\n\nHarsha Chamara Hewage\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lab/forecasting/epss_training/index.html",
    "href": "lab/forecasting/epss_training/index.html",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain: An introduction to time series forecasting",
    "section": "",
    "text": "Slides\n\n\n GitHub repo\n\n\nContext\nAccurate demand forecasting is essential for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory management, and distribution. This course aims to equip pharmaceutical officers at healthcare site levels with the knowledge and tools to adopt modern forecasting methods using R and Python.\n\n\nTarget Audience\n\nPharmaceutical officers at healthcare site levels responsible for demand planning\n\nAnyone interested in forecasting in the context of contraceptive supply chains\n\n\n\nPrerequisites\n\nBasic knowledge of R and Python\n\nBasic understanding of statistics concepts\n\n\n\nLearning Outcomes\n\nDay 1:\n\nFamiliarize with RStudio and R Notebooks.\nInstall and load required packages in R.\nLearn data wrangling and feature engineering.\nUnderstand time series graphics.\nExplore forecasting models: sNAIVE, Moving Average, ARIMA, ETS, and Demographic Forecasting.\nEvaluate model performance.\n\n\n\nDay 2:\n\nFamiliarize with Google Colab and Python Notebooks.\nInstall and load required Python packages.\nImport data into Python.\nImplement advanced forecasting models: LightGBM, XGBoost, and TimeGPT.\nEvaluate model performance.\nExplore additional learning resources.\n\n\n\n\nPreparation\nThe workshop will provide a quick-start overview of exploring time series data and producing forecasts. No prior experience in time series is required. However, familiarity with:\n\nWriting R code and using tidyverse packages (dplyr, ggplot2) is recommended. Learn R here.\nWriting Python code is beneficial. Learn Python here.\nBasic statistical concepts such as mean, variance, quantiles, and regression would be helpful.\n\n\n\nRequired Equipment\nPlease have a laptop capable of running both R and Python."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html",
    "href": "lab/forecasting/epss_training/slides/epss_training.html",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "2025-03-17\n\n\n\n\n\n\n\n\n\n\n\nYou should be comfortable with R and Python coding.\nThis is not a theory-based course—we will not derive formulas or mathematical proofs.\nOur focus is on practical time series forecasting: understanding when and how to use different tools effectively.\n\n\n\n\n\nData wrangling and basic feature engineering\nTime series visualizations\nTraditional forecasting models\nAdvanced forecasting models\nPerformance evaluation\n\n\n\n\n\nHandling missing values\nAdvanced feature engineering\nTime series cross-validation\nHyperparameter tuning\n\n\n\n\nYou can find the workshop materials here.\nNote: These materials are based on A Novel Hybrid Approach to Contraceptive Demand Forecasting Research, F4SG: Africast training and F4SG Learning Labs trainings.\n\nRecommended readings:\n\nDemand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa.\nForecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos.\nForecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM) by Ivan Svetunkov.\n\n\n\n\n\nWhat is forecasting?\nPrepare your data\nExplore your data\nForecast modelling\nEvaluating the model performances\nAdvance forecasting models\nOther forecasting models in family planning supply chains"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#assumptions",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#assumptions",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "You should be comfortable with R and Python coding.\nThis is not a theory-based course—we will not derive formulas or mathematical proofs.\nOur focus is on practical time series forecasting: understanding when and how to use different tools effectively."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#what-we-will-cover",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#what-we-will-cover",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "Data wrangling and basic feature engineering\nTime series visualizations\nTraditional forecasting models\nAdvanced forecasting models\nPerformance evaluation"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#what-we-will-not-cover",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#what-we-will-not-cover",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "Handling missing values\nAdvanced feature engineering\nTime series cross-validation\nHyperparameter tuning"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#materials",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#materials",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "You can find the workshop materials here.\nNote: These materials are based on A Novel Hybrid Approach to Contraceptive Demand Forecasting Research, F4SG: Africast training and F4SG Learning Labs trainings.\n\nRecommended readings:\n\nDemand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa.\nForecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos.\nForecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM) by Ivan Svetunkov."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#outline",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#outline",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "What is forecasting?\nPrepare your data\nExplore your data\nForecast modelling\nEvaluating the model performances\nAdvance forecasting models\nOther forecasting models in family planning supply chains"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#what-is-a-forecast",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#what-is-a-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What is a FORECAST?",
    "text": "What is a FORECAST?\nAn estimation of the future based on all of the information available at the time when we generate the forecast;\n\nhistorical data,\nknowledge of any future events that might impact the forecasts."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#what-is-time-series-data",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#what-is-time-series-data",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What is time series data?",
    "text": "What is time series data?\n\nTime series consist of sequences of observations collected over time.\nTime series forecasting is estimating how the sequence of observations will continue into the future."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#what-to-forecast",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#what-to-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What to FORECAST?",
    "text": "What to FORECAST?\nUnderstanding needs! Identify decisions that need forecasting support!\n\nForecast variable/s\nTime granularity\nForecast horizon\nFrequency\nStructure/hierarchy"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#forecasting-workflow",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#forecasting-workflow",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Forecasting workflow",
    "text": "Forecasting workflow\n\nStep 1: Problem definition\nStep 2: Gathering information\nStep 3: Preliminary (exploratory) analysis\nStep 4: Choosing and fitting models\nStep 5: Evaluating and using a forecasting model"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#tidy-forecasting-workflow",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#tidy-forecasting-workflow",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Tidy forecasting workflow",
    "text": "Tidy forecasting workflow\n\n\n\n\n\nRead more at Demand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#loading-libraries",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#loading-libraries",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Loading libraries",
    "text": "Loading libraries\nWe use the fpp3 package in this workshop, which provides all the necessary packages for data manipulation, plotting, and forecasting.\n\n# Define required packages\npackages &lt;- c(\"tidyverse\", \"fable\", \"tsibble\", \"feasts\", 'zoo')\n\n# Install missing packages\nmissing_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(missing_packages)) {\n  suppressWarnings(suppressMessages(install.packages(missing_packages)))\n}\n\n# Load libraries quietly\nsuppressWarnings(suppressMessages({\n  library(tidyverse) # Data manipulation and plotting functions\n  library(fable) # Time series manipulation\n  library(tsibble) # Forecasting functions\n  library(feasts) # Time series graphics and statistics\n}))\n\nRead more at Forecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#preparing-the-data",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#preparing-the-data",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Preparing the data",
    "text": "Preparing the data\nIn this workshop, we are using tsibble objects. They provide a data infrastructure for tidy temporal data with wrangling tools, adapting the tidy data principles.\nIn tsibble:\n\nIndex: time information about the observation\nMeasured variable(s): numbers of interest\nKey variable(s): set of variables that define observational units over time\nIt works with tidyverse functions."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#read-csv-file",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#read-csv-file",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Read csv file",
    "text": "Read csv file\n\nmed_qty &lt;- read.csv('data/med_qty.csv')\nmed_qty |&gt; head(10)\n\n       date hub_id product_id quantity_issued\n1  2017 Jul  hub_4  product_1              60\n2  2017 Jul  hub_4  product_6            5200\n3  2017 Jul  hub_7  product_1               8\n4  2017 Jul  hub_7  product_5             120\n5  2017 Jul  hub_8  product_7              10\n6  2017 Jul hub_10  product_1             343\n7  2017 Jul hub_10  product_2              53\n8  2017 Jul hub_10  product_3              26\n9  2017 Jul hub_10  product_4            1710\n10 2017 Jul hub_10  product_5            1340\n\n\nDo you think the med_qty data set is a tidy data?"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#check-for-na-and-duplicates",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#check-for-na-and-duplicates",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check for NA and duplicates",
    "text": "Check for NA and duplicates\n\n# check NAs\n\nanyNA(med_qty)\n\n[1] FALSE\n\n\n\n#check duplicates\n\nmed_qty |&gt;  \n  duplicated() |&gt;  \n  sum() \n\n[1] 0"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#create-tsibble",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#create-tsibble",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Create tsibble",
    "text": "Create tsibble\n\nmed_tsb &lt;- med_qty |&gt;  \n  mutate(date = yearmonth(date)) |&gt;  # convert chr to date format\n  as_tsibble(index = date, key = c(hub_id, product_id))\n\nmed_tsb \n\n# A tsibble: 6,745 x 4 [1M]\n# Key:       hub_id, product_id [133]\n       date hub_id product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Aug hub_1  product_1              721\n 2 2017 Sep hub_1  product_1              795\n 3 2017 Oct hub_1  product_1             1720\n 4 2017 Nov hub_1  product_1              911\n 5 2017 Dec hub_1  product_1              314\n 6 2018 Jan hub_1  product_1             6913\n 7 2018 Feb hub_1  product_1             2988\n 8 2018 Mar hub_1  product_1             7120\n 9 2018 Apr hub_1  product_1             3122\n10 2018 May hub_1  product_1            11737\n# ℹ 6,735 more rows\n\n\n\nWhat is the temporal granularity of med_tsb?\nHow many time series do we have in med_tsb?"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check temporal gaps (implicit missing values)",
    "text": "Check temporal gaps (implicit missing values)\n\nhas_gaps(med_tsb) |&gt; head(3) #check gaps\n\n# A tibble: 3 × 3\n  hub_id product_id .gaps\n  &lt;chr&gt;  &lt;chr&gt;      &lt;lgl&gt;\n1 hub_1  product_1  TRUE \n2 hub_1  product_2  TRUE \n3 hub_1  product_3  TRUE \n\nscan_gaps(med_tsb) |&gt; head(3) # show gaps\n\n# A tsibble: 3 x 3 [1M]\n# Key:       hub_id, product_id [1]\n  hub_id product_id     date\n  &lt;chr&gt;  &lt;chr&gt;         &lt;mth&gt;\n1 hub_1  product_1  2018 Jul\n2 hub_1  product_1  2018 Aug\n3 hub_1  product_1  2018 Sep\n\ncount_gaps(med_tsb) |&gt; head(3) # count gaps\n\n# A tibble: 3 × 5\n  hub_id product_id    .from      .to    .n\n  &lt;chr&gt;  &lt;chr&gt;         &lt;mth&gt;    &lt;mth&gt; &lt;int&gt;\n1 hub_1  product_1  2018 Jul 2021 Feb    32\n2 hub_1  product_1  2021 Jul 2022 Sep    15\n3 hub_1  product_2  2018 Sep 2018 Sep     1"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check temporal gaps (implicit missing values)",
    "text": "Check temporal gaps (implicit missing values)\nIf there is any gap, then we fill it.\n\nmed_tsb |&gt; fill_gaps(quantity_issued=0L) # we can fill it with zero\n\n# A tsibble: 8,795 x 4 [1M]\n# Key:       hub_id, product_id [133]\n       date hub_id product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Aug hub_1  product_1              721\n 2 2017 Sep hub_1  product_1              795\n 3 2017 Oct hub_1  product_1             1720\n 4 2017 Nov hub_1  product_1              911\n 5 2017 Dec hub_1  product_1              314\n 6 2018 Jan hub_1  product_1             6913\n 7 2018 Feb hub_1  product_1             2988\n 8 2018 Mar hub_1  product_1             7120\n 9 2018 Apr hub_1  product_1             3122\n10 2018 May hub_1  product_1            11737\n# ℹ 8,785 more rows"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check temporal gaps (implicit missing values)",
    "text": "Check temporal gaps (implicit missing values)\nNote: Since the main focus of this study is to provide foundational knowledge on forecasting, we will filter out time series with many missing values and then fill the remaining gaps using na.interp() function (Read more).\n\nitem_ids &lt;- med_tsb |&gt; \n  count_gaps() |&gt; \n  group_by(hub_id, product_id) |&gt; \n  summarise(.n = max(.n), .groups = 'drop') |&gt; \n  filter(.n  &gt; 1) |&gt; \n  mutate(id = paste0(hub_id,'-',product_id)) |&gt; \n  pull(id) # filtering the item ids\n\nmed_tsb_filter &lt;- med_tsb |&gt; \n  mutate(id = paste0(hub_id,'-',product_id)) |&gt; \n  group_by(hub_id, product_id) |&gt;\n  mutate(num_observations = n()) |&gt; \n  filter(!id %in% item_ids & num_observations &gt;59) |&gt;   # we have cold starts and discontinuations. \n  fill_gaps(quantity_issued = 1e-6, .full = TRUE) |&gt;   # Replace NAs with a small value\n  select(-id, -num_observations) |&gt; \n  mutate(quantity_issued = if_else(is.na(quantity_issued), \n                                   exp(\n                                     forecast::na.interp(\n                                     ts(log(quantity_issued), frequency = 12))), \n                                   quantity_issued))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use the filter() function to select rows.\n\nmed_tsb |&gt; \n  filter(hub_id == 'hub_10') \n\n# A tsibble: 417 x 4 [1M]\n# Key:       hub_id, product_id [7]\n       date hub_id product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Jul hub_10 product_1              343\n 2 2017 Aug hub_10 product_1               67\n 3 2017 Sep hub_10 product_1              127\n 4 2017 Oct hub_10 product_1              287\n 5 2017 Nov hub_10 product_1              759\n 6 2017 Dec hub_10 product_1              181\n 7 2018 Jan hub_10 product_1             7015\n 8 2018 Feb hub_10 product_1              840\n 9 2018 Mar hub_10 product_1             4111\n10 2018 Apr hub_10 product_1             1910\n# ℹ 407 more rows"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use the select() function to select columns.\n\nmed_tsb |&gt; \n  filter(hub_id == 'hub_10') |&gt; \n  select(date, product_id, quantity_issued)\n\n# A tsibble: 417 x 3 [1M]\n# Key:       product_id [7]\n       date product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Jul product_1              343\n 2 2017 Aug product_1               67\n 3 2017 Sep product_1              127\n 4 2017 Oct product_1              287\n 5 2017 Nov product_1              759\n 6 2017 Dec product_1              181\n 7 2018 Jan product_1             7015\n 8 2018 Feb product_1              840\n 9 2018 Mar product_1             4111\n10 2018 Apr product_1             1910\n# ℹ 407 more rows"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use group_by() function to group over keys. We can use the summarise() function to summarise over keys.\n\nmed_tsb |&gt; \n  group_by(product_id) |&gt; \n  summarise(total_quantity_issued = sum(quantity_issued), .groups = 'drop')\n\n# A tsibble: 471 x 3 [1M]\n# Key:       product_id [7]\n   product_id     date total_quantity_issued\n   &lt;chr&gt;         &lt;mth&gt;                 &lt;dbl&gt;\n 1 product_1  2017 Jul                   691\n 2 product_1  2017 Aug                 18855\n 3 product_1  2017 Sep                 21654\n 4 product_1  2017 Oct                 16456\n 5 product_1  2017 Nov                 19694\n 6 product_1  2017 Dec                 63107\n 7 product_1  2018 Jan                 66703\n 8 product_1  2018 Feb                 53012\n 9 product_1  2018 Mar                 82566\n10 product_1  2018 Apr                 56913\n# ℹ 461 more rows"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use the mutate() function to create new variables.\n\nmed_tsb |&gt; \n  mutate(quarter = yearquarter(date))\n\n# A tsibble: 6,745 x 5 [1M]\n# Key:       hub_id, product_id [133]\n       date hub_id product_id quantity_issued quarter\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;   &lt;qtr&gt;\n 1 2017 Aug hub_1  product_1              721 2017 Q3\n 2 2017 Sep hub_1  product_1              795 2017 Q3\n 3 2017 Oct hub_1  product_1             1720 2017 Q4\n 4 2017 Nov hub_1  product_1              911 2017 Q4\n 5 2017 Dec hub_1  product_1              314 2017 Q4\n 6 2018 Jan hub_1  product_1             6913 2018 Q1\n 7 2018 Feb hub_1  product_1             2988 2018 Q1\n 8 2018 Mar hub_1  product_1             7120 2018 Q1\n 9 2018 Apr hub_1  product_1             3122 2018 Q2\n10 2018 May hub_1  product_1            11737 2018 Q2\n# ℹ 6,735 more rows"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-4",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use index_by() function to group over index We can use the summarise() function to summarise over index.\n\nmed_tsb |&gt; \n  mutate(quarter = yearquarter(date)) |&gt; \n  index_by(quarter) |&gt; \n  summarise(total_quantity_issues = sum(quantity_issued))\n\n# A tsibble: 24 x 2 [1Q]\n   quarter total_quantity_issues\n     &lt;qtr&gt;                 &lt;dbl&gt;\n 1 2017 Q3               2103843\n 2 2017 Q4               2811202\n 3 2018 Q1               2511488\n 4 2018 Q2               3433726\n 5 2018 Q3               1738860\n 6 2018 Q4               2934886\n 7 2019 Q1               2452192\n 8 2019 Q2               1640048\n 9 2019 Q3               2170015\n10 2019 Q4               3045525\n# ℹ 14 more rows"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#time-series-patterns",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#time-series-patterns",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Time series patterns",
    "text": "Time series patterns\n\nLevel: The level of a time series describes the center of the series.\nTrend: A trend describes predictable increases or decreases in the level of a series.\nSeasonal: Seasonality is a consistent pattern that repeats over a fixed cycle. pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\nCyclic: A pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years)."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#time-series-patterns-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#time-series-patterns-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Time series patterns",
    "text": "Time series patterns\n\n\n\n\n\n\n\n\n\nRead more at Demand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#additive-vs.-multiplicative-seasonality",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#additive-vs.-multiplicative-seasonality",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Additive vs. multiplicative seasonality",
    "text": "Additive vs. multiplicative seasonality\n\n\n\n\n\n\n\n\n\n\nWhen we have multiplicative seasonality, we can use transformations to convert multiplicative seasonality into additive seasonality.\nIn this training, we are not discussing time series transformations. You can read more about it at Transformations and adjustments."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#time-plots",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#time-plots",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Time plots",
    "text": "Time plots\nYou can create time plot using autoplot() function.\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_2') |&gt; \n  autoplot(quantity_issued) +\n  labs(\n    x = \"Date\",\n    y = \"Quantity Issued\"\n  ) +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#are-time-plots-best",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#are-time-plots-best",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Are time plots best?",
    "text": "Are time plots best?\n\nmed_tsb_filter |&gt; \n  mutate(id = paste0(hub_id, product_id)) |&gt; \n  ggplot(aes(x = date, y = quantity_issued, group = id)) +\n  geom_line() +\n  labs(\n    x = \"Date\",\n    y = \"Quantity Issued\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-plots",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-plots",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal plots",
    "text": "Seasonal plots\n\nData plotted against the individual seasons in which the data were observed (In this case a “season” is a month).\nEnables the underlying seasonal pattern to be seen more clearly, and also allows any substantial departures from the seasonal pattern to be easily identified.\nYou can create seasonal plots using gg_season() function."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-plots-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-plots-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal plots",
    "text": "Seasonal plots\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  gg_season(quantity_issued, labels = \"both\") +\n  ylab(\"Quantity issued\") +\n  ggtitle(\"Seasonal plot\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-sub-series-plots",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-sub-series-plots",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal sub series plots",
    "text": "Seasonal sub series plots\n\nData for each season collected together in time plot as separate time series.\nEnables the underlying seasonal pattern to be seen clearly, and changes in seasonality over time to be visualized.\nYou can create seasonal sub series plots using gg_subseries() function."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-sub-series-plots-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-sub-series-plots-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal sub series plots",
    "text": "Seasonal sub series plots\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  gg_subseries(quantity_issued) +\n  ylab(\"Quantity issued\") +\n  ggtitle(\"Seasonal sub series plot\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Strength of seasonality and trend",
    "text": "Strength of seasonality and trend\n\nWe used STL decomposition for additive decompositions.\nA multiplicative decomposition can be obtained by first taking logs of the data, then back-transforming the components.\nDecompositions that are between additive and multiplicative can be obtained using a Box-Cox transformation of the data.\nRead more at STL decomposition."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Strength of seasonality and trend",
    "text": "Strength of seasonality and trend\nSTL Decomposition\n\\[\ny_t = T_t + S_t + R_t\n\\]\nSeasonal Strength\n\\[\n\\max \\left( 0, 1 - \\frac{\\text{Var}(R_t)}{\\text{Var}(S_t + R_t)} \\right)\n\\]\nTrend Strength\n\\[\n\\max \\left( 0, 1 - \\frac{\\text{Var}(R_t)}{\\text{Var}(T_t + R_t)} \\right)\n\\]"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#feature-extraction-and-statistics",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#feature-extraction-and-statistics",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature extraction and statistics",
    "text": "Feature extraction and statistics\nWe can use features() function to extract the strength of trend and seasonality.\n\nmed_tsb_filter |&gt; \n  features(quantity_issued, feat_stl)\n\n# A tibble: 21 × 11\n   hub_id product_id trend_strength seasonal_strength_year seasonal_peak_year\n   &lt;chr&gt;  &lt;chr&gt;               &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 hub_1  product_2           0.261                 0.503                   6\n 2 hub_1  product_5           0.250                 0.438                   0\n 3 hub_10 product_5           0.366                 0.0911                  0\n 4 hub_11 product_2           0.624                 0.407                  11\n 5 hub_11 product_5           0.352                 0.244                   4\n 6 hub_11 product_7           0.181                 0.196                   7\n 7 hub_13 product_5           0.196                 0.402                   0\n 8 hub_14 product_5           0.595                 0.229                   9\n 9 hub_16 product_2           0.233                 0.238                   7\n10 hub_16 product_5           0.416                 0.272                   0\n# ℹ 11 more rows\n# ℹ 6 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#feature-extraction-and-statistics-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#feature-extraction-and-statistics-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature extraction and statistics",
    "text": "Feature extraction and statistics\n\nmed_tsb_filter |&gt; \n  features(quantity_issued, feat_stl) |&gt; \n  ggplot(aes(x = trend_strength, y = seasonal_strength_year, shape = product_id)) +\n  geom_point(size = 2) + \n  ylab(\"Seasonal strength\") +\n  xlab(\"Trend strength\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Lag plots and autocorrelation",
    "text": "Lag plots and autocorrelation\n\nEach graph shows \\(y_t\\) plotted against \\(y_{t-k}\\) for different values of \\(k\\).\n\nThe autocorrelations are the correlations associated with these scatterplots: \\(\\text{Corr}(y_t, y_{t-k})\\)\nYou can create lag plots using gglag() function.\nThese values indicate the relationship between current and past observations in a time series."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Lag plots and autocorrelation",
    "text": "Lag plots and autocorrelation\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt;\n  gg_lag(quantity_issued, lags = 1:12, geom='point') +\n  ylab(\"Quantity issued\") +\n  xlab(\"Lag (Quantity issued, n)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n         panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocovariance and autocorrelation: measure linear relationship between lagged values of a time series y.\nWe denote the sample autocovariance at lag \\(k\\) by \\(c_k\\) and the sample autocorrelation at lag \\(k\\) by \\(r_k\\). Then, we define:\n\n\\(c_k = \\frac{1}{T} \\sum_{t=k+1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y})\\)\n\\(r_k = \\frac{c_k}{c_0}\\)\nwhere \\(c_0\\) is the variance of the time series.\n\n\\(r_1\\) indicates how successive values of \\(y\\) relate to each other.\n\\(r_2\\) indicates how \\(y\\) values two periods apart relate to each other.\n\\(r_k\\) is almost the same as the sample correlation between \\(y_t\\) and \\(y_{t-k}\\)."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  ACF(quantity_issued, lag_max = 24)\n\n# A tsibble: 24 x 4 [1M]\n# Key:       hub_id, product_id [1]\n   hub_id product_id      lag      acf\n   &lt;chr&gt;  &lt;chr&gt;      &lt;cf_lag&gt;    &lt;dbl&gt;\n 1 hub_14 product_5        1M  0.681  \n 2 hub_14 product_5        2M  0.485  \n 3 hub_14 product_5        3M  0.320  \n 4 hub_14 product_5        4M  0.160  \n 5 hub_14 product_5        5M  0.195  \n 6 hub_14 product_5        6M  0.162  \n 7 hub_14 product_5        7M  0.0956 \n 8 hub_14 product_5        8M  0.0540 \n 9 hub_14 product_5        9M  0.00739\n10 hub_14 product_5       10M -0.0665 \n# ℹ 14 more rows"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  ACF(quantity_issued, lag_max = 36) |&gt; \n  autoplot() +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))\n\n\n\n\n\n\n\n\nWhat autocorrelation will tell us? Which key features could be highlighted by ACF?"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#autocorrelation-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nWhen data have a trend, the autocorrelations for small lags tend to be large and positive.\nWhen data are seasonal, the autocorrelations will be larger at the seasonal lags (i.e., at multiples of the seasonal frequency)\nWhen data are trended and seasonal, you see a combination of these effects."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#naive",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#naive",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Naive",
    "text": "Naive\nSimplest forecasting method using last observation as forecast.\n\\(\\hat{y}_{t+h|t} = y_t\\)\nAssumptions\n\nNo systematic pattern in data\nRecent observations are most relevant\n\nStrengths & Weaknesses\n✓ Simple benchmark model\n✓ Requires no computation\n✗ Ignores all patterns\n✗ Poor for trending/seasonal data"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#naive-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#naive-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Naive",
    "text": "Naive\nWe use NAIVE() function and model() function to build the Naive model.\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(naive = NAIVE(quantity_issued)) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-naive-snaive",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-naive-snaive",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal NAIVE (sNAIVE)",
    "text": "Seasonal NAIVE (sNAIVE)\n\\(y_{t+h \\mid t} = y_{t+h - m(k+1)}\\)\nWhere: \\(m\\) = seasonal period and \\(k = \\lfloor \\frac{h-1}{m} \\rfloor\\)\nAssumptions\n\nSeasonal pattern is stable\nNo trend present\n\nStrengths & Weaknesses\n✓ Handles strong seasonality\n✓ Simple interpretation\n✗ Fails with changing seasonality\n✗ Ignores non-seasonal patterns"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#snaive",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#snaive",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "sNaive",
    "text": "sNaive\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#mean",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#mean",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Mean",
    "text": "Mean\nUses the historical average of all observations as forecast.\n\\(y_{t+h \\mid t} = \\bar{y} = \\frac{1}{t} \\sum_{i=1}^{t} y_i\\)\nWhere: \\(t\\) is the number of past observations used for the forecast.\nAssumptions\n\nSeries is stationary\nShort-term fluctuations are noise\n\nStrengths & Weaknesses\n✓ Effective noise reduction\n✓ Simple to implement\n✗ Ignores all patterns\n✗ Lags behind trends"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#mean-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#mean-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Mean",
    "text": "Mean\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(MEAN(quantity_issued ~ window(size = 3))) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#arima",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#arima",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA",
    "text": "ARIMA\nCombines Autoregressive (AR) and Moving Average (MA) components with differencing.\n\nAR: autoregressive (lagged observations as inputs)\nI: integrated (differencing to make series stationary)\nMA: moving average (lagged errors as inputs)\n\n\nThe ARIMA model is given by:\n\\((1 - \\phi_1 B - \\dots - \\phi_p B^p)(1 - B)^d y_t = c + (1 + \\theta_1 B + \\dots + \\theta_q B^q) \\epsilon_t\\)\nWhere: \\(B\\): Backshift operator, \\(\\phi\\): AR coefficients, \\(\\theta\\): MA coefficients, \\(d\\): Differencing order, \\(p\\): AR order, \\(q\\): MA order and \\(\\epsilon_t\\): White noise"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#arima-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#arima-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA",
    "text": "ARIMA\nAssumptions\n\nSeries is stationary\nLinear relationship between past values and errors\nWhite noise errors\nNo missing values in series\n\nStrengths & Weaknesses\n✓ Flexible for various time series patterns\n✓ Perform well for short term horizons\n✗ Requires stationarity for optimal performance\n✗ The parameters are often not easily interpretable in terms of trend or seasonality"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#arima-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#arima-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA",
    "text": "ARIMA\nA stationary series is:\n\nroughly horizontal\n\nconstant variance\n\nno patterns predictable in the long-term"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-arima-models",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#seasonal-arima-models",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal ARIMA models",
    "text": "Seasonal ARIMA models\n\n\n\nARIMA\n\\(~\\underbrace{(p, d, q)}\\)\n\\(\\underbrace{(P, D, Q)_{m}}\\)\n\n\n\n\n\n\\({\\uparrow}\\)\n\\({\\uparrow}\\)\n\n\n\nNon-seasonal part\nSeasonal part of\n\n\n\nof the model\nof the model\n\n\n\n\n\\(m\\): number of observations per year.\n\\(d\\): first differences, \\(D\\): seasonal differences\n\\(p\\): AR lags, \\(q\\): MA lags\n\\(P\\): seasonal AR lags, \\(Q\\): seasonal MA lags\n\nSeasonal and non-seasonal terms combine multiplicatively."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#arima-automatic-modelling",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#arima-automatic-modelling",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA automatic modelling",
    "text": "ARIMA automatic modelling\n\nPlot the data. Identify any unusual observations.\nIf necessary, transform the data (e.g., Box-Cox transformation) to stabilize the variance.\nUse ARIMA() to automatically select a model.\nCheck the residuals from your chosen model and if they do not look like white noise, try a modified model.\nOnce the residuals look like white noise, calculate forecasts."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#arima-automatic-modelling-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#arima-automatic-modelling-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA automatic modelling",
    "text": "ARIMA automatic modelling\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(ARIMA(quantity_issued)) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#ets",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#ets",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nETS stands for Exponential Smoothing and is based on a state space framework that decomposes a time series into three components:\n\n\n\n\n\n\n\n\n\nGeneral Notation\n\nE T S\nExponenTial Smoothing\n\n\n\n\n\n↗\n↑\n↖\n\n\n\nError\nTrend\nSeason\n\n\n\n\nError: Additive (\"A\") or multiplicative (\"M\")\nTrend: None (\"N\"), additive (\"A\"), multiplicative (\"M\"), or damped (\"Ad\" or \"Md\").\nSeasonality: None (\"N\"), additive (\"A\") or multiplicative (\"M\")\n\n For example, ETS(A,N,N) is the simple exponential smoothing model (no trend or seasonality) with additive errors."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#ets-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#ets-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nHow do we combine these elements?\nAdditively?\n\\(y_t = \\ell_{t-1} + b_{t-1} + s_{t-m} + \\varepsilon_t\\)\n\nMultiplicatively?\n\\(y_t = \\ell_{t-1}b_{t-1}s_{t-m}(1 + \\varepsilon_t)\\)\n\nPerhaps a mix of both?\n\\(y_t = (\\ell_{t-1} + b_{t-1}) s_{t-m} + \\varepsilon_t\\)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#ets-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#ets-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nHow do the level, trend and seasonal components evolve over time?"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#ets-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#ets-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nAssumptions\n\nDecomposable patterns\nRecent observations more important\nConsistent error structure (additive/multiplicative)\n\nStrengths & Weaknesses\n✓ They can be adapted to various data characteristics with different error, trend, and seasonal formulations\n✓ Often very effective when the underlying components are stable\n✗ Parameter estimates (including smoothing parameters and initial states) can affect the forecasts\n✗ May struggle to capture sudden shifts or non-standard patterns if the smoothing parameters are constant"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#ets-automatic-modelling",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#ets-automatic-modelling",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS automatic modelling",
    "text": "ETS automatic modelling\n\nApply each model that is appropriate to the data.\nOptimize parameters and initial values using MLE (or some other criterion).\nSelect best method using AICc.\nUse ETS() to automatically select a model.\nProduce forecasts using best method."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#ets-automatic-modelling-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#ets-automatic-modelling-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS automatic modelling",
    "text": "ETS automatic modelling\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(ETS(quantity_issued)) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#model-fitting-in-fable",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#model-fitting-in-fable",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Model fitting in Fable",
    "text": "Model fitting in Fable\n\nThe model() function trains models on data.\nIt returns a mable object.\nA mable is a model table, each cell corresponds to a fitted model.\n\n\n# Fit the models\n\nfit_all &lt;- med_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(\n    naive = NAIVE(quantity_issued),\n    snaive = SNAIVE(quantity_issued ~ lag('year')),\n    mean = MEAN(quantity_issued ~ window(size = 3)),\n    arima = ARIMA(quantity_issued),\n    ets = ETS(quantity_issued)\n    )\n\nfit_all\n\n# A mable: 1 x 7\n# Key:     hub_id, product_id [1]\n  hub_id product_id   naive   snaive    mean                     arima\n  &lt;chr&gt;  &lt;chr&gt;      &lt;model&gt;  &lt;model&gt; &lt;model&gt;                   &lt;model&gt;\n1 hub_1  product_5  &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;MEAN&gt; &lt;ARIMA(0,1,1)(0,0,2)[12]&gt;\n# ℹ 1 more variable: ets &lt;model&gt;"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#extract-information-from-mable",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#extract-information-from-mable",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Extract information from mable",
    "text": "Extract information from mable\n\nfit_all  |&gt;  select(snaive) |&gt;  report()\nfit_all |&gt;  tidy()\nfit_all  |&gt;  glance()\n\n\nThe report() function gives a formatted model-specific display.\nThe tidy() function is used to extract the coefficients from the models.\nWe can extract information about some specific model using the filter() and select()functions."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#producing-forecasts",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#producing-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Producing forecasts",
    "text": "Producing forecasts\n\nThe forecast() function is used to produce forecasts from estimated models.\nh can be specified with:\n\na number (the number of future observations)\nnatural language (the length of time to predict)\nprovide a dataset of future time periods"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#producing-forecasts-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#producing-forecasts-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Producing forecasts",
    "text": "Producing forecasts\n\nfit_all_fc &lt;- fit_all |&gt; \n  forecast(h = 'year')\n\n#h = \"year\" is equivalent to setting h = 12.\n\nfit_all_fc\n\n# A fable: 60 x 6 [1M]\n# Key:     hub_id, product_id, .model [5]\n   hub_id product_id .model     date\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;mth&gt;\n 1 hub_1  product_5  naive  2023 Jul\n 2 hub_1  product_5  naive  2023 Aug\n 3 hub_1  product_5  naive  2023 Sep\n 4 hub_1  product_5  naive  2023 Oct\n 5 hub_1  product_5  naive  2023 Nov\n 6 hub_1  product_5  naive  2023 Dec\n 7 hub_1  product_5  naive  2024 Jan\n 8 hub_1  product_5  naive  2024 Feb\n 9 hub_1  product_5  naive  2024 Mar\n10 hub_1  product_5  naive  2024 Apr\n# ℹ 50 more rows\n# ℹ 2 more variables: quantity_issued &lt;dist&gt;, .mean &lt;dbl&gt;"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#visualising-forecasts",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#visualising-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Visualising forecasts",
    "text": "Visualising forecasts\n\nfit_all_fc |&gt; \n  autoplot(level = NULL) +\n  autolayer(med_tsb_filter |&gt; \n              filter_index(\"2022 JAn\" ~ .) |&gt; \n              filter(hub_id == 'hub_1' & product_id == 'product_5'), color = 'black') +\n  labs(title = \"Forecasts for monthly quantity issued\", y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA)) +\n  guides(colour=guide_legend(title=\"Forecast\"))"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#what-is-wrong-with-point-forecasts",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#what-is-wrong-with-point-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What is wrong with point forecasts?",
    "text": "What is wrong with point forecasts?\nA point forecast is a single-value prediction representing the most likely future outcome, based on current data and models.\nThe disadvantage of point forecast;\n✗ It ignores additional information in future.\n✗ It does not explain uncertainties around future.\n✗ It can not deal with assymmetric."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nInterval forecasts: A prediction interval is an interval within which power generation may lie, with a certain probability."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nQuantile forecasts: A quantile forecast provides a value that the future observation is expected to be below with a specified probability."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nDistribution forecasts: A comprehensive probabilistic forecast capturing the full range of potential outcomes across all time horizons."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nScenario forecasts: A spectrum of potential futures derived from probabilistic modeling to inform decision- making under uncertainty."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#forecast-distributions-from-bootstrapping",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#forecast-distributions-from-bootstrapping",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Forecast distributions from bootstrapping",
    "text": "Forecast distributions from bootstrapping\nWhen a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance.\n\nA one-step forecast error is defined as\n\n\\(e_t = y_t - \\hat{y}_{t|t-1}\\), \\(y_t = \\hat{y}_{t|t-1} + e_t\\)\n\nSo we can simulate the next observation of a time series using\n\n\\(y_{T+1} = \\hat{y}_{T+1|T} + e_{T+1}\\)\n\nAdding the new simulated observation to our data set, we can repeat the process to obtain\n\n\\(y_{T+2} = \\hat{y}_{T+2|T+1} + e_{T+2}\\)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#generate-different-futures-forecast",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#generate-different-futures-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Generate different futures forecast",
    "text": "Generate different futures forecast\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  generate(h = 12, bootstrap = TRUE, times = 5)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#generate-probabilistic-forecast",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#generate-probabilistic-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Generate probabilistic forecast",
    "text": "Generate probabilistic forecast\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  forecast(h = 12, bootstrap = TRUE, times = 1000)\n\n# A fable: 12 x 6 [1M]\n# Key:     hub_id, product_id, .model [1]\n   hub_id product_id .model     date quantity_issued  .mean\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;mth&gt;          &lt;dist&gt;  &lt;dbl&gt;\n 1 hub_1  product_5  snaive 2023 Jul    sample[1000]  3790.\n 2 hub_1  product_5  snaive 2023 Aug    sample[1000] 32090.\n 3 hub_1  product_5  snaive 2023 Sep    sample[1000] 13039.\n 4 hub_1  product_5  snaive 2023 Oct    sample[1000] 13203.\n 5 hub_1  product_5  snaive 2023 Nov    sample[1000] 28567.\n 6 hub_1  product_5  snaive 2023 Dec    sample[1000] 26470.\n 7 hub_1  product_5  snaive 2024 Jan    sample[1000] 11766.\n 8 hub_1  product_5  snaive 2024 Feb    sample[1000] 25393.\n 9 hub_1  product_5  snaive 2024 Mar    sample[1000] 11540.\n10 hub_1  product_5  snaive 2024 Apr    sample[1000] 11942.\n11 hub_1  product_5  snaive 2024 May    sample[1000] 16975.\n12 hub_1  product_5  snaive 2024 Jun    sample[1000]  5496."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#prediction-intervals",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#prediction-intervals",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nForecast intervals can be extracted using the hilo() function.\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  forecast(h = 12, bootstrap = TRUE, times = 1000) |&gt; \n  hilo(level = 75) |&gt; \n  unpack_hilo(\"75%\")\n\n# A tsibble: 12 x 8 [1M]\n# Key:       hub_id, product_id, .model [1]\n   hub_id product_id .model     date quantity_issued  .mean `75%_lower`\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;mth&gt;          &lt;dist&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1 hub_1  product_5  snaive 2023 Jul    sample[1000]  3509.     -8015. \n 2 hub_1  product_5  snaive 2023 Aug    sample[1000] 32690.     20230. \n 3 hub_1  product_5  snaive 2023 Sep    sample[1000] 12713.       850. \n 4 hub_1  product_5  snaive 2023 Oct    sample[1000] 13433.      1624. \n 5 hub_1  product_5  snaive 2023 Nov    sample[1000] 29074.     16981. \n 6 hub_1  product_5  snaive 2023 Dec    sample[1000] 25767.     14190. \n 7 hub_1  product_5  snaive 2024 Jan    sample[1000] 11377.       -48.1\n 8 hub_1  product_5  snaive 2024 Feb    sample[1000] 25310.     13406. \n 9 hub_1  product_5  snaive 2024 Mar    sample[1000] 11876.      -410. \n10 hub_1  product_5  snaive 2024 Apr    sample[1000] 12137.       -44.1\n11 hub_1  product_5  snaive 2024 May    sample[1000] 16901.      4829. \n12 hub_1  product_5  snaive 2024 Jun    sample[1000]  5407.     -6448. \n# ℹ 1 more variable: `75%_upper` &lt;dbl&gt;"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#forecast-accuracy-evaluation-using-test-sets",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#forecast-accuracy-evaluation-using-test-sets",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Forecast accuracy evaluation using test sets",
    "text": "Forecast accuracy evaluation using test sets\n\nWe mimic the real life situation\nWe pretend we don’t know some part of data (new data)\nIt must not be used for any aspect of model training\nForecast accuracy is computed only based on the test set\n\nTraining and test sets"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\nIn order to evaluate the performance of a forecasting model, we compute its forecast accuracy.\nForecast accuracy is compared by measuring errors based on the test set.\nIdeally it should allow comparing benefits from improved accuracy with the cost of obtaining the improvement."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\nForecast Error\n\\(e_{T+h} = y_{T+h} - \\hat{y}_{T+h\\mid T}\\)\nwhere\n- \\(y_{T+h}\\) is the \\((T+h)^\\text{th}\\) observation \\((h=1,\\dots,H)\\), and\n- \\(\\hat{y}_{T+h\\mid T}\\) is the forecast based on data up to time \\(T\\).\n\nRead more on How to choose appropriate error measure by Ivan Svetunkov."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\n\n\n\n\n\n\n\nMeasure\nFormula\nNotes\n\n\n\n\nMAE  (Mean Absolute Error)\n\\(\\text{MAE} = \\text{mean}(|e_{T+h}|)\\)\nScale dependent\n\n\nMSE  (Mean Squared Error)\n\\(\\text{MSE} = \\text{mean}(e_{T+h}^2)\\)\nScale dependent\n\n\nMAPE  (Mean Absolute Percentage Error)\n\\(\\text{MAPE} = 100\\,\\text{mean}(|e_{T+h}|/|y_{T+h}|)\\)\nScale independent; use if \\(y_t \\gg 0\\) and \\(y\\) has a natural zero\n\n\nRMSE  (Root Mean Squared Error)\n\\(\\text{RMSE} = \\sqrt{\\text{mean}(e_{T+h}^2)}\\)\nScale dependent"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\n\n\n\n\n\n\n\nMeasure\nFormula\nNotes\n\n\n\n\nMASE  (Mean Absolute Scaled Error)\n\\(\\text{MASE} = \\text{mean}(|e_{T+h}|/Q)\\)\nNon-seasonal: \\(Q = \\frac{1}{T-1}\\sum_{t=2}^T |y_t-y_{t-1}|\\)  Seasonal: \\(Q = \\frac{1}{T-m}\\sum_{t=m+1}^T |y_t-y_{t-m}|\\), where \\(m\\) is the seasonal frequency\n\n\nRMSSE  (Root Mean Squared Scaled Error)\n\\(\\text{RMSSE} = \\sqrt{\\text{mean}(e_{T+h}^2/Q)}\\)\nNon-seasonal: \\(Q = \\frac{1}{T-1}\\sum_{t=2}^T (y_t-y_{t-1})^2\\)  Seasonal: \\(Q = \\frac{1}{T-m}\\sum_{t=m+1}^T (y_t-y_{t-m})^2\\), where \\(m\\) is the seasonal frequency"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-4",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\nCreate train and test sets.\n\nf_horizon &lt;- 12 # forecast horizon\n\ntrain &lt;- med_tsb_filter |&gt; # create train set\n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt; \n  filter_index(. ~ '2022 June')\n\nfit_all &lt;- train |&gt; # model fitting\n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(\n    naive = NAIVE(quantity_issued),\n    snaive = SNAIVE(quantity_issued ~ lag('year')),\n    mean = MEAN(quantity_issued ~ window(size = 3)),\n    arima = ARIMA(quantity_issued),\n    ets = ETS(quantity_issued)\n    )\n\nfit_all_fc &lt;- fit_all |&gt; # forecasting\n  forecast(h = f_horizon)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-5",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\nfit_all_fc |&gt; \n  accuracy(med_tsb_filter |&gt;\n             filter(hub_id == 'hub_1' & product_id == 'product_5'),\n           measures = list(point_accuracy_measures))\n\n# A tibble: 5 × 12\n  .model hub_id product_id .type     ME   RMSE    MAE    MPE  MAPE  MASE RMSSE\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arima  hub_1  product_5  Test   5787. 10679.  8732.   5.08  59.3  1.23  1.21\n2 ets    hub_1  product_5  Test   5400. 11163.  8681.   5.33  57.0  1.23  1.27\n3 mean   hub_1  product_5  Test   8568. 12274.  9664.  29.9   54.3  1.37  1.39\n4 naive  hub_1  product_5  Test  -3627.  9508.  8803. -76.1   94.1  1.24  1.08\n5 snaive hub_1  product_5  Test   5237. 12571. 11346.  -1.06  85.6  1.60  1.43\n# ℹ 1 more variable: ACF1 &lt;dbl&gt;"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nCoverage\n\nMeasures how often the true value falls within a prediction interval\nTypically assessed for specific confidence levels (e.g., 95% interval)\n\nExample: A 95% prediction interval should contain the true value 95% of the time."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nSharpness\n\nRefers to the width of prediction intervals\nMeasures how precise or focused the forecast is\n\nExample: A forecast predicting monthly sales qty between 2500-5000 is sharper than 500-10000."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nQuantile score/ Pin ball loss\n\nAssesses entire prediction interval, not just point forecast\nPenalizes too narrow and too wide intervals\nInterpretation: Lower values indicate better calibrated intervals\n\n\\[\nQ_{p,t} =\n\\begin{cases}\n2(1 - p)(f_{p,t} - y_t), & \\text{if } y_t &lt; f_{p,t}, \\\\[1mm]\n2p(y_t - f_{p,t}),       & \\text{if } y_t \\geq f_{p,t}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nCRPS (Continuous Ranked Probability Score)\n\nProper scoring rule\nMeasures accuracy of full predictive distribution\nGeneralizes absolute error to probabilistic forecasts\nInterpretation: Lower CRPS = better forecast\nAdvantage: Sensitive to distance, rewards sharp and calibrated forecasts\n\n\\(\\large \\text{CRPS} = \\text{mean}(p_j),\\)\nwhere\n\\(p_j = \\int_{-\\infty}^{\\infty} \\left(G_j(x) - F_j(x)\\right)^2dx,\\)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-4",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nCRPS (Continuous Ranked Probability Score)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-5",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\n\nfit_all_fc |&gt; \n  accuracy(med_tsb_filter |&gt;\n             filter(hub_id == 'hub_1' & product_id == 'product_5'),\n           measures = list(distribution_accuracy_measures)) |&gt; \n  select(-percentile)\n\n# A tibble: 5 × 5\n  .model hub_id product_id .type  CRPS\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt;\n1 arima  hub_1  product_5  Test  6311.\n2 ets    hub_1  product_5  Test  6648.\n3 mean   hub_1  product_5  Test  7081.\n4 naive  hub_1  product_5  Test  8034.\n5 snaive hub_1  product_5  Test  7823."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#feature-engineering",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#feature-engineering",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature engineering",
    "text": "Feature engineering\nIn this training, we only do a basic feature engineering.\n\n# Load data\nfrom google.colab import files\nuploaded = files.upload()\ndf = pd.read_csv('med_tsb_filter.csv')\n\n# Make the yearmonth as date format\ndf['date'] = pd.to_datetime(df['date']) + pd.offsets.MonthEnd(0)\n\n# Feature Engineering\ndf['month'] = df['date'].dt.month  # create month feature\n\n# categorical encoding\nenc = OrdinalEncoder()\ndf[['hub_id_cat', 'product_id_cat']] = enc.fit_transform(df[['hub_id', 'product_id']])\n\n# Create unique identifier for series\ndf['unique_id'] = df['hub_id'] + '_' + df['product_id']"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#feature-engineering-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#feature-engineering-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature engineering",
    "text": "Feature engineering\n\n# Create series and exogenous data\nseries = df[['date', 'unique_id', 'quantity_issued']]\nexog = df[['date', 'unique_id', 'month', 'hub_id_cat', 'product_id_cat']]\n\n# Transform series and exog to dictionaries\n\nseries_dict = series_long_to_dict(\n    data      = series,\n    series_id = 'unique_id',\n    index     = 'date',\n    values    = 'quantity_issued',\n    freq      = 'M'\n)\n\nexog_dict = exog_long_to_dict(\n    data      = exog,\n    series_id = 'unique_id',\n    index     = 'date',\n    freq      = 'M'\n)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#feature-engineering-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#feature-engineering-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature engineering",
    "text": "Feature engineering\n\n# Partition data in train and test\nend_train = '2022-06-30'\nstart_test = pd.to_datetime(end_train) + pd.DateOffset(months=1)  # Add 1 month\n\nseries_dict_train = {k: v.loc[:end_train] for k, v in series_dict.items()}\nexog_dict_train = {k: v.loc[:end_train] for k, v in exog_dict.items()}\nseries_dict_test = {k: v.loc[start_test:] for k, v in series_dict.items()}\nexog_dict_test = {k: v.loc[start_test:] for k, v in exog_dict.items()}"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#xgboost",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#xgboost",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\nExtreme Gradient Boosting (XGBoost) is a scalable tree-based gradient boosting machine learning algorithm.\n\\(\\hat{y}_{t+h|t} = \\sum_{k=1}^K f_k(\\mathbf{x}_t), \\quad f_k \\in \\mathcal{F}\\)\nWhere: \\(K\\) = number of trees, \\(f_k\\) = tree function, \\(\\mathbf{x}_t\\) = feature vector (lags, calendar features, etc.)\n\n\n\nsource: Rui Guo et al."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\nAssumptions\n\nPredictive patterns can be captured through feature engineering\nRelationships between features and target are stable\nNo strong temporal dependencies beyond engineered features\n\nStrengths & Weaknesses\n✓ Handles non-linear relationships well\n✓ Provides feature importance metrics\n✗ Requires careful parameter tuning\n✗ Less interpretable than linear models"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Fit xgboost forecaster\nregressor_xgb = XGBRegressor(tree_method = 'hist',\n                             enable_categorical = True)\n\nforecaster_xgb = ForecasterRecursiveMultiSeries(\n                 regressor          = regressor_xgb,\n                 transformer_series = None,\n                 lags               = 4,\n                 dropna_from_series = False\n             )\n\nforecaster_xgb.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True)\n\nforecaster_xgb"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Feature importance plot for XGB\nplt.figure(figsize=(10, 6))\nfeat_xgb = forecaster_xgb.get_feature_importances()\nsns.barplot(x='importance', y='feature', data=feat_xgb.sort_values('importance', ascending=False).head(10))\nplt.title('XGBoost Feature Importance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-4",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# XGB predictions and plot\nboot = 100\npredictions_xgb = forecaster_xgb.predict_bootstrapping(steps=12, exog=exog_dict_test, n_boot=boot)\n\n# Create prediction DF and plot example series\nexample_series = list(series_dict_test.keys())[2]\nxgb_pred_test = predictions_xgb[example_series].copy()\n\n# Calculate statistics\nmean_pred = xgb_pred_test.mean(axis=1)\nlower_pred = xgb_pred_test.quantile(0.025, axis=1)\nupper_pred = xgb_pred_test.quantile(0.975, axis=1)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-5",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(series_dict_test[example_series], label='Actual', color='black')\nplt.plot(mean_pred, label='XGB Prediction', color='blue')\nplt.fill_between(mean_pred.index, \n                 lower_pred, \n                 upper_pred,\n                 color='blue', alpha=0.2)\nplt.title('XGBoost Forecast with 95% Prediction Intervals')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-6",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#xgboost-6",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Create prediciton df\n\npred_id = list(predictions_xgb.keys())\n\n# Create an empty DataFrame\nxgb_pred = pd.DataFrame(columns=['date', 'unique_id', 'model'] + [f'X_{i}' for i in range(boot)])\n\nfor i in pred_id:\n    xgb_pred_test = predictions_xgb[i]\n    xgb_pred_test = xgb_pred_test.reset_index()\n    xgb_pred_test.columns = ['date'] + [f'X_{i}' for i in range(boot)]\n    xgb_pred_test['unique_id'] = i\n    xgb_pred_test['model'] = 'xgb'\n    xgb_pred = pd.concat([xgb_pred, xgb_pred_test])\n\nxgb_pred.head()"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\nLight Gradient Boosting Machine (LightGBM) uses leaf-wise tree growth for efficiency whereas other boosting methods divide the tree level‐wise.\n\n\n\nsource: Sheng Dong et al."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\nAssumptions\n\nSimilar to XGBoost but more efficient with large datasets\nHandles categorical features natively\n\nStrengths & Weaknesses\n✓ Faster training speed\n✓ Lower memory usage\n✗ Sensitive to small datasets\n✗ May overfit with noisy data"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Fit lightgbm forecaster\n\nregressor_lgbm = LGBMRegressor(\n                boosting_type = 'gbdt',\n                metric = 'mae',\n                learning_rate = 0.1,\n                num_iterations = 200,\n                n_estimators = 100,\n                objective = 'poisson')\n\nforecaster_lgbm = ForecasterRecursiveMultiSeries(\n                 regressor          = regressor_lgbm, \n                 transformer_series = None,\n                 lags               = 4,  \n                 dropna_from_series = False\n             )\n\nforecaster_lgbm.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True)\n\nforecaster_lgbm"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Feature importance plot for LGBM\nplt.figure(figsize=(10, 6))\nfeat_lgbm = forecaster_lgbm.get_feature_importances()\nsns.barplot(x='importance', y='feature', data=feat_lgbm.sort_values('importance', ascending=False).head(10))\nplt.title('LightGBM Feature Importance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-4",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# LGBM predictions and plot\nboot = 100\npredictions_lgbm = forecaster_lgbm.predict_bootstrapping(steps=12, exog=exog_dict_test, n_boot=boot)\n\n# Create prediction DF and plot example series\nexample_series = list(series_dict_test.keys())[2]\nlgbm_pred_test = predictions_lgbm[example_series].copy()\n\n# Calculate statistics\nmean_pred = lgbm_pred_test.mean(axis=1)\nlower_pred = lgbm_pred_test.quantile(0.025, axis=1)\nupper_pred = lgbm_pred_test.quantile(0.975, axis=1)"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-5",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(series_dict_test[example_series], label='Actual', color='black')\nplt.plot(mean_pred, label='LGBM Prediction', color='blue')\nplt.fill_between(mean_pred.index, \n                 lower_pred, \n                 upper_pred,\n                 color='blue', alpha=0.2)\nplt.title('LGBM Forecast with 95% Prediction Intervals')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-6",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#lightgbm-6",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Create prediciton df\n\npred_id = list(predictions_lgbm.keys())\n\n# Create an empty DataFrame\nlgbm_pred = pd.DataFrame(columns=['date', 'unique_id', 'model'] + [f'X_{i}' for i in range(boot)])\n\nfor i in pred_id:\n    lgbm_pred_test = predictions_lgbm[i]\n    lgbm_pred_test = lgbm_pred_test.reset_index()\n    lgbm_pred_test.columns = ['date'] + [f'X_{i}' for i in range(boot)]\n    lgbm_pred_test['unique_id'] = i\n    lgbm_pred_test['model'] = 'lgbm'\n    lgbm_pred = pd.concat([lgbm_pred, lgbm_pred_test])\n\nlgbm_pred.head()"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#model-evaluation",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#model-evaluation",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Model evaluation",
    "text": "Model evaluation\n\n# Calculate metrics for both models\nxgb_mase, xgb_rmse, xgb_qs, xgb_crps = calculate_metrics(predictions_xgb, series_dict_test, series_dict_train)\nlgbm_mase, lgbm_rmse, lgbm_qs, lgbm_crps = calculate_metrics(predictions_lgbm, series_dict_test, series_dict_train)\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nAverage MASE\nAverage RMSE\nAverage Quantile Score\nAverage CRPS\n\n\n\n\nXGBoost\n1.218\n4878.992\n1151.738\n3548.056\n\n\nLightGBM\n1.061\n4953.944\n310.234\n3498.346\n\n\n\n\nNote: We can improve the performance of XGBoost and LightGBM through better feature engineering and hyperparameter tuning."
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#timegpt",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#timegpt",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\nFoundational time series model for time series forecasting by Nixtla (Read more).\nAssumptions\n\nNo strict stationarity requirements\nAutomatically handles multiple series\n\nStrengths & Weaknesses\n✓ Zero configuration needed\n✓ Handles complex patterns\n✗ Requires API access\n✗ Black-box model"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\nGet API key from Nixtla\n\nVisit https://nixtla.io/\nSign up for free account\nNavigate to API Keys section\nCreate new key and copy it\n\n\n!pip install nixtla\n\n# Load libraries\nfrom nixtla import NixtlaClient\n\n# Initialize Nixtla client\nnixtla_client = NixtlaClient(api_key='your_api_key_here')"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\n\n# Since we already have done the feature engineering, we dont need to do it again\n# Create unique identifier and rename columns for TimeGPT\ndf_timegpt = df.rename(columns={'date': 'ds', 'quantity_issued': 'y'}).drop(columns=['hub_id', 'product_id'])\n\n# Split data into train-test\nend_train = '2022-06-30'\ntrain_df = df_timegpt[df_timegpt['ds'] &lt;= end_train]\ntest_df = df_timegpt[df_timegpt['ds'] &gt; end_train]"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\n\n# TimeGPT Base Model\ntimegpt_fcst =  nixtla_client.forecast(\n    df=train_df,\n    h=len(test_df['ds'].unique()),\n    freq='M',\n    level=[90, 95]  # 90% and 95% prediction intervals\n)\n\nnixtla_client.plot(train_df, timegpt_fcst, time_col='ds', target_col='y')"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-4",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#timegpt-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\n\n# TimeGPT with Exogenous Variables\n\n# Prepare exogenous data\nexog_features = ['month', 'hub_id_cat', 'product_id_cat']\n\n# Future exogenous variables (from your test set)\nfuture_exog = test_df[['unique_id', 'ds'] + exog_features]\n\ntimegpt_reg_fcst = nixtla_client.forecast(\n    df=train_df,\n    X_df=future_exog,\n    h=len(test_df['ds'].unique()),\n    freq='M',\n    level=[90, 95]\n)\n\nnixtla_client.plot(train_df, timegpt_reg_fcst, time_col='ds', target_col='y')"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#model-evaluation-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#model-evaluation-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Model evaluation",
    "text": "Model evaluation\n\n# Calculate metrics for both models\nxgb_mase, xgb_rmse, xgb_qs, xgb_crps = calculate_metrics(predictions_xgb, series_dict_test, series_dict_train)\nlgbm_mase, lgbm_rmse, lgbm_qs, lgbm_crps = calculate_metrics(predictions_lgbm, series_dict_test, series_dict_train)\n\n# Calculate metrics for base model\nbase_metrics = calculate_metrics(timegpt_fcst, test_df, train_df)\n\n# Calculate metrics for regressor model\nreg_metrics = calculate_metrics(timegpt_reg_fcst, test_df, train_df)\n\n\n\n\n\n\n\n\n\n\n\nModel\nAverage MASE\nAverage RMSE\nAverage Quantile Score\n\n\n\n\nTimeGPT Base Model\n1.019\n4261.135\n41.892\n\n\nTimeGPT Regressor Model\n1.125\n5016.053\n57.571"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nA population-based contraceptive needs estimation model combining:\n\nPopulation dynamics\nFamily planning indicators\nMethod/brand distribution factors"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-1",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\n\\[\\begin{equation}\n\\begin{split}\ny_{i,t} = & \\left(\\sum_{j=15}^{50} \\text{mCPR}_{t,j} \\times \\text{WomenPopulation}_{t,j}\\right) \\\\\n           & \\times \\text{MethodMix}_{t,i} \\times \\text{CYP}_{t,i} \\times \\text{BrandMix}_{t,i} \\times \\text{SourceShare}_t\n\\end{split}\n\\end{equation}\\]\n\n\\(i\\): Contraceptive product\n\\(t\\): Time period (year)\n\\(\\text{mCPR}\\): Modern Contraceptive Prevalence Rate (%)\n\\(\\text{WomenPopulation}\\): Women aged 15-49\n\\(\\text{MethodMix}\\): Contraceptive method distribution\n\\(\\text{CYP}\\): Couple-Years of Protection factor\n\\(\\text{BrandMix}\\): Brand preference distribution\n\\(\\text{SourceShare}\\): Provider type distribution"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-2",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nAssumptions\n\nStable demographic patterns during forecast period\nConsistent reporting of family planning indicators\nAccurate CYP values for different methods\nHistorical brand/source mixes remain valid\nLinear relationship between population and needs\nProper spatial distribution via site coordinates\nValid monthly weight distribution"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-3",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nStrengths & Weaknesses\n✓ Directly ties to population dynamics\n✓ Incorporates multiple programmatic factors\n✓ Enables spatial allocation to health sites\n✓ Aligns with public health planning frameworks\n✗ Sensitive to input data quality\n✗ Static assumptions about behavior patterns\n✗ Limited responsiveness to sudden changes\n✗ Provides national level need"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#next-steps-and-further-learning",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#next-steps-and-further-learning",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Next steps and further learning",
    "text": "Next steps and further learning\n\nForecasting for social good learning labs\nForecasting: Principles and Practice\nForecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nNixtla\nSKTIME\nskforecast"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#thank-you",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#thank-you",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Thank you!",
    "text": "Thank you!\n\n\nScan the QR Code and follow us on LinkedIn…"
  },
  {
    "objectID": "lab/data_analytics/index.html",
    "href": "lab/data_analytics/index.html",
    "title": "Data Analytics",
    "section": "",
    "text": "A basic introduction to Python\n\n\n\nPython\n\n\nJupyter Lab\n\n\nData analytics\n\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Markov Processes in Healthcare Supply Chains\n\n\n\nStochastic modelling\n\n\nHealthcare\n\n\nR\n\n\nPython\n\n\n\nWorkshop\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html",
    "href": "lab/data_analytics/intro_python/index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda\n\n\n\n\n\nUnderstanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project\n\n\n\n\nPython is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more.\n\n\n\n\nEasy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike\n\n\n\n\nPython can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer.\n\n\n\nAn IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!\n\n\n\nGoogle Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!\n\n\n\nAnaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!\n\n\n\nGetting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item\n\n\n\n\n\n\n# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data.\n\n\n\n\n\nTo ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#section-1-python-and-ide-essentials",
    "href": "lab/data_analytics/intro_python/index.html#section-1-python-and-ide-essentials",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#section-2-working-with-jupyter-lab",
    "href": "lab/data_analytics/intro_python/index.html#section-2-working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Understanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#what-is-python",
    "href": "lab/data_analytics/intro_python/index.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more."
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#why-learn-python",
    "href": "lab/data_analytics/intro_python/index.html#why-learn-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Easy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#installing-python",
    "href": "lab/data_analytics/intro_python/index.html#installing-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer."
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#setting-up-an-integrated-development-environment-ide",
    "href": "lab/data_analytics/intro_python/index.html#setting-up-an-integrated-development-environment-ide",
    "title": "Introduction to Python",
    "section": "",
    "text": "An IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#setting-up-google-colab",
    "href": "lab/data_analytics/intro_python/index.html#setting-up-google-colab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Google Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#setting-up-anaconda-additional-resources",
    "href": "lab/data_analytics/intro_python/index.html#setting-up-anaconda-additional-resources",
    "title": "Introduction to Python",
    "section": "",
    "text": "Anaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#working-with-jupyter-lab",
    "href": "lab/data_analytics/intro_python/index.html#working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Getting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#example-workflow-in-jupyter-lab",
    "href": "lab/data_analytics/intro_python/index.html#example-workflow-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data."
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#setting-up-the-working-directory-in-jupyter-lab",
    "href": "lab/data_analytics/intro_python/index.html#setting-up-the-working-directory-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "To ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#who-is-the-course-for",
    "href": "lab/data_analytics/intro_python/index.html#who-is-the-course-for",
    "title": "A basic introduction to Python",
    "section": "Who is the course for?",
    "text": "Who is the course for?\nThis course is primarily aimed at learners who require a practical introduction to Python. It assumes no previous experience using Python."
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#learning-objectives",
    "href": "lab/data_analytics/intro_python/index.html#learning-objectives",
    "title": "A basic introduction to Python",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nFamiliarize yourself with Python and an Integrated Development Environment (IDE) such as Jupyter Lab or VS Code, which we’ll use to interact with Python.\nUnderstand the basics of working with Python, including how to write and execute code.\nUse three different ways to work with Python: Python Shell, Python Scripts, and Jupyter Lab.\nLearn about Python’s basic data types and structures.\nInstall and import required libraries.\nFind resources for help when coding in Python."
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#prerequisites",
    "href": "lab/data_analytics/intro_python/index.html#prerequisites",
    "title": "A basic introduction to Python",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nNo prior knowledge of Python is assumed.\nInstall Python and GitHub Desktop."
  },
  {
    "objectID": "lab/data_analytics/intro_python/index.html#course-topics",
    "href": "lab/data_analytics/intro_python/index.html#course-topics",
    "title": "A basic introduction to Python",
    "section": "Course Topics",
    "text": "Course Topics\n\nPython and IDE Essentials\n\nSection 1: Introduction to Python and IDEs\n\nIntroduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nOverview of Jupyter Lab, VS Code, Google Colab, and Anaconda\n\n\n\nSection 2: Working with Jupyter Lab\n\nUnderstanding the basic syntax\nWriting and executing code in Python\nSetting up and organizing a Python project"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda\n\n\n\n\n\nUnderstanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project\n\n\n\n\nPython is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more.\n\n\n\n\nEasy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike\n\n\n\n\nPython can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer.\n\n\n\nAn IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!\n\n\n\nGoogle Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!\n\n\n\nAnaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!\n\n\n\nGetting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item\n\n\n\n\n\n\n# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data.\n\n\n\n\n\nTo ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#section-1-python-and-ide-essentials",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#section-1-python-and-ide-essentials",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#section-2-working-with-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#section-2-working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Understanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#what-is-python",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more."
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#why-learn-python",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#why-learn-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Easy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#installing-python",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#installing-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer."
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-an-integrated-development-environment-ide",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-an-integrated-development-environment-ide",
    "title": "Introduction to Python",
    "section": "",
    "text": "An IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-google-colab",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-google-colab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Google Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-anaconda-additional-resources",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-anaconda-additional-resources",
    "title": "Introduction to Python",
    "section": "",
    "text": "Anaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#working-with-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Getting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#example-workflow-in-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#example-workflow-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data."
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-the-working-directory-in-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/intro_to_python.html#setting-up-the-working-directory-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "To ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html",
    "href": "lab/data_analytics/intro_python/materials/index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda\n\n\n\n\n\nUnderstanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project\n\n\n\n\nPython is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more.\n\n\n\n\nEasy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike\n\n\n\n\nPython can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer.\n\n\n\nAn IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!\n\n\n\nGoogle Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!\n\n\n\nAnaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!\n\n\n\nGetting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item\n\n\n\n\n\n\n# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data.\n\n\n\n\n\nTo ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#section-1-python-and-ide-essentials",
    "href": "lab/data_analytics/intro_python/materials/index.html#section-1-python-and-ide-essentials",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#section-2-working-with-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/index.html#section-2-working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Understanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#what-is-python",
    "href": "lab/data_analytics/intro_python/materials/index.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more."
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#why-learn-python",
    "href": "lab/data_analytics/intro_python/materials/index.html#why-learn-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Easy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#installing-python",
    "href": "lab/data_analytics/intro_python/materials/index.html#installing-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer."
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#setting-up-an-integrated-development-environment-ide",
    "href": "lab/data_analytics/intro_python/materials/index.html#setting-up-an-integrated-development-environment-ide",
    "title": "Introduction to Python",
    "section": "",
    "text": "An IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#setting-up-google-colab",
    "href": "lab/data_analytics/intro_python/materials/index.html#setting-up-google-colab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Google Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#setting-up-anaconda-additional-resources",
    "href": "lab/data_analytics/intro_python/materials/index.html#setting-up-anaconda-additional-resources",
    "title": "Introduction to Python",
    "section": "",
    "text": "Anaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#working-with-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/index.html#working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Getting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item"
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#example-workflow-in-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/index.html#example-workflow-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data."
  },
  {
    "objectID": "lab/data_analytics/intro_python/materials/index.html#setting-up-the-working-directory-in-jupyter-lab",
    "href": "lab/data_analytics/intro_python/materials/index.html#setting-up-the-working-directory-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "To ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#post-training-feedback",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#post-training-feedback",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Post-Training Feedback",
    "text": "Post-Training Feedback"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#self-assessment",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#self-assessment",
    "title": "Home",
    "section": "Self-Assessment",
    "text": "Self-Assessment"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#self-assessment",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#self-assessment",
    "title": "Home",
    "section": "Self-Assessment",
    "text": "Self-Assessment"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#assumptions",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#assumptions",
    "title": "Methodological Skills for Research",
    "section": "",
    "text": "This session is designed for undergraduate students.\nNo prior knowledge of research philosophy or paradigms is expected.\nThe session is descriptive and exploratory, not heavily theoretical or mathematical."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#what-we-will-cover",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#what-we-will-cover",
    "title": "Methodological Skills for Research",
    "section": "",
    "text": "Introduction to research philosophy: ontology, epistemology, axiology\nUnderstanding research paradigms: positivism, interpretivism, critical realism, pragmatism, post-structuralism\nDefining methodology vs. methods\nOverview of qualitative, quantitative, and mixed-methods approaches\nExploring core research principles: objectivity, validity, reliability, reflexivity, and ethics\nUsing real-world examples to illustrate abstract concepts"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#what-we-will-not-cover",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#what-we-will-not-cover",
    "title": "Methodological Skills for Research",
    "section": "",
    "text": "Detailed coverage of data analysis techniques\nIn-depth theoretical derivations or philosophical debates\nSpecific guidance on research proposal writing\nFull training in data collection instruments (e.g., surveys, interview guides)"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#materials",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#materials",
    "title": "Methodological Skills for Research",
    "section": "",
    "text": "You can find the lecture materials here."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#ontology",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#ontology",
    "title": "Methodological Skills for Research",
    "section": "Ontology",
    "text": "Ontology\n\nOntology – concerns the nature of reality and what we believe exists.\nAsks: What is real? Do social phenomena have an existence independent of our perception?\nRealist vs. relativist ontologies: a single objective reality vs. multiple socially constructed realities."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#epistemology",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#epistemology",
    "title": "Methodological Skills for Research",
    "section": "Epistemology",
    "text": "Epistemology\n\nEpistemology – concerns knowledge and how we can know about reality.\nAsks: What counts as acceptable knowledge? Can we obtain objective truth or only subjective understandings?\nPositivist epistemology seeks observable, measurable evidence, while interpretivist epistemology emphasizes understanding meanings in context."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#axiology",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#axiology",
    "title": "Methodological Skills for Research",
    "section": "Axiology",
    "text": "Axiology\n\nAxiology – the study of values; examines the role of researchers’ values and ethics in research.\nConsiders issues of right and wrong, what is worth researching, and the value judgments we bring to research.\nInfluences how we interpret findings and what we see as important (e.g., a researcher’s personal and cultural values can shape research focus)."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#what-is-a-research-paradigm",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#what-is-a-research-paradigm",
    "title": "Methodological Skills for Research",
    "section": "What is a research paradigm?",
    "text": "What is a research paradigm?\n\nA research paradigm is a worldview or basic set of beliefs that guides how research is conducted.\nIt influences what should be studied, how it should be studied, and how results are interpreted.\nMajor paradigms include positivism, post-positivism, interpretivism (constructivism), critical (realism), pragmatism, and postmodern/post-structural paradigms."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-positivism",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-positivism",
    "title": "Methodological Skills for Research",
    "section": "Paradigm – Positivism",
    "text": "Paradigm – Positivism\n\nPositivism – assumes an objective reality that can be measured. Advocates applying natural science methods to study social phenomena.\nSeeks causal relationships and generalizable laws; uses testable hypotheses and focuses on observable facts (not subjective opinions).\nPositivist research aims to be value-free and objective, with the researcher as a neutral observer."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-interpretivism",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-interpretivism",
    "title": "Methodological Skills for Research",
    "section": "Paradigm – Interpretivism",
    "text": "Paradigm – Interpretivism\n\nInterpretivism – assumes reality is socially constructed and subjective, with multiple valid perspectives.\nEmphasizes understanding the meanings and context of human experiences rather than finding universal laws. Research is often qualitative and contextual.\nResearchers recognize their own role and values in the process; reflexivity is encouraged to understand how interpretations are formed."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-critical-realism",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-critical-realism",
    "title": "Methodological Skills for Research",
    "section": "Paradigm – Critical Realism",
    "text": "Paradigm – Critical Realism\n\nCritical Realism – a post-positivist paradigm acknowledging an independent reality that exists, but our understanding of it is inevitably imperfect.\nDistinguishes between the “real” (underlying structures) and the “observable”; unobservable mechanisms cause observable events.\nUses theory to identify underlying social structures or mechanisms. Bridges positivism and interpretivism by accepting an external reality but viewing knowledge of it as theory-laden."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-pragmatism",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-pragmatism",
    "title": "Methodological Skills for Research",
    "section": "Paradigm – Pragmatism",
    "text": "Paradigm – Pragmatism\n\nPragmatism – focuses on what works in practice. Truth is viewed as what is useful in answering the research question.\nRejects strict either/or choices between paradigms: methods and concepts are chosen for their practical usefulness rather than adherence to a single philosophy.\nOften underpins mixed methods research: quantitative and qualitative approaches are combined to provide actionable knowledge from multiple perspectives."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-post-structuralism",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#paradigm-post-structuralism",
    "title": "Methodological Skills for Research",
    "section": "Paradigm – Post-Structuralism",
    "text": "Paradigm – Post-Structuralism\n\nPost-Structuralism – a critical paradigm (related to postmodernism) that questions stable structures and universal truths.\nArgues that meaning and knowledge are not fixed; they are constructed through language, discourse, and power relations, and thus can be deconstructed.\nChallenges assumptions about reality and truth, highlighting how perspectives are influenced by culture, language, and power dynamics."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#activity-locating-your-paradigm",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#activity-locating-your-paradigm",
    "title": "Methodological Skills for Research",
    "section": "Activity – Locating your paradigm",
    "text": "Activity – Locating your paradigm\n\n\n\n−+\n05:00\n\n\n\n\nThink-Pair-Share: What discipline or field are you in? Discuss which research paradigm(s) are common in your field.\nAre there taken-for-granted assumptions about what should be studied, how to study it, and what counts as evidence in your field?"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#principle-objectivity",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#principle-objectivity",
    "title": "Methodological Skills for Research",
    "section": "Principle – Objectivity",
    "text": "Principle – Objectivity\n\nObjectivity – maintaining impartiality and avoiding personal biases when conducting research.\nResearchers strive to observe and report facts as they are, independent of their own beliefs or values.\nIn practice, complete objectivity is challenging, especially in social research – awareness of potential bias is crucial."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#principles-validity-reliability",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#principles-validity-reliability",
    "title": "Methodological Skills for Research",
    "section": "Principles – Validity & Reliability",
    "text": "Principles – Validity & Reliability\n\nValidity – the extent to which a study or measurement actually measures what it intends to measure (accuracy/truthfulness of findings).\nReliability – the consistency or repeatability of results; a reliable study yields similar results under consistent conditions.\nA study should aim to be both valid and reliable – e.g., a survey that consistently gives the same result is reliable, but it must also measure the right concept to be valid."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#principle-reflexivity",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#principle-reflexivity",
    "title": "Methodological Skills for Research",
    "section": "Principle – Reflexivity",
    "text": "Principle – Reflexivity\n\nReflexivity – the practice of reflecting on how the researcher’s own beliefs, experiences, and biases influence the research.\nAcknowledges that researchers are part of the social world they study, rather than completely objective outsiders.\nReflexive research involves being transparent about one’s positionality and how it may shape data collection, analysis, and interpretations."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#principle-ethics",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#principle-ethics",
    "title": "Methodological Skills for Research",
    "section": "Principle – Ethics",
    "text": "Principle – Ethics\n\nResearch Ethics – guidelines for the responsible conduct of research, defining what is acceptable or unacceptable behavior with participants and data.\nKey ethical principles: informed consent, avoiding harm to participants, confidentiality and privacy, honesty and integrity in data and analysis.\nEthical research ensures the rights and well-being of participants are protected and that the knowledge gained is trustworthy."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#activity-ethics-scenario",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#activity-ethics-scenario",
    "title": "Methodological Skills for Research",
    "section": "Activity – Ethics scenario",
    "text": "Activity – Ethics scenario\n\n\n\n−+\n05:00\n\n\n\n\nImagine you are conducting interviews on a sensitive topic (e.g., mental health). How would you ensure your study is ethical?"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#quantitative-research",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#quantitative-research",
    "title": "Methodological Skills for Research",
    "section": "Quantitative research",
    "text": "Quantitative research\n\nQuantitative research – deals with numerical data and statistics to test hypotheses and examine relationships.\nTypically seeks generalizable results and causal explanations; often associated with positivist approaches (aims for objectivity and measurement).\nExample: measuring class size and test scores across schools to see if smaller classes lead to higher performance (data analyzed statistically)."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#qualitative-research",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#qualitative-research",
    "title": "Methodological Skills for Research",
    "section": "Qualitative research",
    "text": "Qualitative research\n\nQualitative research – deals with non-numerical data (words, observations) to understand concepts, experiences, or social contexts in depth.\nProduces rich, detailed insights rather than broad generalizations; often aligned with interpretivist approaches (understanding meanings in context).\nExample: interviewing teachers and students to explore how classroom size affects their experiences and learning, gathering in-depth narratives."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#mixed-methods",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#mixed-methods",
    "title": "Methodological Skills for Research",
    "section": "Mixed methods",
    "text": "Mixed methods\n\nMixed methods – integrates both quantitative and qualitative approaches in one study to address a question from multiple angles.\nCombines numerical data with narrative data to provide a more comprehensive understanding and to corroborate findings across methods.\nExample: studying climate change adaptation by analyzing climate data (quantitative) and interviewing community members (qualitative) to merge scientific and local perspectives."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#methodology-vs.-methods",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#methodology-vs.-methods",
    "title": "Methodological Skills for Research",
    "section": "Methodology vs. Methods",
    "text": "Methodology vs. Methods\n\nMethodology – the overall strategy or research design guiding how you investigate a problem (the logic of inquiry) (E.g., experimental, survey, ethnography, case study).\nMethods – the specific techniques or procedures for data collection and analysis (the tools) (E.g., questionnaires, interviews, observations, statistical analysis).\nRemember: methodology is why and how you’re doing the research (at a strategic level), whereas methods are what you actually do to collect and analyze data."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#example-poor-housing-design",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#example-poor-housing-design",
    "title": "Methodological Skills for Research",
    "section": "Example – Poor housing design",
    "text": "Example – Poor housing design\n\nScenario: A report claims many new houses suffer from “poor design.” How might researchers study this issue differently based on their paradigm?\nOntology: Is “poor design” a measurable fact (objective criteria) or a concept defined by stakeholders’ perceptions (subjective)?\nEpistemology: Should we gain knowledge by measuring design features (collect facts) or by understanding what “poor design” means to residents and designers (interpret meanings)?\nMethods: A positivist approach might survey homes against design quality criteria & quantify residents’ satisfaction. An interpretivist approach might conduct interviews or focus groups to explore how people perceive and discuss “poor design”."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#activity-plan-a-study",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#activity-plan-a-study",
    "title": "Methodological Skills for Research",
    "section": "Activity – Plan a study",
    "text": "Activity – Plan a study\n\n\n\n−+\n10:00\n\n\n\n\nChoose a research question of interest (e.g., “How does social media use affect academic performance?”).\nWhat research approach would you use (qualitative, quantitative, or mixed) and why? What paradigm might this align with?\nIdentify one or two specific methods you would use to collect data, and explain how these methods fit your research question and paradigm."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#wrapping-up",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#wrapping-up",
    "title": "Methodological Skills for Research",
    "section": "Wrapping up",
    "text": "Wrapping up\n\nMethodological skills involve making informed choices at each stage of research – from philosophical stance to data collection methods.\nNo single paradigm or method is “best” for all questions. The goal is to align your approach with your research question and objectives, and to be able to justify your choices.\nUnderstanding these foundations helps you critically evaluate others’ research and conduct your own studies in a rigorous, reflective, and ethical manner."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#thank-you",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#thank-you",
    "title": "Home",
    "section": "Thank you!",
    "text": "Thank you!\n\n\nScan the QR Code and follow us on LinkedIn…\n\n\n\n\n\n\n\n\n\n\n\nMethodological Skills for Research"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#blind-men-and-the-elephant",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#blind-men-and-the-elephant",
    "title": "Methodological Skills for Research",
    "section": "Blind men and the elephant",
    "text": "Blind men and the elephant"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#what-is-reality",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#what-is-reality",
    "title": "Home",
    "section": "What is reality",
    "text": "What is reality"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#research-onion",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#research-onion",
    "title": "Methodological Skills for Research",
    "section": "Research onion",
    "text": "Research onion\n\n\n\n\n\nSource: © Mark Saunders, Philip Lewis and Adrian Thornhill 2006."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-transport-logistics-examples",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-transport-logistics-examples",
    "title": "Home",
    "section": "Research paradigms: transport & logistics examples",
    "text": "Research paradigms: transport & logistics examples"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-transport-logistics-examples-1",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-transport-logistics-examples-1",
    "title": "Home",
    "section": "Research Paradigms: Transport & Logistics Examples",
    "text": "Research Paradigms: Transport & Logistics Examples\n\n\n\n\n\n\n\n\n\n\nParadigm\nExample Research Question\nApproach\nMethodology\nOutcome/Focus\n\n\n\n\nPositivism\nWhat is the impact of traffic congestion on last-mile delivery times in urban areas?\nQuantitative analysis using GPS and traffic data\nStatistical modeling (e.g., regression)\nIdentifies causal relationships and general patterns to support predictions\n\n\nInterpretivism\nHow do long-haul truck drivers experience life on the road?\nInterviews and fieldwork to understand drivers’ perspectives\nThematic analysis, narrative inquiry\nExplores subjective experiences and meanings from participants’ viewpoints\n\n\nCritical Realism\nWhy do delivery delays persist in a warehouse despite technology upgrades?\nInvestigate both surface data and underlying structures (e.g., labor, policies)\nCase study, process tracing\nUncovers hidden mechanisms driving observable problems\n\n\nPragmatism\nHow can route planning tools be improved to balance delivery speed and driver satisfaction?\nCombine delivery data with driver feedback to develop practical recommendations\nMixed methods (quantitative + qualitative)\nSolves practical problems using a combination of methods that “work”\n\n\nPost-Structuralism\nHow is the concept of “efficiency” constructed in logistics marketing discourse?\nExamine how companies use language to define “efficiency” in ads, reports, training manuals\nDiscourse analysis\nReveals how language shapes meaning and reflects power dynamics in the industry"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#different-ways-of-seeing",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#different-ways-of-seeing",
    "title": "Methodological Skills for Research",
    "section": "Different ways of seeing",
    "text": "Different ways of seeing\n\nWhat is “out there” to know\nWhat, and how, can we know about it?\nHow do we go about acquiring the knowledge to find out about it?\nWhat techniques or procedures will we use to acquire this knowledge?"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-examples",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-examples",
    "title": "Methodological Skills for Research",
    "section": "Research Paradigms: Examples",
    "text": "Research Paradigms: Examples\n\n\n\n\n\n\n\n\n\n\nParadigm\nResearch Question\nApproach\nMethodology\nOutcome/Focus\n\n\n\n\nPositivism\nWhat is the impact of traffic congestion on last-mile delivery times in urban areas?\nQuantitative analysis using GPS and traffic data\nStatistical modeling (e.g., regression)\nIdentifies causal relationships and general patterns to support predictions\n\n\nInterpretivism\nHow do long-haul truck drivers experience life on the road?\nInterviews and fieldwork to understand drivers’ perspectives\nThematic analysis, narrative inquiry\nExplores subjective experiences and meanings from participants’ viewpoints"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-examples-1",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-examples-1",
    "title": "Methodological Skills for Research",
    "section": "Research Paradigms: Examples",
    "text": "Research Paradigms: Examples\n\n\n\n\n\n\n\n\n\n\nParadigm\nResearch Question\nApproach\nMethodology\nOutcome/Focus\n\n\n\n\nCritical Realism\nWhy do delivery delays persist in a warehouse despite technology upgrades?\nInvestigate both surface data and underlying structures (e.g., labor, policies)\nCase study, process tracing\nUncovers hidden mechanisms driving observable problems\n\n\nPragmatism\nHow can route planning tools be improved to balance delivery speed and driver satisfaction?\nCombine delivery data with driver feedback to develop practical recommendations\nMixed methods (quantitative + qualitative)\nSolves practical problems using a combination of methods that “work”"
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-examples-2",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html#research-paradigms-examples-2",
    "title": "Methodological Skills for Research",
    "section": "Research Paradigms: Examples",
    "text": "Research Paradigms: Examples\n\n\n\n\n\n\n\n\n\n\nParadigm\nResearch Question\nApproach\nMethodology\nOutcome/Focus\n\n\n\n\nPost-Structuralism\nHow is the concept of “efficiency” constructed in logistics marketing discourse?\nExamine how companies use language to define “efficiency” in ads, reports, training manuals\nDiscourse analysis\nReveals how language shapes meaning and reflects power dynamics in the industry"
  },
  {
    "objectID": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-4",
    "href": "lab/forecasting/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nFemale condom needs (age 20-15) for 2020 (example)\n\n\n\n\n\n\n\n\n\nParameter\nValue\nUnit\nSources\n\n\n\n\nA. Women population 20 – 25\n345,000\n# of people\nWorldPop\n\n\nB. Percentage of women who will use a contraceptive product (mCPR)\n68%\n%\nPMA DataLab\n\n\nC. Percentage of the method mix\n26%\n%\nPMA DataLab\n\n\nD. CYP\n120\n-\nUSAID\n\n\nE. Percentage of brand A mix\n45%\n%\nAuthor calculation\n\n\nF. Percentage of source mix\n13%\n%\nUSAID\n\n\nTotal requirement (A × B × C × D × E × F)\n629,694\n# of products\n-"
  },
  {
    "objectID": "talks/intro_method_skills/index.html#context",
    "href": "talks/intro_method_skills/index.html#context",
    "title": "Methodological Skills for Research",
    "section": "Context",
    "text": "Context\nUnderstanding research methodologies is essential for conducting rigorous, credible, and context-appropriate research in fields like transport, logistics, and beyond. This lecture introduces key philosophical foundations, paradigms, and methodological choices that shape how research questions are framed and investigated. It supports students in developing critical thinking and research design skills, which are foundational for academic projects and evidence-based decision-making in logistics and supply chain management."
  },
  {
    "objectID": "talks/intro_method_skills/index.html#target-audience",
    "href": "talks/intro_method_skills/index.html#target-audience",
    "title": "Methodological Skills for Research",
    "section": "Target Audience",
    "text": "Target Audience\n\nUndergraduate students studying Transport and Logistics Management\nAnyone interested in gaining a practical understanding of research paradigms, methodologies, and methods\nStudents preparing to develop their undergraduate dissertations or research proposals"
  },
  {
    "objectID": "talks/intro_method_skills/index.html#prerequisites",
    "href": "talks/intro_method_skills/index.html#prerequisites",
    "title": "Methodological Skills for Research",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCuriosity about research and how knowledge is constructed\nNo prior experience with research philosophy is required\nBasic understanding of research goals (e.g., solving real-world problems, generating new insights)"
  },
  {
    "objectID": "talks/intro_method_skills/index.html#learning-outcomes",
    "href": "talks/intro_method_skills/index.html#learning-outcomes",
    "title": "Methodological Skills for Research",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this lecture, students will be able to:\n\n✅ Distinguish between ontology, epistemology, axiology, methodology, and methods\n✅ Understand and compare major research paradigms: Positivism, Interpretivism, Critical Realism, Pragmatism, and Post-Structuralism\n✅ Recognize how paradigms influence research design choices\n✅ Identify suitable methodologies and data collection methods for different research problems\n✅ Apply these concepts to real-world examples in transport and logistics contexts\n✅ Critically reflect on their own positionality and philosophical assumptions in research"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#assumptions",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#assumptions",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Assumptions",
    "text": "Assumptions\n\nYou should be familiar with basic probability and random variables.\nYou are expected to be comfortable with R (or Python) for basic simulation and matrix operations.\nThis is not a theory-heavy workshop—we will use simple examples to build intuition, not derive theorems."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-cover",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-cover",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "What we will cover",
    "text": "What we will cover\n\nKey concepts in Markov chains\nTransition matrices and system evolution\nSteady-state distributions and their interpretations\nApplications of Markov models in healthcare supply chains\nCode demonstrations and simulations"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-not-cover",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-not-cover",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "What we will not cover",
    "text": "What we will not cover\n\nProofs of Markov chain convergence theorems\nAdvanced topics like Hidden Markov Models or Semi-Markov processes\nContinuous-time Markov processes in full generality\nFormal classification of all chain types (e.g., reducibility, ergodicity)\nMarcov process with rewards"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#materials",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#materials",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Materials",
    "text": "Materials\n\nYou can find the workshop materials here.\n\nNote: These materials are based on my learnings at NATCOR Taught Course Centre: Stochastic Modelling Course."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#outline",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#outline",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Outline",
    "text": "Outline\n\nWhy stochastic modeling?\nWhat are Markov processes?\nBrand switching as a DTMC example\nCode walk-throughs in R"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#why-stochastic-modelling-in-healthcare-supply-chains",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#why-stochastic-modelling-in-healthcare-supply-chains",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Why stochastic modelling in healthcare supply chains?",
    "text": "Why stochastic modelling in healthcare supply chains?\n\n\nDemand is unpredictable: Patient arrivals, seasonal outbreaks, and changing usage patterns\nSupply is uncertain: Delivery delays, stock losses, funding gaps, partial shipments\nHelps quantify risks: Probability of stockouts, unmet demand, cold chain failures\nEnables simulation of long-run behavior: nderstand steady-state stock levels, refill patterns, or equipment uptime\nUseful for evaluating interventions: E.g., What if we promote a local brand? Add a backup supplier? Use mobile delivery?"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states",
    "text": "System states\nWe want to model the behaviour of a system which can change its state from one period to the next.\nFor example:\n\n\n\n\n\n\n\n\n\n\nSystem\nStates\nTime unit\n\n\n\n\nWeather\nSunny, cloudy, rainy, …\nDay or hour\n\n\nNo. of people in a healtcare centre\n0, 1, 2, 3, …\nMinute or second\n\n\nStatus of job application\n“In preparation”, “Submitted”, “Invited for interview”, …\nDay or hour?"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states: weather example",
    "text": "System states: weather example\nWe are interested in the transitions between different states. Suppose there are only 3 possible states in weather:\n\nFrom any of the 3 states we can get to any of the other states in a single transition (or stay in the same state). A sample trajectory of the system could be:\n\\([Sunny,  Cloudy,  Cloudy,  Rainy,  Sunny,  Cloudy,  Rainy,  Rainy, …]\\)"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states: weather example cont.",
    "text": "System states: weather example cont.\nFor convenience, let’s give each of the 3 states a number: \\(0 – Sunny\\), \\(1 – Cloudy\\), \\(2 – Rainy\\)\n\nLet \\(S\\) be the set of system states. So in our example: \\(S = {0, 1, 2}\\)\n\nLet \\(X_n\\) be the state of the system after \\(n\\) time units (e.g. days).\n\nFor example:"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.-1",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.-1",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states: weather example cont.",
    "text": "System states: weather example cont.\nSuppose that the system begins as follows:\n\\(X_0 = 0, X_1 = 1, X_2 = 1, X_3 = 2\\)\nGiven the above, what is the probability that \\(X_4 = 0\\), i.e. it is sunny after 4 days? We can write this as a conditional probability:\n\\[\nPr(X_4 = 0\\,|\\,\\underbrace{X_0 = 0,\\,X_1 = 1,\\, X_2 = 1,\\,X_3 = 2)}_{\\text{entire history of the process}}\n\\] Suppose we assume that the probability that \\(X_4 = 0\\) depends only on the value of \\(X_3\\), and not on \\(X_2\\), \\(X_1\\) or \\(X_0\\). We can then write:\n\\[\nPr(X_4 = 0\\,|\\,\\underbrace{X_0 = 0,\\,X_1 = 1,\\, X_2 = 1,\\,X_3 = 2)}_{\\text{entire history of the process}} = Pr(X_4 = 0\\,|\\,\\underbrace{X_3 = 2)}_{\\text{current state}}\n\\]"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#markov-process-definition",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#markov-process-definition",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Markov process: definition",
    "text": "Markov process: definition\nA stochastic process with the Markov Property:\n\n\\((X_1, X_2, X_3, ...)\\)\n\nin which the probability distribution for state \\(X_{n+1}\\) depends only on the state \\(X_n\\), and not on any of the states from \\(X_0\\) up to \\(X_{n-1}\\) (for all \\(n \\ge 0\\)).\n\nWriting this more mathematically, we can say:  \\[\n\\Pr\\bigl(X_{n+1} = j \\;\\bigm|\\; X_0 = x_0,\\; X_1 = x_1,\\;\\dots,\\; X_n = x_n\\bigr)\n\\;=\\;\n\\Pr\\bigl(X_{n+1} = j \\;\\bigm|\\; X_n = x_n\\bigr)\n\\]\nKey idea: “The future depends only on the present, not on the past.”\nSo the probability distribution for the next state depends only on the current state, not on the history of previous states. Can be discrete or continuous in time."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#what-is-a-good-example-of-a-markov-chain",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#what-is-a-good-example-of-a-markov-chain",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "What is a good example of a Markov chain?",
    "text": "What is a good example of a Markov chain?\n\n\n\n\n\nFor example, in Snakes and Ladders, if \\(X_n = 50\\) then:\n\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=67 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=52 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=53 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=34 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=55 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=56 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#assumptions-of-markov-chain-models",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#assumptions-of-markov-chain-models",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Assumptions of Markov chain models",
    "text": "Assumptions of Markov chain models\n\nRemember, simple Markov chain models rely upon some important assumptions:\n\nThe Markov chain is in exactly one state on any particular time step\nThe probability distribution for the next state only depends on the current state [Markov property]\nThe transition probabilities are the same on every time step"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#transition-probability-matrix",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#transition-probability-matrix",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Transition probability matrix",
    "text": "Transition probability matrix\nLet \\(S = \\{s_1, s_2, \\dots, s_n\\}\\) be the finite state space. The one-step transition probabilities are:\n\\[\np_{ij} = \\Pr(X_{t+1} = s_j \\mid X_t = s_i)\n\\]\nWe collect these into the transition matrix:\n\\[\nP = [p_{ij}], \\quad \\text{where each row sums to 1}\n\\]\nThe \\(n\\)-step transition matrix is:\n\\[\nP^{(n)} = P^n = \\underbrace{P \\cdot P \\cdot \\dots \\cdot P}_{n\\text{ times}}\n\\]\nWe can use the Chapman-Kolmogorov Equation:\n\\[\nP^{(n)} = P^{(k)} P^{(n-k)}\n\\]"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#steady-state-distribution",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#steady-state-distribution",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Steady-state distribution",
    "text": "Steady-state distribution\n\nWhen a Markov chain evolves over time, the probability distribution of states may converge to a fixed vector — called the steady-state distribution, \\(\\pi\\).\nA steady-state distribution exists if the chain is:\n\nIrreducible: all states communicate\nAperiodic: not cyclic\nPositive recurrent: expected return time is finite\n\nSteady-state equation: \\(\\pi P = \\pi, \\quad \\sum_i \\pi_i = 1\\)\nThis means:\n\n\\(\\pi_i\\): long-run proportion of time spent in state \\(i\\)\nIt’s a left eigenvector of \\(P\\) corresponding to eigenvalue 1"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-infinitely-many-states",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-infinitely-many-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Classifying states: infinitely many states",
    "text": "Classifying states: infinitely many states\n\nSuppose we have a Markov chain defined on the infinite state space \\({0, 1, 2, …}\\). If it is in state 0 then it moves to state \\(1\\) with probability \\(1\\). If it is in any other state then it moves up with probability \\(p\\) and moves down with probability \\(1-p\\), where \\(0&lt;p&lt;1\\).\n\nKey point: if a Markov chain has infinitely many states, a steady-state distribution might not exist."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-periodic-states",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-periodic-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Classifying states: periodic states",
    "text": "Classifying states: periodic states\n\nSuppose we have a Markov chain which just goes back and forth between states 1 and 2. We call this a periodic Markov chain with a period of 2, because it can only return to the same state after an even number of time steps.\n\nKey point: even if a steady-state distribution exists, the Markov chain might not “converge” to the steady-state distribution unless it actually starts there."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-absorbing-states",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-absorbing-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Classifying states: absorbing states",
    "text": "Classifying states: absorbing states\nThis time, we assume that if you’re in state 1 you stay there forever – and the same applies to state 2. In this case we call states 1 and 2 absorbing states.\n\nKey point: if states do not all “communicate” with each other (meaning that you cannot necessarily find a path from one state to another), there could be multiple steady-state distributions."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A",
    "text": "Lets promote Brand A\nAssume patients switch weekly between two brands according to the probabilities shown in the table below:\n\n\n\n\nFrom  To\nBrand A\nBrand B\n\n\n\n\nBrand A\n0.92\n0.08\n\n\nBrand B\n0.15\n0.85\n\n\n\n\n\n\n\n\n\nLet \\(X_n\\) denote the preferred brand (either A or B) of a randomly-chosen customer after \\(n\\) weeks. From the table:\n\\(\\Pr(X_{n+1}=1 \\mid X_n = 1) = 0.92\\) \\(\\Pr(X_{n+1}=2 \\mid X_n = 1) = 0.08\\)\n\\(\\Pr(X_{n+1}=1 \\mid X_n = 2) = 0.15\\) \\(\\Pr(X_{n+1}=2 \\mid X_n = 2) = 0.85\\)"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\n\nWe can represent this situation using a discrete-time Markov chain:\n\nWe are using some shorthand notation: \\(p_{ij} = Pr(X_{n+1} = j \\,|\\, X_n = i)\\)"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-1",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-1",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nSuppose that after zero weeks, both brands have a 50% market share. This means a randomly-chosen patient has a 50% chance of preferring Brand A.\nSo: \\(Pr(X_0 = 1) = 0.5\\) and \\(Pr(X_0 = 2) = 0.5\\)\nUsing the switching probabilities and invoking the law of total probability, we can calculate the preferred brand of a randomly-chosen patient after 1 week:\n\\(Pr(X_1 = 1) = \\underbrace{Pr(X_0 = 1)Pr(X_1 = 1|X_0 = 1)}_{\\text{(begin in state 1)}} + \\underbrace{Pr(X_0 = 2)Pr(X_1 = 1|X_0 = 2)}_{\\text{(begin in state 2)}} \\\\ =(0.5×0.92)+(0.5×0.15)=0.535\\)\n\n\\(Pr(X_1 = 2) = \\underbrace{Pr(X_0 = 1)Pr(X_1 = 2|X_0 = 1)}_{\\text{(begin in state 1)}} + \\underbrace{Pr(X_0 = 2)Pr(X_1 = 2|X_0 = 2)}_{\\text{(begin in state 2)}} \\\\=(0.5×0.08)+(0.5×0.85)=0.465\\)"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-2",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-2",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nWe can make these calculations look neater by using matrix-vector notation. Let \\(p_{ij}\\) denote the probability of switching from \\(i\\) to \\(j\\). Obviously, this implies:\n\\(p_{ij}\\ge0,\\;\\forall\\,i,j\\in S\\), \\(\\sum_{j\\in S} p_{ij} = 1,\\;\\forall\\,i\\in S.\\)\nLet \\(P\\) denote the transition matrix:\n\\[\n\\mathbf{P} \\;=\\;\n\\begin{pmatrix}\np_{11} & p_{12} \\\\\np_{21} & p_{22}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n0.92 & 0.08 \\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\]\nAlso let \\(q^{(0)}\\) be the vector of initial market shares:\n\\(q^{(0)}=(0.5, 0.5)\\)\nTo get the expected market shares after one week, we multiply \\(q^{(0)}\\) by \\(P\\) to get \\(q^{(1)}\\)\n\\[\n\\mathbf{q}^{(0)} \\mathbf{P}\n\\;=\\;\n(0.5,\\;0.5)\\,\n\\begin{pmatrix}\n0.92 & 0.08\\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\;=\\;\n(0.535,\\;0.465)\n\\;=\\;\n\\mathbf{q}^{(1)}\n\\]"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-3",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-3",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nTo find the expected market shares after two weeks, we repeat the process, starting from the expected market shares after one week. This means we need to calculate\n\\[\n\\overset{\\substack{\\text{expected market shares}\\\\\\text{after one week}}}\n         {\\mathbf{q}^{(1)}}\n\\;\\times\\;\n\\overset{\\text{transition matrix}}{P}\n\\;\\;=\\;\\;\n\\overset{\\substack{\\text{expected market shares}\\\\\\text{after two weeks}}}\n         {\\mathbf{q}^{(2)}}\n\\]\n\\[\n\\mathbf{q}^{(1)} \\mathbf{P}\n\\;=\\;\n(0.535,\\;0.465)\\,\n\\begin{pmatrix}\n0.92 & 0.08\\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\;=\\;\n(0.56195,\\;0.43805)\n\\;=\\;\n\\mathbf{q}^{(2)}\n\\]\nSimilarly, to find the expected market shares after three weeks:\n\\[\n\\mathbf{q}^{(2)} \\mathbf{P}\n\\;=\\;\n(0.56195,\\;0.43805)\\,\n\\begin{pmatrix}\n0.92 & 0.08\\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\;=\\;\n(0.5827015,\\;0.4172985)\n\\;=\\;\n\\mathbf{q}^{(3)}\n\\]"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-4",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-4",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\n\nIn general, to find the expected market shares after \\(n\\) weeks, we calculate\n\\(\\mathbf{q}^{(n-1)}\\,\\mathbf{P} = \\mathbf{q}^{(n)}\\)\n\nThis is the same as:\n\\(\\mathbf{q}^{(0)}\\underbrace{\\mathbf{P}\\,\\mathbf{P}\\,\\cdots\\,\\mathbf{P}}_{\\text{(n times)}} \\;=\\; \\mathbf{q}^{(0)}\\,\\mathbf{P}^n\\)\n\ni.e. the vector of initial market shares multiplied by \\(\\mathbf{P}\\) to the power \\(n\\)."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-5",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-5",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nIf we use a line graph to show how the expected market shares change with time, we find that both of them appear to converge to fixed values.\n\n\n\n\n\n\n\n\n\nThe market share for Brand A converges to about 65.2%, and the market share for Brand B converges to about 34.8%. We call (0.652, 0.348) the steady-state distribution in this example."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-6",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-6",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nChanging the initial probability vector \\(\\mathbf{q}^{(0)}\\) has no effect on the steady-state distribution."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-7",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-7",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\n\nRecall that: \\(\\mathbf{q}^{(n)}\\) = vector of state probabilities after \\(n\\) time steps (this is the vector of expected market shares in our task) and \\(\\mathbf{P}\\) = transition matrix.\n\nWe have already seen that \\(\\mathbf{q}^{(n)}\\) appears to converge towards a limit as \\(n\\) increases. If we use \\(\\boldsymbol{\\pi}\\) to denote the limiting vector, i.e. \\(\\boldsymbol{\\pi} \\;=\\;\\lim_{n \\to \\infty}\\mathbf{q}^{(n)},\\) then we can take limits to obtain:\n\\[\n\\boldsymbol{\\pi}P = \\boldsymbol{\\pi}\n\\]\nNote: \\(\\boldsymbol{\\pi} = (\\pi_1,\\pi_2)\\), where the numbers \\(\\pi_1\\) and \\(\\pi_2\\) are “unknowns.”"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-8",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-8",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nBy solving the equations \\(\\boldsymbol{\\pi} = (\\pi_1,\\pi_2)\\) (where \\(\\boldsymbol{\\pi}\\) is a ‘vector of unknowns’) we can calculate the steady-state distribution of the Markov chain.\nRecall the market shares example from earlier. Let \\(\\pi_1\\) and \\(\\pi_2\\) denote the (unknown) steady-state expected market shares for brands A and B respectively. We have:\n\\[\n\\boldsymbol{\\pi}P = \\boldsymbol{\\pi}\n\\quad\\Longleftrightarrow\\quad\n(\\pi_1, \\pi_2)\n\\begin{pmatrix}\n0.92 & 0.08 \\\\\n0.15 & 0.85\n\\end{pmatrix}\n=\n(\\pi_1, \\pi_2).\n\\]\nThis gives us a couple of linear equations in \\(\\pi_1\\) and \\(\\pi_2\\):\n\\[\n\\;\\Longleftrightarrow\\;\n\\begin{cases}\n\\pi_1 = 0.92\\,\\pi_1 + 0.15\\,\\pi_2\\\\[6pt]\n\\pi_2 = 0.08\\,\\pi_1 + 0.85\\,\\pi_2\n\\end{cases}\n\\;\\Longleftrightarrow\\;\n\\begin{cases}\n0.08\\,\\pi_1 - 0.15\\,\\pi_2 = 0\\\\[6pt]\n0.08\\,\\pi_1 - 0.15\\,\\pi_2 = 0\n\\end{cases}\n\\quad\\text{(These equations are the same!)}\n\\]\nTo solve these equations we will also have to use the fact that \\(\\pi_1 + \\pi_2 = 1\\)."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-9",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-9",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nLet’s replace one of the two identical equations with \\(\\pi_1 + \\pi_2 = 1\\). Then we have:\n\\[\n\\begin{cases}\n0.08\\,\\pi_1 - 0.15\\,\\pi_2 = 0\\\\[6pt]\n\\pi_1 + \\pi_2 = 1\n\\end{cases}\n\\]\nIf we substitute \\(\\pi_2 = 1 - \\pi_1\\) into the first equation, this gives:\n\\[\n0.08\\,\\pi_1 \\;-\\; 0.15\\,(1 - \\pi_1) \\;=\\; 0\n\\;\\Longleftrightarrow\\;\n0.23\\,\\pi_1 \\;=\\; 0.15\n\\]\nTherefore;\n\\[\n\\pi_1 \\;=\\;\\frac{0.15}{0.23}\\approx 0.652,\n\\qquad\n\\pi_2 \\;=\\;1 - \\pi_1\\approx 0.348\n\\]\nSo the steady-state expected market shares are roughly: 65.2% for Brand A and 34.8% for Brand B."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/index.html#who-is-the-course-for",
    "href": "lab/data_analytics/dl4sg_marcov/index.html#who-is-the-course-for",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Who is the course for?",
    "text": "Who is the course for?\nThis course is intended for healthcare supply chain researchers, practitioners, and students who want to model uncertainty in logistical systems using Markov Chains. It assumes familiarity with basic probability and matrix manipulation in R."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/index.html#learning-objectives",
    "href": "lab/data_analytics/dl4sg_marcov/index.html#learning-objectives",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the structure and assumptions of discrete-time Markov chains (DTMCs)\nApply transition matrices to simulate system evolution over time\nCompute and interpret steady-state distributions\nModel brand switching and service reliability in healthcare supply chains\nSimulate long-run outcomes and interpret them visually in R\nLink model insights to supply chain policy decisions (e.g. stockouts, demand, brand promotion)"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/index.html#prerequisites",
    "href": "lab/data_analytics/dl4sg_marcov/index.html#prerequisites",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nComfortable with basic probability (random variables, distributions)\nFamiliarity with R and tidyverse for matrix operations and plotting\nNo prior knowledge of Markov chains is assumed"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/index.html#course-topics",
    "href": "lab/data_analytics/dl4sg_marcov/index.html#course-topics",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Course Topics",
    "text": "Course Topics\n\nMarkov Chains for Healthcare Supply Chains\n\nSection 1: Foundations of Markov Chains\n\nState-based systems and probabilistic transitions\nThe Markov property and memoryless dynamics\nTransition probability matrices and system trajectories\n\n\n\nSection 2: Steady-State Analysis\n\nn-step transitions and convergence\nExistence and uniqueness of steady-state distributions\nInterpreting long-run behaviour in real systems\n\n\n\nSection 3: Brand Switching Case Study\n\nPromote-local strategy for paracetamol brands\nSimulate switching behaviour and market share convergence\nSolve steady-state equations using R\n\n\n\nSection 4: Applied Simulation in R\n\nVector-matrix calculations\nTransition matrix exponentiation\nPlotting and comparing convergence paths"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "In this lab, we explore a discrete-time Markov chain modeling brand switching between two paracetamol brands:\n\nBrand A: locally manufactured (state 1)\nBrand B: imported (state 2)\n\nPatients switch weekly according to the transition matrix:\n\n\n\nFrom  To\nBrand A\nBrand B\n\n\n\n\nBrand A\n0.92\n0.08\n\n\nBrand B\n0.15\n0.85\n\n\n\nWe will:\n\nCompute expected market shares after the first three weeks.\nWrite a function to calculate the steady-state distribution.\nPlot convergence over time.\nVerify independence of the steady state from initial shares.\nAdd simulation and sensitivity tasks.\n\n\n\nSet the initial market-share vector \\(q^{(0)} = (0.5, 0.5)\\). Manually compute \\(q^{(1)}, q^{(2)}, q^{(3)}\\) using matrix multiplication.\n\n# Define transition matrix P\nP &lt;- matrix(c(0.92, 0.08,\n              0.15, 0.85), \n            nrow = 2, byrow = TRUE,\n            dimnames = list(c(\"A\", \"B\"), c(\"A\", \"B\")))\n# Initial distribution\nq0 &lt;- c(A = 0.5, B = 0.5)\n# Compute first three periods\nq_list &lt;- accumulate(1:3, ~ .x %*% P, .init = q0)[-1]\n# Display results\nresults1 &lt;- tibble(week = 1:3,\n       A = map_dbl(q_list, 1),\n       B = map_dbl(q_list, 2))\nknitr::kable(results1, digits=6)\n\n\n\n\nweek\nA\nB\n\n\n\n\n1\n0.535000\n0.465000\n\n\n2\n0.561950\n0.438050\n\n\n3\n0.582702\n0.417298\n\n\n\n\n\n\n\n\nImplement a function steady_state(P) that returns the long-run distribution via eigen decomposition. Verify it on \\(P\\) above.\n\nsteady_state &lt;- function(P) {\n  eig &lt;- eigen(t(P))\n  vec &lt;- Re(eig$vectors[, which.min(Mod(eig$values - 1))])\n  pi &lt;- vec / sum(vec)\n  pi\n}\n# Test\ndist_ss &lt;- steady_state(P)\nknitr::kable(t(round(dist_ss, 6)), col.names = c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.652174\n0.347826\n\n\n\n\n\n\n\n\nSimulate the expected market-share evolution for 30 weeks and plot \\(q^{(n)}\\) to illustrate convergence.\n\ntotal_weeks &lt;- 30\npi_mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\ncolnames(pi_mat) &lt;- c(\"A\", \"B\")\npi_mat[1, ] &lt;- q0\nfor (i in 2:(total_weeks + 1)) {\n  pi_mat[i, ] &lt;- pi_mat[i - 1, ] %*% P\n}\ndata_conv &lt;- as_tibble(pi_mat) |&gt;\n  mutate(week = 0:total_weeks) |&gt;\n  pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\nsteady_vals &lt;- steady_state(P)\n\ndata_conv |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Market-Share Convergence over Time\",\n       x = \"Week\",\n       y = \"Expected Market Share\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nThe convergence plot shows both shares approaching the dashed lines at (0.652174, 0.347826) by around week 20.\n\n\n\nUsing initial distributions \\((0.5, 0.5), (0.75, 0.25), (0.25, 0.75), (0.9, 0.1)\\), confirm that all converge to the same steady state.\n\ninit_list &lt;- list(\n  \"0.5,0.5\" = c(0.5, 0.5),\n  \"0.75,0.25\" = c(0.75, 0.25),\n  \"0.25,0.75\" = c(0.25, 0.75),\n  \"0.9,0.1\" = c(0.9, 0.1)\n)\nframe_list &lt;- map2_dfr(init_list, names(init_list), function(init_dist, label) {\n  mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\n  colnames(mat) &lt;- c(\"A\", \"B\")\n  mat[1, ] &lt;- init_dist\n  for (i in 2:(total_weeks + 1)) mat[i, ] &lt;- mat[i - 1, ] %*% P\n  as_tibble(mat) |&gt;\n    mutate(week = 0:total_weeks, init = label) |&gt;\n    pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\n})\nframe_list |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line() +\n  facet_wrap(~ init) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Convergence Across Different Initial Distributions\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nRegardless of initial shares, all paths converge to approximately (0.652174, 0.347826) by week 30.\n\n\n\nSimulate the brand preference for 10,000 patients over 30 weeks by sampling transitions. Compare empirical frequencies at week 30 to the theoretical steady state.\n\nset.seed(123)\nn_patients &lt;- 10000\nn_weeks &lt;- 30\n# Initialize state 1=A, 2=B randomly according to q0\nstates &lt;- sample(c(1,2), n_patients, replace = TRUE, prob = q0)\nfor (t in 1:n_weeks) {\n  probs &lt;- P[states, ]\n  states &lt;- apply(probs, 1, function(pr) sample(c(1,2), 1, prob = pr))\n}\nempirical &lt;- prop.table(table(states))\nknitr::kable(t(round(empirical, 4)), col.names=c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.6446\n0.3554\n\n\n\n\n\nEmpirical shares at week 30 (approx):\n\nBrand A: 0.6518\nBrand B: 0.3482\n\nClose to theoretical (0.6522, 0.3478).\n\n\n\nVary the promotion effect: change \\(p_{21}\\) (Brand B → A) from 0.15 to 0.20 and 0.10. Compute new steady states and discuss impact.\n\nfor (p21 in c(0.20, 0.10)) {\n  P_var &lt;- matrix(c(0.92, 0.08,\n                    p21, 1-p21), nrow=2, byrow=TRUE)\n  ss &lt;- steady_state(P_var)\n  cat(\"p21 =\", p21, \"→ steady state:\", round(ss,4), \"\\n\")\n}\n\np21 = 0.2 → steady state: 0.7143 0.2857 \np21 = 0.1 → steady state: 0.5556 0.4444 \n\n\nIncreasing the promotion increases the long-run share of Brand A, while decreasing it lowers that share."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-1-initial-evolution",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-1-initial-evolution",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Set the initial market-share vector \\(q^{(0)} = (0.5, 0.5)\\). Manually compute \\(q^{(1)}, q^{(2)}, q^{(3)}\\) using matrix multiplication.\n\n# Define transition matrix P\nP &lt;- matrix(c(0.92, 0.08,\n              0.15, 0.85), \n            nrow = 2, byrow = TRUE,\n            dimnames = list(c(\"A\", \"B\"), c(\"A\", \"B\")))\n# Initial distribution\nq0 &lt;- c(A = 0.5, B = 0.5)\n# Compute first three periods\nq_list &lt;- accumulate(1:3, ~ .x %*% P, .init = q0)[-1]\n# Display results\nresults1 &lt;- tibble(week = 1:3,\n       A = map_dbl(q_list, 1),\n       B = map_dbl(q_list, 2))\nknitr::kable(results1, digits=6)\n\n\n\n\nweek\nA\nB\n\n\n\n\n1\n0.535000\n0.465000\n\n\n2\n0.561950\n0.438050\n\n\n3\n0.582702\n0.417298"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-2-steady-state-distribution-function",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-2-steady-state-distribution-function",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Implement a function steady_state(P) that returns the long-run distribution via eigen decomposition. Verify it on \\(P\\) above.\n\nsteady_state &lt;- function(P) {\n  eig &lt;- eigen(t(P))\n  vec &lt;- Re(eig$vectors[, which.min(Mod(eig$values - 1))])\n  pi &lt;- vec / sum(vec)\n  pi\n}\n# Test\ndist_ss &lt;- steady_state(P)\nknitr::kable(t(round(dist_ss, 6)), col.names = c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.652174\n0.347826"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-3-convergence-plot",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-3-convergence-plot",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Simulate the expected market-share evolution for 30 weeks and plot \\(q^{(n)}\\) to illustrate convergence.\n\ntotal_weeks &lt;- 30\npi_mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\ncolnames(pi_mat) &lt;- c(\"A\", \"B\")\npi_mat[1, ] &lt;- q0\nfor (i in 2:(total_weeks + 1)) {\n  pi_mat[i, ] &lt;- pi_mat[i - 1, ] %*% P\n}\ndata_conv &lt;- as_tibble(pi_mat) |&gt;\n  mutate(week = 0:total_weeks) |&gt;\n  pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\nsteady_vals &lt;- steady_state(P)\n\ndata_conv |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Market-Share Convergence over Time\",\n       x = \"Week\",\n       y = \"Expected Market Share\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nThe convergence plot shows both shares approaching the dashed lines at (0.652174, 0.347826) by around week 20."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-4-independence-from-initial-shares",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-4-independence-from-initial-shares",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Using initial distributions \\((0.5, 0.5), (0.75, 0.25), (0.25, 0.75), (0.9, 0.1)\\), confirm that all converge to the same steady state.\n\ninit_list &lt;- list(\n  \"0.5,0.5\" = c(0.5, 0.5),\n  \"0.75,0.25\" = c(0.75, 0.25),\n  \"0.25,0.75\" = c(0.25, 0.75),\n  \"0.9,0.1\" = c(0.9, 0.1)\n)\nframe_list &lt;- map2_dfr(init_list, names(init_list), function(init_dist, label) {\n  mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\n  colnames(mat) &lt;- c(\"A\", \"B\")\n  mat[1, ] &lt;- init_dist\n  for (i in 2:(total_weeks + 1)) mat[i, ] &lt;- mat[i - 1, ] %*% P\n  as_tibble(mat) |&gt;\n    mutate(week = 0:total_weeks, init = label) |&gt;\n    pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\n})\nframe_list |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line() +\n  facet_wrap(~ init) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Convergence Across Different Initial Distributions\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nRegardless of initial shares, all paths converge to approximately (0.652174, 0.347826) by week 30."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-5-simulation-challenge",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-5-simulation-challenge",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Simulate the brand preference for 10,000 patients over 30 weeks by sampling transitions. Compare empirical frequencies at week 30 to the theoretical steady state.\n\nset.seed(123)\nn_patients &lt;- 10000\nn_weeks &lt;- 30\n# Initialize state 1=A, 2=B randomly according to q0\nstates &lt;- sample(c(1,2), n_patients, replace = TRUE, prob = q0)\nfor (t in 1:n_weeks) {\n  probs &lt;- P[states, ]\n  states &lt;- apply(probs, 1, function(pr) sample(c(1,2), 1, prob = pr))\n}\nempirical &lt;- prop.table(table(states))\nknitr::kable(t(round(empirical, 4)), col.names=c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.6446\n0.3554\n\n\n\n\n\nEmpirical shares at week 30 (approx):\n\nBrand A: 0.6518\nBrand B: 0.3482\n\nClose to theoretical (0.6522, 0.3478)."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-6-sensitivity-analysis",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-6-sensitivity-analysis",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Vary the promotion effect: change \\(p_{21}\\) (Brand B → A) from 0.15 to 0.20 and 0.10. Compute new steady states and discuss impact.\n\nfor (p21 in c(0.20, 0.10)) {\n  P_var &lt;- matrix(c(0.92, 0.08,\n                    p21, 1-p21), nrow=2, byrow=TRUE)\n  ss &lt;- steady_state(P_var)\n  cat(\"p21 =\", p21, \"→ steady state:\", round(ss,4), \"\\n\")\n}\n\np21 = 0.2 → steady state: 0.7143 0.2857 \np21 = 0.1 → steady state: 0.5556 0.4444 \n\n\nIncreasing the promotion increases the long-run share of Brand A, while decreasing it lowers that share."
  },
  {
    "objectID": "talks/intro_method_skills/slides/intro_method_skills.html",
    "href": "talks/intro_method_skills/slides/intro_method_skills.html",
    "title": "Methodological Skills for Research",
    "section": "",
    "text": "2025-04-11\n\n\n\nThis session is designed for undergraduate students.\nNo prior knowledge of research philosophy or paradigms is expected.\nThe session is descriptive and exploratory, not heavily theoretical or mathematical.\n\n\n\n\n\nIntroduction to research philosophy: ontology, epistemology, axiology\nUnderstanding research paradigms: positivism, interpretivism, critical realism, pragmatism, post-structuralism\nDefining methodology vs. methods\nOverview of qualitative, quantitative, and mixed-methods approaches\nExploring core research principles: objectivity, validity, reliability, reflexivity, and ethics\nUsing real-world examples to illustrate abstract concepts\n\n\n\n\n\nDetailed coverage of data analysis techniques\nIn-depth theoretical derivations or philosophical debates\nSpecific guidance on research proposal writing\nFull training in data collection instruments (e.g., surveys, interview guides)\n\n\n\n\nYou can find the lecture materials here."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html",
    "href": "lab/data_analytics/dl4sg_marcov/materials/dl4sg_markov.html",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "",
    "text": "2025-05-15"
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html",
    "title": "Home",
    "section": "",
    "text": "This exercise is based on the brand switching example discussed in the slides. It models consumer behavior using a discrete-time Markov chain with two states representing two brands."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html#introduction",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html#introduction",
    "title": "Home",
    "section": "",
    "text": "This exercise is based on the brand switching example discussed in the slides. It models consumer behavior using a discrete-time Markov chain with two states representing two brands."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html#excel-based-exploration",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html#excel-based-exploration",
    "title": "Home",
    "section": "Excel-Based Exploration",
    "text": "Excel-Based Exploration\n\nInitial Setup\nOpen the “Brand switching example – spreadsheet” Excel file.\n\nInitial market shares are in cells B2 (Brand 1) and C2 (Brand 2) — both start at 50%.\nTransition probabilities are found in:\n\nE2: \\(\\Pr(\\text{Brand 1 next week} \\mid \\text{Brand 1 this week})\\)\nF2: \\(\\Pr(\\text{Brand 2 next week} \\mid \\text{Brand 1 this week})\\)\nE3: \\(\\Pr(\\text{Brand 1 next week} \\mid \\text{Brand 2 this week})\\)\nF3: \\(\\Pr(\\text{Brand 2 next week} \\mid \\text{Brand 2 this week})\\)\n\n\nCells B3 and C3 contain formulas to calculate expected market shares after one week.\n\n\nTask 1A\nCopy the formulas in B3 and C3 down for at least 30 rows.\n\nQuestion: How long does it take until the expected market share for Brand 1 exceeds 60%?\n\nAnswer: _________\n\n\nTask 1B\nChange the initial market shares: - Case 1: 30% (Brand 1) and 70% (Brand 2) - Case 2: 15% (Brand 1) and 85% (Brand 2)\n\nQuestion: In each case, how many weeks does it take for Brand 1 to overtake Brand 2?\n\nAnswers: - Case 1: _________ - Case 2: _________\nRegardless of starting conditions, the system quickly converges to a steady-state distribution.\n\n\nTask 1C\nInspect further down the spreadsheet and record the steady-state expected market shares for both brands to 4 decimal places.\nAnswer: - Brand 1: _________ - Brand 2: _________\nThese values are determined by the transition matrix, not the initial shares."
  },
  {
    "objectID": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html#using-r-to-find-steady-state-distribution",
    "href": "lab/data_analytics/dl4sg_marcov/materials/lab/00_dtcm.html#using-r-to-find-steady-state-distribution",
    "title": "Home",
    "section": "Using R to Find Steady-State Distribution",
    "text": "Using R to Find Steady-State Distribution\nWe now use R to compute the steady-state distribution algebraically.\nR Function to Solve for Steady-State\n\nlibrary(tidyverse)\n\nsteady_state_dist &lt;- function(P) {\n  # Calculate the steady-state distribution of a discrete-time Markov chain\n  # defined by the transition matrix P\n  \n  # Parameters:\n  # P : matrix\n  #   The transition matrix of the Markov chain of interest.\n  \n  # Returns:\n  # A vector representing the steady-state probabilities for the Markov chain.\n  \n  # Check if P is a square matrix\n  if (!is.matrix(P) || nrow(P) != ncol(P)) {\n    stop(\"P must be a square matrix\")\n  }\n  \n  # Number of states\n  dim &lt;- nrow(P)\n  \n  # Set up the system of equations\n  Q &lt;- P - diag(dim)  # P - I\n  ones &lt;- rep(1, dim) \n  Q &lt;- cbind(Q, ones)  # Append column of ones\n  QTQ &lt;- Q %*% t(Q)    # Compute Q * Q^T\n  bQT &lt;- rep(1, dim)   # Right-hand side vector\n  \n  # Solve the equations and return the solution\n  return(solve(QTQ, bQT))\n}\n\n\nTask 2A\nOpen the Python script 00_dtcm.qmd. This script can be used to find the steady-state distribution of a Markov chain, given the transition probability matrix. In our case, we are interested in using it to calculate the steady-state expected market shares for the two brands. Find the steady-state distribution in each of the following cases:\n\\[\nP_1 =\n\\begin{bmatrix}\n0.96 & 0.04 \\\\\n0.07 & 0.93\n\\end{bmatrix}\n\\]\n\n\\[\nP_2 =\n\\begin{bmatrix}\n0.75 & 0.25 \\\\\n0.22 & 0.78\n\\end{bmatrix}\n\\] \n\\[\nP_3 =\n\\begin{bmatrix}\n0.99 & 0.01 \\\\\n0.02 & 0.98\n\\end{bmatrix}\n\\]\n\n# Transition matrices\nmatrices &lt;- list(\n  matrix(c(__, __, __, __), 2, byrow = TRUE),\n  matrix(c(__, __, __, __), 2, byrow = TRUE),\n  matrix(c(__, __, __, __), 2, byrow = TRUE)\n)\n\n# Compute steady-state for each\nfor (P in matrices) {\n  pi_dist &lt;- steady_state_dist(P)\n  print(t(pi_dist))\n}\n\n\n🧠 Even with very similar transition probabilities, the resulting steady-state market shares can be surprisingly different.\n\n\n\nTask 2B – Extra Challenge: Algebraic derivation of steady-state\nLet the transition matrix be:\n\\[\nP = \\begin{bmatrix}\n1 - a & a \\\\\n2a & 1 - 2a\n\\end{bmatrix}, \\quad \\text{where } 0 &lt; a &lt; 0.5\n\\]\nWe want to show the steady-state vector is:\n\\[\n\\boldsymbol{\\pi} = \\left(\\frac{2}{3}, \\frac{1}{3}\\right)\n\\]\n\na_vals &lt;- c(__, __, __, __)\n\nfor (a in a_vals) {\n  P &lt;- matrix(c(__, __, __, __), 2, byrow = TRUE)\n  pi_dist &lt;- steady_state_dist(P)\n  print(t(pi_dist))\n}"
  },
  {
    "objectID": "lab/dl4sg_marcov/index.html#who-is-the-course-for",
    "href": "lab/dl4sg_marcov/index.html#who-is-the-course-for",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Who is the course for?",
    "text": "Who is the course for?\nThis course is intended for healthcare supply chain researchers, practitioners, and students who want to model uncertainty in logistical systems using Markov Chains. It assumes familiarity with basic probability and matrix manipulation in R."
  },
  {
    "objectID": "lab/dl4sg_marcov/index.html#learning-objectives",
    "href": "lab/dl4sg_marcov/index.html#learning-objectives",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the structure and assumptions of discrete-time Markov chains (DTMCs)\nApply transition matrices to simulate system evolution over time\nCompute and interpret steady-state distributions\nModel brand switching and service reliability in healthcare supply chains\nSimulate long-run outcomes and interpret them visually in R\nLink model insights to supply chain policy decisions (e.g. stockouts, demand, brand promotion)"
  },
  {
    "objectID": "lab/dl4sg_marcov/index.html#prerequisites",
    "href": "lab/dl4sg_marcov/index.html#prerequisites",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nComfortable with basic probability (random variables, distributions)\nFamiliarity with R and tidyverse for matrix operations and plotting\nNo prior knowledge of Markov chains is assumed"
  },
  {
    "objectID": "lab/dl4sg_marcov/index.html#course-topics",
    "href": "lab/dl4sg_marcov/index.html#course-topics",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Course Topics",
    "text": "Course Topics\n\nMarkov Chains for Healthcare Supply Chains\n\nSection 1: Foundations of Markov Chains\n\nState-based systems and probabilistic transitions\nThe Markov property and memoryless dynamics\nTransition probability matrices and system trajectories\n\n\n\nSection 2: Steady-State Analysis\n\nn-step transitions and convergence\nExistence and uniqueness of steady-state distributions\nInterpreting long-run behaviour in real systems\n\n\n\nSection 3: Brand Switching Case Study\n\nPromote-local strategy for paracetamol brands\nSimulate switching behaviour and market share convergence\nSolve steady-state equations using R\n\n\n\nSection 4: Applied Simulation in R\n\nVector-matrix calculations\nTransition matrix exponentiation\nPlotting and comparing convergence paths"
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#outline",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#outline",
    "title": "WHAT WAS LOST",
    "section": "Outline",
    "text": "Outline\n\n\nWhat was never counted...\nThe fundamental question\nWhat we are going to do\nNumerical experiment\nWhat’s NEXT"
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen",
    "title": "WHAT WAS LOST",
    "section": "Seen the UNSEEN",
    "text": "Seen the UNSEEN\n\n\n\n\n\nThis is Nilu.\nShe went to the pharmacy today to get contraceptive Product A.\nBut it wasn’t in stock.\n\n\n\n\nImage generated using ChatGPT."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen-1",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen-1",
    "title": "WHAT WAS LOST",
    "section": "Seen the UNSEEN",
    "text": "Seen the UNSEEN\n\n\n\n\n\nShe didn’t go to the pharmacy today.\nWhy would she?\nThe last two times she went, they didn’t have the product she needed.\nThe system doesn’t know this.\nIt sees “no demand” and it continously logs Nilu’s silence as data.\n\n\n\n\nImage generated using ChatGPT."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen-2",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen-2",
    "title": "WHAT WAS LOST",
    "section": "Seen the UNSEEN",
    "text": "Seen the UNSEEN\n\n\n\n\n\nWhen data is censored by stockouts or service interruptions…\n…forecasts fail.\nNot just by being wrong, But by being blind.\nThis creates a broken trust and leads to\nUNMET DEMAND.\n\n\n\n\nImage generated using ChatGPT."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen-3",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#seen-the-unseen-3",
    "title": "WHAT WAS LOST",
    "section": "Seen the UNSEEN",
    "text": "Seen the UNSEEN\n\n\n\n\n\nIn reality…\nThere are more than\n218 million women\nlike Nilu still have an unmet need for family planning.\nUltimately, this results in dropouts, unwanted pregnancies, and almost 7 million hospitalizations each year in developing countries.\n\n\n\n\nImage generated using ChatGPT.\nSources: USAID, 2020; PATH, 2019; Mukasa et al., 2017; Gilda et al., 2016"
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#contraceptive-products-arent-easily-substituted",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#contraceptive-products-arent-easily-substituted",
    "title": "WHAT WAS LOST",
    "section": "Contraceptive products aren’t easily SUBSTITUTED",
    "text": "Contraceptive products aren’t easily SUBSTITUTED\n\n\n\n\n*Percent of women who will get pregnant within the first year of typical use."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#key-definitions",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#key-definitions",
    "title": "WHAT WAS LOST",
    "section": "Key definitions",
    "text": "Key definitions\n\n\nStockouts: Periods when demand is higher than available inventory, leading to censored observations of demand.\nInterruptions: Periods when no products are issued despite available stock, thus artificially recorded as zero demand.\nCensored Demand: Demand occurring during periods when products are unavailable (stockouts or interruptions), thus not fully observable.\nTrue Demand: Actual demand that would have occurred if sufficient stock was available."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#censorship-scenarios",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#censorship-scenarios",
    "title": "WHAT WAS LOST",
    "section": "Censorship scenarios",
    "text": "Censorship scenarios\nHow can a demand forecasting and inventory optimization model that incorporates lost sales estimation and contextual field data enhance contraceptive supply chain performance and reduce stockouts in developing countries?\n\n\n\n\n\n\n\n\nFigure 1: Censorship scenarios in family planning supply chains."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#why-this-is-critical",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#why-this-is-critical",
    "title": "WHAT WAS LOST",
    "section": "Why this is critical",
    "text": "Why this is critical\n\n\n\n\n\n\n\nFrequent stockouts are common in family planning supply chains, especially in developing countries, significantly impacting public health outcomes.\n\n\n\n\nDuring my recent field visit to Ethiopia, stockouts were repeatedly identified by demand planners as a major barrier to effective contraceptive supply management.\n\n\n\n\nTraditional forecasting methods fail under censorship.\n\n\n\n\nPrior research inadequately addresses demand estimation under conditions of frequent stockouts and interruptions, often leading to biased forecasts and suboptimal inventory decisions.\n\n\n\n\n\n\nImage generated using ChatGPT.\nSources: Bijvank et al., 2011; Karimi et al., 2021; Thanos et as., 2022 ; Trapero, 2024"
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#how-we-can-fill-the-gaps",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#how-we-can-fill-the-gaps",
    "title": "WHAT WAS LOST",
    "section": "How we can fill the gaps",
    "text": "How we can fill the gaps\n\n\n\n\n\n\n\nRQ1: How accurately can a Tobit Kalman Filter with conformal prediction estimate true demand under censorship?\n\n\n\n\nRQ2: How does demand reconstruction improve inventory performance compared to baseline planning methods?\n\n\n\n\nRQ3: How do planners adjust their orders in response to proposed model-generated recommendations?\n\n\n\n\n\n\nImage generated using ChatGPT."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#our-proposed-framework",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#our-proposed-framework",
    "title": "WHAT WAS LOST",
    "section": "Our proposed framework",
    "text": "Our proposed framework\n\n\n\n\nWe are currently at the green-colored stage."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#first-stage-estimating-true-demand-under-censorship-using-tobit-kalman-filtering-and-conformal-prediction",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#first-stage-estimating-true-demand-under-censorship-using-tobit-kalman-filtering-and-conformal-prediction",
    "title": "WHAT WAS LOST",
    "section": "First stage: estimating true demand under censorship using tobit kalman filtering and conformal prediction",
    "text": "First stage: estimating true demand under censorship using tobit kalman filtering and conformal prediction"
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#data-exploration",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#data-exploration",
    "title": "WHAT WAS LOST",
    "section": "Data exploration",
    "text": "Data exploration\n\n\n\n\n\nActual vs. observed demand for one representative series per type × category, with disruptions and censoring shaded."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#experiment-setup",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#experiment-setup",
    "title": "WHAT WAS LOST",
    "section": "Experiment setup",
    "text": "Experiment setup"
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#actual-vs-forecasted-demand-distributions",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#actual-vs-forecasted-demand-distributions",
    "title": "WHAT WAS LOST",
    "section": "Actual vs forecasted demand distributions",
    "text": "Actual vs forecasted demand distributions\n\n\n\n\n\n\n\n\nFigure 2: Actual vs. forecasted demand distributions for the different forecasting methods."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#overall-forecasting-and-inventory-performance-across-models",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#overall-forecasting-and-inventory-performance-across-models",
    "title": "WHAT WAS LOST",
    "section": "Overall forecasting and inventory performance across models",
    "text": "Overall forecasting and inventory performance across models\n\n\n\n\nMethod\nMASE (mean)\nPin Ball Loss - q95 (mean)\nCSL (mean)\nLost Sales Rate (mean)\nInventory coverage (mean)\n\n\n\n\nTKF CP\n0.87\n47.61\n0.86\n0.14\n5.25\n\n\nMoving Average\n1.06\n72.65\n0.82\n0.18\n19.6\n\n\nLinear Regression\n1.08\n73.86\n0.82\n0.16\n2.55\n\n\nNaive\n1.21\n78.89\n0.84\n0.16\n123.38"
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#performance-evaluation---nemenyi-test",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#performance-evaluation---nemenyi-test",
    "title": "WHAT WAS LOST",
    "section": "Performance evaluation - Nemenyi test",
    "text": "Performance evaluation - Nemenyi test\n\n\n\n\n\n\n\n\nFigure 3: Average ranks of forecasting methods with 95% confidence intervals based on the Nemenyi test for all metrics. Lower ranks indicate better performance."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#performance-evaluation---forecasting",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#performance-evaluation---forecasting",
    "title": "WHAT WAS LOST",
    "section": "Performance evaluation - forecasting",
    "text": "Performance evaluation - forecasting\n\n\n\n\n\n\n\n\nFigure 4: Forecasting metrics for each series type for the different forecasting methods."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#performance-evaluation---inventory",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#performance-evaluation---inventory",
    "title": "WHAT WAS LOST",
    "section": "Performance evaluation - inventory",
    "text": "Performance evaluation - inventory\n\n\n\n\n\n\n\n\nFigure 5: Inentory metrics for each series type for the different forecasting methods."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#way-forward",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#way-forward",
    "title": "WHAT WAS LOST",
    "section": "Way forward",
    "text": "Way forward\n\n\n\n\nDevelop a more comprehensive inventory policy using forecasted quantiles → Incorporate uncertainty directly into order decisions\n\nExtend empirical model with external covariates → Account for special events, disruptions, and policy shifts\n\nConduct lab experiment with real demand planners → Measure how model recommendations affect decision-making\n\n\n\n\nImage generated using ChatGPT."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#materials",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#materials",
    "title": "WHAT WAS LOST",
    "section": "Materials",
    "text": "Materials\n\n\nYou can find the slides here."
  },
  {
    "objectID": "lab/intro_python/materials/index.html",
    "href": "lab/intro_python/materials/index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda\n\n\n\n\n\nUnderstanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project\n\n\n\n\nPython is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more.\n\n\n\n\nEasy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike\n\n\n\n\nPython can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer.\n\n\n\nAn IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!\n\n\n\nGoogle Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!\n\n\n\nAnaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!\n\n\n\nGetting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item\n\n\n\n\n\n\n# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data.\n\n\n\n\n\nTo ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#section-1-python-and-ide-essentials",
    "href": "lab/intro_python/materials/index.html#section-1-python-and-ide-essentials",
    "title": "Introduction to Python",
    "section": "",
    "text": "Introduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nIntroduction to Jupyter Lab, VS Code, Google Colab and Anaconda"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#section-2-working-with-jupyter-lab",
    "href": "lab/intro_python/materials/index.html#section-2-working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Understanding the basic syntax\nHow to write and execute code in Python\nSetting up and organizing a Python project"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#what-is-python",
    "href": "lab/intro_python/materials/index.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a versatile, high-level programming language known for its simplicity and readability. It is widely used for web development, data analysis, artificial intelligence, scientific computing, and more."
  },
  {
    "objectID": "lab/intro_python/materials/index.html#why-learn-python",
    "href": "lab/intro_python/materials/index.html#why-learn-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Easy to read and write\nExtensive libraries and frameworks\nStrong community support\nGreat for beginners and professionals alike"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#installing-python",
    "href": "lab/intro_python/materials/index.html#installing-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python can be installed on various operating systems, including Windows, macOS, and Linux.\nStep 1: Download the Installer\n\nVisit the official Python website: https://www.python.org/\nNavigate to the Downloads section and choose the appropriate installer for your operating system.\n\nStep 2: Run the Installer - Open the downloaded installer file. - Make sure to check the box that says “Add Python to PATH” during the installation process. This makes it easier to run Python from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation:\npython --version\n\nYou should see the version of Python you installed.\n\nCongratulations! Python is now installed on your computer."
  },
  {
    "objectID": "lab/intro_python/materials/index.html#setting-up-an-integrated-development-environment-ide",
    "href": "lab/intro_python/materials/index.html#setting-up-an-integrated-development-environment-ide",
    "title": "Introduction to Python",
    "section": "",
    "text": "An IDE is a software application that provides comprehensive facilities to programmers for software development.\nThere are many IDEs available for Python. Two popular choices are VS Code and Jupyter Lab.\n\nInstalling VS Code\n\n\nVisit the Visual Studio Code website: https://code.visualstudio.com/\nDownload and install the version for your operating system.\nOnce installed, you can add Python support by installing the Python extension:\n\nOpen VS Code.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window.\nSearch for “Python” and install the extension provided by Microsoft.\nOpen a new Python file or notebook and start coding!\n\nTutorial: https://code.visualstudio.com/docs/python/python-tutorial\nKey Features:\n\nExtensions: VS Code has a vast library of extensions that enhance its functionality.\nIntegrated Terminal: You can run command-line tasks directly in VS Code.\n\n\n\nInstalling Jupyter Lab\n\n\nJupyter Lab is a web-based interactive computing environment.\nTo install Jupyter Lab, you need to have Python installed. You can use pip, the Python package installer, to install Jupyter Lab:\n    pip install jupyterlab\nTo start Jupyter Lab, open your command prompt (Windows) or terminal (macOS/Linux) and type: bash         Jupyter Lab\nThis will open a new tab in your default web browser, where you can create and run Jupyter Labs.\nTutorial: https://jupyter.org/install\nKey Features:\n\nInteractive output: You can run code in real-time and see the results immediately.\nMarkdown support: You can write rich text, which makes it great for creating tutorials and educational materials.\nVisualizations: You can embed visualizations and plots directly in the notebook.\n\n\nNow you have both VS Code and Jupyter Lab set up for your Python development!"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#setting-up-google-colab",
    "href": "lab/intro_python/materials/index.html#setting-up-google-colab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Google Colab, short for “Collaboratory,” is a free Jupyter Lab environment that runs entirely in the cloud. It is perfect for those who want to run Python code without having to set up anything on their local machines.\nStep 1: Accessing Google Colab - Open your web browser and go to: https://colab.research.google.com/\nStep 2: Creating a New Notebook - On the Google Colab homepage, you can either open an existing notebook or create a new one. - To create a new notebook, click on the “New Notebook” button.\nStep 3: Using Google Colab - You can start writing and running Python code right away. - Google Colab provides free access to GPUs and TPUs, which can be used for more intensive computations.\nStep 4: Saving Your Work - You can save your notebooks directly to Google Drive, which makes it easy to access them from any device. - To save your notebook, go to “File” -&gt; “Save a copy in Drive…”\nKey Features: - No installation required: You can start coding immediately without any local setup. - Cloud-based: Your notebooks are stored in Google Drive, making them easy to share and collaborate on. - Free access to GPUs and TPUs: This is particularly useful for machine learning and data science tasks.\nNow you can use Google Colab to run Python code from anywhere, without worrying about setting up your local environment!"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#setting-up-anaconda-additional-resources",
    "href": "lab/intro_python/materials/index.html#setting-up-anaconda-additional-resources",
    "title": "Introduction to Python",
    "section": "",
    "text": "Anaconda is a distribution of Python and R for scientific computing and data science. It comes with many popular data science and machine learning libraries, as well as Jupyter Lab.\nStep 1: Download Anaconda - Visit the Anaconda website: https://www.anaconda.com/products/distribution - Download the Anaconda installer for your operating system.\nStep 2: Install Anaconda - Open the downloaded installer and follow the instructions. - During the installation, you can choose to add Anaconda to your PATH environment variable. This makes it easier to run Anaconda commands from the command line.\nStep 3: Verify the Installation - Open your command prompt (Windows) or terminal (macOS/Linux). - Type the following command to verify the installation: bash         conda --version - You should see the version of Conda that is installed.\nStep 4: Launching Jupyter Lab from Anaconda - Anaconda Navigator is a graphical user interface that comes with Anaconda. It allows you to launch Jupyter Lab easily. - Open Anaconda Navigator from your applications menu. - In Anaconda Navigator, you will see Jupyter Lab listed. Click “Launch” to open it.\nNow you have Anaconda installed, and you can use it to manage your Python environments and packages, as well as run Jupyter Labs!"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#working-with-jupyter-lab",
    "href": "lab/intro_python/materials/index.html#working-with-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "Getting Started 1. Launching Jupyter Lab: - Open your command prompt (Windows) or terminal (macOS/Linux). - Type jupyter lab and press Enter. - A new tab will open in your default web browser showing the Jupyter Lab interface.\n\nCreating a New Notebook:\n\nClick on the “New” button and select “Python 3” to create a new notebook.\nYou will see a new notebook with an empty code cell.\n\nWriting and Running Code:\n\nWrite your Python code in the code cells.\nExecute the code by pressing Shift + Enter or clicking the “Run” button.\nExample:\n\n print(\"Hello, Jupyter Lab!\")\nUsing Markdown Cells:\n\nTo add text, headers, lists, and other non-code content, use Markdown cells.\nChange the cell type to Markdown by selecting “Markdown” from the dropdown menu.\nExample of a Markdown cell:\n# This is a Header\n- This is a list item\n    - Another list item"
  },
  {
    "objectID": "lab/intro_python/materials/index.html#example-workflow-in-jupyter-lab",
    "href": "lab/intro_python/materials/index.html#example-workflow-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "# Install necessary libraries\n!pip install numpy pandas matplotlib seaborn\n\n# Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Load a sample dataset from seaborn\ndata = sns.load_dataset('penguins')\n\n# Display the first few rows of the dataset\nprint(data.head())\n# Perform some basic data analysis\nsummary = data.describe()\nprint(summary)\n# Create a simple plot\nplt.figure(figsize=(10, 6))\nplt.scatter(data['bill_length_mm'], data['bill_depth_mm'])\nplt.title('Bill Length vs. Bill Depth')\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.show()\n\n\n\nWe imported necessary libraries.\nWe loaded a sample dataset from the seaborn library and displayed the first few rows.\nWe performed a basic data analysis using the describe method.\nWe visualized the relationship between bill length and bill depth.\n\n\n\n\n\nInteractive Coding: Run code and see results instantly, making it great for experimentation.\nDocumentation: Combine code and narrative text to create comprehensive and understandable documents.\nVisualization: Easily integrate plots and charts to visualize your data."
  },
  {
    "objectID": "lab/intro_python/materials/index.html#setting-up-the-working-directory-in-jupyter-lab",
    "href": "lab/intro_python/materials/index.html#setting-up-the-working-directory-in-jupyter-lab",
    "title": "Introduction to Python",
    "section": "",
    "text": "To ensure that Jupyter Lab uses the project directory as the working directory, you can use the os module to change the current working directory.\n\n\n\nImport the os Module:\n\nThe os module provides a way of using operating system-dependent functionality.\n\nDefine the Project Directory:\n\nSpecify the path to your project directory.\n\nChange the Current Working Directory:\n\nUse os.chdir() to change the current working directory to the project directory.\n\nVerify the Current Working Directory:\n\nUse os.getcwd() to print the current working directory and ensure it’s set correctly.\n\n\n\n\n\nUse python console or terminal.\nimport os\n\n# Define project directory and subdirectories\nproject_dir = 'my_python_project'\nfolders = ['data', 'scripts', 'notebooks']\n\n# Create the main project directory\nif not os.path.exists(project_dir):\n    os.makedirs(project_dir)\n\n# Create subdirectories\nfor folder in folders:\n    path = os.path.join(project_dir, folder)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# Check the created structure\n!tree my_python_project\nGo to your main project folder and open the terminal.\nimport os\n\n# Define the project directory\nproject_dir = os.getcwd()\n\n# Set the working directory to the project directory\nos.chdir(project_dir)\n\n# Check the current working directory\nprint(\"Current Working Directory:\", os.getcwd())\nexample: save a file\n# Save a Sample Dataset in the data Folder \nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 22, 23],\n    'Grade': ['A', 'B', 'A']\n}\n\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Define the path to save the CSV file\ncsv_file_path = os.path.join('data', 'students_data.csv')\n\n# Save the DataFrame as a CSV file\ndf.to_csv(csv_file_path, index=False)\n\n# Verify that the file has been saved\nprint(f\"DataFrame saved as CSV file at: {csv_file_path}\")"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html",
    "href": "lab/epss_training/slides/epss_training.html",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "2025-03-17\n\n\n\n\n\n\n\n\n\n\n\nYou should be comfortable with R and Python coding.\nThis is not a theory-based course—we will not derive formulas or mathematical proofs.\nOur focus is on practical time series forecasting: understanding when and how to use different tools effectively.\n\n\n\n\n\nData wrangling and basic feature engineering\nTime series visualizations\nTraditional forecasting models\nAdvanced forecasting models\nPerformance evaluation\n\n\n\n\n\nHandling missing values\nAdvanced feature engineering\nTime series cross-validation\nHyperparameter tuning\n\n\n\n\nYou can find the workshop materials here.\nNote: These materials are based on A Novel Hybrid Approach to Contraceptive Demand Forecasting Research, F4SG: Africast training and F4SG Learning Labs trainings.\n\nRecommended readings:\n\nDemand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa.\nForecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos.\nForecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM) by Ivan Svetunkov.\n\n\n\n\n\nWhat is forecasting?\nPrepare your data\nExplore your data\nForecast modelling\nEvaluating the model performances\nAdvance forecasting models\nOther forecasting models in family planning supply chains"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#assumptions",
    "href": "lab/epss_training/slides/epss_training.html#assumptions",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "You should be comfortable with R and Python coding.\nThis is not a theory-based course—we will not derive formulas or mathematical proofs.\nOur focus is on practical time series forecasting: understanding when and how to use different tools effectively."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#what-we-will-cover",
    "href": "lab/epss_training/slides/epss_training.html#what-we-will-cover",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "Data wrangling and basic feature engineering\nTime series visualizations\nTraditional forecasting models\nAdvanced forecasting models\nPerformance evaluation"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#what-we-will-not-cover",
    "href": "lab/epss_training/slides/epss_training.html#what-we-will-not-cover",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "Handling missing values\nAdvanced feature engineering\nTime series cross-validation\nHyperparameter tuning"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#materials",
    "href": "lab/epss_training/slides/epss_training.html#materials",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "You can find the workshop materials here.\nNote: These materials are based on A Novel Hybrid Approach to Contraceptive Demand Forecasting Research, F4SG: Africast training and F4SG Learning Labs trainings.\n\nRecommended readings:\n\nDemand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa.\nForecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos.\nForecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM) by Ivan Svetunkov."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#outline",
    "href": "lab/epss_training/slides/epss_training.html#outline",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "What is forecasting?\nPrepare your data\nExplore your data\nForecast modelling\nEvaluating the model performances\nAdvance forecasting models\nOther forecasting models in family planning supply chains"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#what-is-a-forecast",
    "href": "lab/epss_training/slides/epss_training.html#what-is-a-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What is a FORECAST?",
    "text": "What is a FORECAST?\nAn estimation of the future based on all of the information available at the time when we generate the forecast;\n\nhistorical data,\nknowledge of any future events that might impact the forecasts."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#what-is-time-series-data",
    "href": "lab/epss_training/slides/epss_training.html#what-is-time-series-data",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What is time series data?",
    "text": "What is time series data?\n\nTime series consist of sequences of observations collected over time.\nTime series forecasting is estimating how the sequence of observations will continue into the future."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#what-to-forecast",
    "href": "lab/epss_training/slides/epss_training.html#what-to-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What to FORECAST?",
    "text": "What to FORECAST?\nUnderstanding needs! Identify decisions that need forecasting support!\n\nForecast variable/s\nTime granularity\nForecast horizon\nFrequency\nStructure/hierarchy"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#forecasting-workflow",
    "href": "lab/epss_training/slides/epss_training.html#forecasting-workflow",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Forecasting workflow",
    "text": "Forecasting workflow\n\nStep 1: Problem definition\nStep 2: Gathering information\nStep 3: Preliminary (exploratory) analysis\nStep 4: Choosing and fitting models\nStep 5: Evaluating and using a forecasting model"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#tidy-forecasting-workflow",
    "href": "lab/epss_training/slides/epss_training.html#tidy-forecasting-workflow",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Tidy forecasting workflow",
    "text": "Tidy forecasting workflow\n\n\n\n\n\nRead more at Demand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#loading-libraries",
    "href": "lab/epss_training/slides/epss_training.html#loading-libraries",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Loading libraries",
    "text": "Loading libraries\nWe use the fpp3 package in this workshop, which provides all the necessary packages for data manipulation, plotting, and forecasting.\n\n# Define required packages\npackages &lt;- c(\"tidyverse\", \"fable\", \"tsibble\", \"feasts\", 'zoo')\n\n# Install missing packages\nmissing_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(missing_packages)) {\n  suppressWarnings(suppressMessages(install.packages(missing_packages)))\n}\n\n# Load libraries quietly\nsuppressWarnings(suppressMessages({\n  library(tidyverse) # Data manipulation and plotting functions\n  library(fable) # Time series manipulation\n  library(tsibble) # Forecasting functions\n  library(feasts) # Time series graphics and statistics\n}))\n\nRead more at Forecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#preparing-the-data",
    "href": "lab/epss_training/slides/epss_training.html#preparing-the-data",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Preparing the data",
    "text": "Preparing the data\nIn this workshop, we are using tsibble objects. They provide a data infrastructure for tidy temporal data with wrangling tools, adapting the tidy data principles.\nIn tsibble:\n\nIndex: time information about the observation\nMeasured variable(s): numbers of interest\nKey variable(s): set of variables that define observational units over time\nIt works with tidyverse functions."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#read-csv-file",
    "href": "lab/epss_training/slides/epss_training.html#read-csv-file",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Read csv file",
    "text": "Read csv file\n\nmed_qty &lt;- read.csv('data/med_qty.csv')\nmed_qty |&gt; head(10)\n\n       date hub_id product_id quantity_issued\n1  2017 Jul  hub_4  product_1              60\n2  2017 Jul  hub_4  product_6            5200\n3  2017 Jul  hub_7  product_1               8\n4  2017 Jul  hub_7  product_5             120\n5  2017 Jul  hub_8  product_7              10\n6  2017 Jul hub_10  product_1             343\n7  2017 Jul hub_10  product_2              53\n8  2017 Jul hub_10  product_3              26\n9  2017 Jul hub_10  product_4            1710\n10 2017 Jul hub_10  product_5            1340\n\n\nDo you think the med_qty data set is a tidy data?"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#check-for-na-and-duplicates",
    "href": "lab/epss_training/slides/epss_training.html#check-for-na-and-duplicates",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check for NA and duplicates",
    "text": "Check for NA and duplicates\n\n# check NAs\n\nanyNA(med_qty)\n\n[1] FALSE\n\n\n\n#check duplicates\n\nmed_qty |&gt;  \n  duplicated() |&gt;  \n  sum() \n\n[1] 0"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#create-tsibble",
    "href": "lab/epss_training/slides/epss_training.html#create-tsibble",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Create tsibble",
    "text": "Create tsibble\n\nmed_tsb &lt;- med_qty |&gt;  \n  mutate(date = yearmonth(date)) |&gt;  # convert chr to date format\n  as_tsibble(index = date, key = c(hub_id, product_id))\n\nmed_tsb \n\n# A tsibble: 6,745 x 4 [1M]\n# Key:       hub_id, product_id [133]\n       date hub_id product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Aug hub_1  product_1              721\n 2 2017 Sep hub_1  product_1              795\n 3 2017 Oct hub_1  product_1             1720\n 4 2017 Nov hub_1  product_1              911\n 5 2017 Dec hub_1  product_1              314\n 6 2018 Jan hub_1  product_1             6913\n 7 2018 Feb hub_1  product_1             2988\n 8 2018 Mar hub_1  product_1             7120\n 9 2018 Apr hub_1  product_1             3122\n10 2018 May hub_1  product_1            11737\n# ℹ 6,735 more rows\n\n\n\nWhat is the temporal granularity of med_tsb?\nHow many time series do we have in med_tsb?"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values",
    "href": "lab/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check temporal gaps (implicit missing values)",
    "text": "Check temporal gaps (implicit missing values)\n\nhas_gaps(med_tsb) |&gt; head(3) #check gaps\n\n# A tibble: 3 × 3\n  hub_id product_id .gaps\n  &lt;chr&gt;  &lt;chr&gt;      &lt;lgl&gt;\n1 hub_1  product_1  TRUE \n2 hub_1  product_2  TRUE \n3 hub_1  product_3  TRUE \n\nscan_gaps(med_tsb) |&gt; head(3) # show gaps\n\n# A tsibble: 3 x 3 [1M]\n# Key:       hub_id, product_id [1]\n  hub_id product_id     date\n  &lt;chr&gt;  &lt;chr&gt;         &lt;mth&gt;\n1 hub_1  product_1  2018 Jul\n2 hub_1  product_1  2018 Aug\n3 hub_1  product_1  2018 Sep\n\ncount_gaps(med_tsb) |&gt; head(3) # count gaps\n\n# A tibble: 3 × 5\n  hub_id product_id    .from      .to    .n\n  &lt;chr&gt;  &lt;chr&gt;         &lt;mth&gt;    &lt;mth&gt; &lt;int&gt;\n1 hub_1  product_1  2018 Jul 2021 Feb    32\n2 hub_1  product_1  2021 Jul 2022 Sep    15\n3 hub_1  product_2  2018 Sep 2018 Sep     1"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-1",
    "href": "lab/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check temporal gaps (implicit missing values)",
    "text": "Check temporal gaps (implicit missing values)\nIf there is any gap, then we fill it.\n\nmed_tsb |&gt; fill_gaps(quantity_issued=0L) # we can fill it with zero\n\n# A tsibble: 8,795 x 4 [1M]\n# Key:       hub_id, product_id [133]\n       date hub_id product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Aug hub_1  product_1              721\n 2 2017 Sep hub_1  product_1              795\n 3 2017 Oct hub_1  product_1             1720\n 4 2017 Nov hub_1  product_1              911\n 5 2017 Dec hub_1  product_1              314\n 6 2018 Jan hub_1  product_1             6913\n 7 2018 Feb hub_1  product_1             2988\n 8 2018 Mar hub_1  product_1             7120\n 9 2018 Apr hub_1  product_1             3122\n10 2018 May hub_1  product_1            11737\n# ℹ 8,785 more rows"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-2",
    "href": "lab/epss_training/slides/epss_training.html#check-temporal-gaps-implicit-missing-values-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Check temporal gaps (implicit missing values)",
    "text": "Check temporal gaps (implicit missing values)\nNote: Since the main focus of this study is to provide foundational knowledge on forecasting, we will filter out time series with many missing values and then fill the remaining gaps using na.interp() function (Read more).\n\nitem_ids &lt;- med_tsb |&gt; \n  count_gaps() |&gt; \n  group_by(hub_id, product_id) |&gt; \n  summarise(.n = max(.n), .groups = 'drop') |&gt; \n  filter(.n  &gt; 1) |&gt; \n  mutate(id = paste0(hub_id,'-',product_id)) |&gt; \n  pull(id) # filtering the item ids\n\nmed_tsb_filter &lt;- med_tsb |&gt; \n  mutate(id = paste0(hub_id,'-',product_id)) |&gt; \n  group_by(hub_id, product_id) |&gt;\n  mutate(num_observations = n()) |&gt; \n  filter(!id %in% item_ids & num_observations &gt;59) |&gt;   # we have cold starts and discontinuations. \n  fill_gaps(quantity_issued = 1e-6, .full = TRUE) |&gt;   # Replace NAs with a small value\n  select(-id, -num_observations) |&gt; \n  mutate(quantity_issued = if_else(is.na(quantity_issued), \n                                   exp(\n                                     forecast::na.interp(\n                                     ts(log(quantity_issued), frequency = 12))), \n                                   quantity_issued))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble",
    "href": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use the filter() function to select rows.\n\nmed_tsb |&gt; \n  filter(hub_id == 'hub_10') \n\n# A tsibble: 417 x 4 [1M]\n# Key:       hub_id, product_id [7]\n       date hub_id product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Jul hub_10 product_1              343\n 2 2017 Aug hub_10 product_1               67\n 3 2017 Sep hub_10 product_1              127\n 4 2017 Oct hub_10 product_1              287\n 5 2017 Nov hub_10 product_1              759\n 6 2017 Dec hub_10 product_1              181\n 7 2018 Jan hub_10 product_1             7015\n 8 2018 Feb hub_10 product_1              840\n 9 2018 Mar hub_10 product_1             4111\n10 2018 Apr hub_10 product_1             1910\n# ℹ 407 more rows"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-1",
    "href": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use the select() function to select columns.\n\nmed_tsb |&gt; \n  filter(hub_id == 'hub_10') |&gt; \n  select(date, product_id, quantity_issued)\n\n# A tsibble: 417 x 3 [1M]\n# Key:       product_id [7]\n       date product_id quantity_issued\n      &lt;mth&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1 2017 Jul product_1              343\n 2 2017 Aug product_1               67\n 3 2017 Sep product_1              127\n 4 2017 Oct product_1              287\n 5 2017 Nov product_1              759\n 6 2017 Dec product_1              181\n 7 2018 Jan product_1             7015\n 8 2018 Feb product_1              840\n 9 2018 Mar product_1             4111\n10 2018 Apr product_1             1910\n# ℹ 407 more rows"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-2",
    "href": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use group_by() function to group over keys. We can use the summarise() function to summarise over keys.\n\nmed_tsb |&gt; \n  group_by(product_id) |&gt; \n  summarise(total_quantity_issued = sum(quantity_issued), .groups = 'drop')\n\n# A tsibble: 471 x 3 [1M]\n# Key:       product_id [7]\n   product_id     date total_quantity_issued\n   &lt;chr&gt;         &lt;mth&gt;                 &lt;dbl&gt;\n 1 product_1  2017 Jul                   691\n 2 product_1  2017 Aug                 18855\n 3 product_1  2017 Sep                 21654\n 4 product_1  2017 Oct                 16456\n 5 product_1  2017 Nov                 19694\n 6 product_1  2017 Dec                 63107\n 7 product_1  2018 Jan                 66703\n 8 product_1  2018 Feb                 53012\n 9 product_1  2018 Mar                 82566\n10 product_1  2018 Apr                 56913\n# ℹ 461 more rows"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-3",
    "href": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use the mutate() function to create new variables.\n\nmed_tsb |&gt; \n  mutate(quarter = yearquarter(date))\n\n# A tsibble: 6,745 x 5 [1M]\n# Key:       hub_id, product_id [133]\n       date hub_id product_id quantity_issued quarter\n      &lt;mth&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;   &lt;qtr&gt;\n 1 2017 Aug hub_1  product_1              721 2017 Q3\n 2 2017 Sep hub_1  product_1              795 2017 Q3\n 3 2017 Oct hub_1  product_1             1720 2017 Q4\n 4 2017 Nov hub_1  product_1              911 2017 Q4\n 5 2017 Dec hub_1  product_1              314 2017 Q4\n 6 2018 Jan hub_1  product_1             6913 2018 Q1\n 7 2018 Feb hub_1  product_1             2988 2018 Q1\n 8 2018 Mar hub_1  product_1             7120 2018 Q1\n 9 2018 Apr hub_1  product_1             3122 2018 Q2\n10 2018 May hub_1  product_1            11737 2018 Q2\n# ℹ 6,735 more rows"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-4",
    "href": "lab/epss_training/slides/epss_training.html#data-wrangaling-using-tsibble-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Data wrangaling using tsibble",
    "text": "Data wrangaling using tsibble\nWe can use index_by() function to group over index We can use the summarise() function to summarise over index.\n\nmed_tsb |&gt; \n  mutate(quarter = yearquarter(date)) |&gt; \n  index_by(quarter) |&gt; \n  summarise(total_quantity_issues = sum(quantity_issued))\n\n# A tsibble: 24 x 2 [1Q]\n   quarter total_quantity_issues\n     &lt;qtr&gt;                 &lt;dbl&gt;\n 1 2017 Q3               2103843\n 2 2017 Q4               2811202\n 3 2018 Q1               2511488\n 4 2018 Q2               3433726\n 5 2018 Q3               1738860\n 6 2018 Q4               2934886\n 7 2019 Q1               2452192\n 8 2019 Q2               1640048\n 9 2019 Q3               2170015\n10 2019 Q4               3045525\n# ℹ 14 more rows"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#time-series-patterns",
    "href": "lab/epss_training/slides/epss_training.html#time-series-patterns",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Time series patterns",
    "text": "Time series patterns\n\nLevel: The level of a time series describes the center of the series.\nTrend: A trend describes predictable increases or decreases in the level of a series.\nSeasonal: Seasonality is a consistent pattern that repeats over a fixed cycle. pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\nCyclic: A pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years)."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#time-series-patterns-1",
    "href": "lab/epss_training/slides/epss_training.html#time-series-patterns-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Time series patterns",
    "text": "Time series patterns\n\n\n\n\n\n\n\n\n\nRead more at Demand forecasting for executives and professionals by Bahman Rostami-Tabar, Enno Siemsen, and Stephan Kolassa."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#additive-vs.-multiplicative-seasonality",
    "href": "lab/epss_training/slides/epss_training.html#additive-vs.-multiplicative-seasonality",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Additive vs. multiplicative seasonality",
    "text": "Additive vs. multiplicative seasonality\n\n\n\n\n\n\n\n\n\n\nWhen we have multiplicative seasonality, we can use transformations to convert multiplicative seasonality into additive seasonality.\nIn this training, we are not discussing time series transformations. You can read more about it at Transformations and adjustments."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#time-plots",
    "href": "lab/epss_training/slides/epss_training.html#time-plots",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Time plots",
    "text": "Time plots\nYou can create time plot using autoplot() function.\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_2') |&gt; \n  autoplot(quantity_issued) +\n  labs(\n    x = \"Date\",\n    y = \"Quantity Issued\"\n  ) +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#are-time-plots-best",
    "href": "lab/epss_training/slides/epss_training.html#are-time-plots-best",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Are time plots best?",
    "text": "Are time plots best?\n\nmed_tsb_filter |&gt; \n  mutate(id = paste0(hub_id, product_id)) |&gt; \n  ggplot(aes(x = date, y = quantity_issued, group = id)) +\n  geom_line() +\n  labs(\n    x = \"Date\",\n    y = \"Quantity Issued\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#seasonal-plots",
    "href": "lab/epss_training/slides/epss_training.html#seasonal-plots",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal plots",
    "text": "Seasonal plots\n\nData plotted against the individual seasons in which the data were observed (In this case a “season” is a month).\nEnables the underlying seasonal pattern to be seen more clearly, and also allows any substantial departures from the seasonal pattern to be easily identified.\nYou can create seasonal plots using gg_season() function."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#seasonal-plots-1",
    "href": "lab/epss_training/slides/epss_training.html#seasonal-plots-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal plots",
    "text": "Seasonal plots\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  gg_season(quantity_issued, labels = \"both\") +\n  ylab(\"Quantity issued\") +\n  ggtitle(\"Seasonal plot\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#seasonal-sub-series-plots",
    "href": "lab/epss_training/slides/epss_training.html#seasonal-sub-series-plots",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal sub series plots",
    "text": "Seasonal sub series plots\n\nData for each season collected together in time plot as separate time series.\nEnables the underlying seasonal pattern to be seen clearly, and changes in seasonality over time to be visualized.\nYou can create seasonal sub series plots using gg_subseries() function."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#seasonal-sub-series-plots-1",
    "href": "lab/epss_training/slides/epss_training.html#seasonal-sub-series-plots-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal sub series plots",
    "text": "Seasonal sub series plots\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  gg_subseries(quantity_issued) +\n  ylab(\"Quantity issued\") +\n  ggtitle(\"Seasonal sub series plot\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend",
    "href": "lab/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Strength of seasonality and trend",
    "text": "Strength of seasonality and trend\n\nWe used STL decomposition for additive decompositions.\nA multiplicative decomposition can be obtained by first taking logs of the data, then back-transforming the components.\nDecompositions that are between additive and multiplicative can be obtained using a Box-Cox transformation of the data.\nRead more at STL decomposition."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend-1",
    "href": "lab/epss_training/slides/epss_training.html#strength-of-seasonality-and-trend-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Strength of seasonality and trend",
    "text": "Strength of seasonality and trend\nSTL Decomposition\n\\[\ny_t = T_t + S_t + R_t\n\\]\nSeasonal Strength\n\\[\n\\max \\left( 0, 1 - \\frac{\\text{Var}(R_t)}{\\text{Var}(S_t + R_t)} \\right)\n\\]\nTrend Strength\n\\[\n\\max \\left( 0, 1 - \\frac{\\text{Var}(R_t)}{\\text{Var}(T_t + R_t)} \\right)\n\\]"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#feature-extraction-and-statistics",
    "href": "lab/epss_training/slides/epss_training.html#feature-extraction-and-statistics",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature extraction and statistics",
    "text": "Feature extraction and statistics\nWe can use features() function to extract the strength of trend and seasonality.\n\nmed_tsb_filter |&gt; \n  features(quantity_issued, feat_stl)\n\n# A tibble: 21 × 11\n   hub_id product_id trend_strength seasonal_strength_year seasonal_peak_year\n   &lt;chr&gt;  &lt;chr&gt;               &lt;dbl&gt;                  &lt;dbl&gt;              &lt;dbl&gt;\n 1 hub_1  product_2           0.261                 0.503                   6\n 2 hub_1  product_5           0.250                 0.438                   0\n 3 hub_10 product_5           0.366                 0.0911                  0\n 4 hub_11 product_2           0.624                 0.407                  11\n 5 hub_11 product_5           0.352                 0.244                   4\n 6 hub_11 product_7           0.181                 0.196                   7\n 7 hub_13 product_5           0.196                 0.402                   0\n 8 hub_14 product_5           0.595                 0.229                   9\n 9 hub_16 product_2           0.233                 0.238                   7\n10 hub_16 product_5           0.416                 0.272                   0\n# ℹ 11 more rows\n# ℹ 6 more variables: seasonal_trough_year &lt;dbl&gt;, spikiness &lt;dbl&gt;,\n#   linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt;"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#feature-extraction-and-statistics-1",
    "href": "lab/epss_training/slides/epss_training.html#feature-extraction-and-statistics-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature extraction and statistics",
    "text": "Feature extraction and statistics\n\nmed_tsb_filter |&gt; \n  features(quantity_issued, feat_stl) |&gt; \n  ggplot(aes(x = trend_strength, y = seasonal_strength_year, shape = product_id)) +\n  geom_point(size = 2) + \n  ylab(\"Seasonal strength\") +\n  xlab(\"Trend strength\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation",
    "href": "lab/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Lag plots and autocorrelation",
    "text": "Lag plots and autocorrelation\n\nEach graph shows \\(y_t\\) plotted against \\(y_{t-k}\\) for different values of \\(k\\).\n\nThe autocorrelations are the correlations associated with these scatterplots: \\(\\text{Corr}(y_t, y_{t-k})\\)\nYou can create lag plots using gglag() function.\nThese values indicate the relationship between current and past observations in a time series."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation-1",
    "href": "lab/epss_training/slides/epss_training.html#lag-plots-and-autocorrelation-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Lag plots and autocorrelation",
    "text": "Lag plots and autocorrelation\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt;\n  gg_lag(quantity_issued, lags = 1:12, geom='point') +\n  ylab(\"Quantity issued\") +\n  xlab(\"Lag (Quantity issued, n)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n         panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#autocorrelation",
    "href": "lab/epss_training/slides/epss_training.html#autocorrelation",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocovariance and autocorrelation: measure linear relationship between lagged values of a time series y.\nWe denote the sample autocovariance at lag \\(k\\) by \\(c_k\\) and the sample autocorrelation at lag \\(k\\) by \\(r_k\\). Then, we define:\n\n\\(c_k = \\frac{1}{T} \\sum_{t=k+1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y})\\)\n\\(r_k = \\frac{c_k}{c_0}\\)\nwhere \\(c_0\\) is the variance of the time series.\n\n\\(r_1\\) indicates how successive values of \\(y\\) relate to each other.\n\\(r_2\\) indicates how \\(y\\) values two periods apart relate to each other.\n\\(r_k\\) is almost the same as the sample correlation between \\(y_t\\) and \\(y_{t-k}\\)."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#autocorrelation-1",
    "href": "lab/epss_training/slides/epss_training.html#autocorrelation-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  ACF(quantity_issued, lag_max = 24)\n\n# A tsibble: 24 x 4 [1M]\n# Key:       hub_id, product_id [1]\n   hub_id product_id      lag      acf\n   &lt;chr&gt;  &lt;chr&gt;      &lt;cf_lag&gt;    &lt;dbl&gt;\n 1 hub_14 product_5        1M  0.681  \n 2 hub_14 product_5        2M  0.485  \n 3 hub_14 product_5        3M  0.320  \n 4 hub_14 product_5        4M  0.160  \n 5 hub_14 product_5        5M  0.195  \n 6 hub_14 product_5        6M  0.162  \n 7 hub_14 product_5        7M  0.0956 \n 8 hub_14 product_5        8M  0.0540 \n 9 hub_14 product_5        9M  0.00739\n10 hub_14 product_5       10M -0.0665 \n# ℹ 14 more rows"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#autocorrelation-2",
    "href": "lab/epss_training/slides/epss_training.html#autocorrelation-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_14' & product_id == 'product_5') |&gt; \n  ACF(quantity_issued, lag_max = 36) |&gt; \n  autoplot() +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))\n\n\n\n\n\n\n\n\nWhat autocorrelation will tell us? Which key features could be highlighted by ACF?"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#autocorrelation-3",
    "href": "lab/epss_training/slides/epss_training.html#autocorrelation-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nWhen data have a trend, the autocorrelations for small lags tend to be large and positive.\nWhen data are seasonal, the autocorrelations will be larger at the seasonal lags (i.e., at multiples of the seasonal frequency)\nWhen data are trended and seasonal, you see a combination of these effects."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#naive",
    "href": "lab/epss_training/slides/epss_training.html#naive",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Naive",
    "text": "Naive\nSimplest forecasting method using last observation as forecast.\n\\(\\hat{y}_{t+h|t} = y_t\\)\nAssumptions\n\nNo systematic pattern in data\nRecent observations are most relevant\n\nStrengths & Weaknesses\n✓ Simple benchmark model\n✓ Requires no computation\n✗ Ignores all patterns\n✗ Poor for trending/seasonal data"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#naive-1",
    "href": "lab/epss_training/slides/epss_training.html#naive-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Naive",
    "text": "Naive\nWe use NAIVE() function and model() function to build the Naive model.\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(naive = NAIVE(quantity_issued)) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#seasonal-naive-snaive",
    "href": "lab/epss_training/slides/epss_training.html#seasonal-naive-snaive",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal NAIVE (sNAIVE)",
    "text": "Seasonal NAIVE (sNAIVE)\n\\(y_{t+h \\mid t} = y_{t+h - m(k+1)}\\)\nWhere: \\(m\\) = seasonal period and \\(k = \\lfloor \\frac{h-1}{m} \\rfloor\\)\nAssumptions\n\nSeasonal pattern is stable\nNo trend present\n\nStrengths & Weaknesses\n✓ Handles strong seasonality\n✓ Simple interpretation\n✗ Fails with changing seasonality\n✗ Ignores non-seasonal patterns"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#snaive",
    "href": "lab/epss_training/slides/epss_training.html#snaive",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "sNaive",
    "text": "sNaive\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#mean",
    "href": "lab/epss_training/slides/epss_training.html#mean",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Mean",
    "text": "Mean\nUses the historical average of all observations as forecast.\n\\(y_{t+h \\mid t} = \\bar{y} = \\frac{1}{t} \\sum_{i=1}^{t} y_i\\)\nWhere: \\(t\\) is the number of past observations used for the forecast.\nAssumptions\n\nSeries is stationary\nShort-term fluctuations are noise\n\nStrengths & Weaknesses\n✓ Effective noise reduction\n✓ Simple to implement\n✗ Ignores all patterns\n✗ Lags behind trends"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#mean-1",
    "href": "lab/epss_training/slides/epss_training.html#mean-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Mean",
    "text": "Mean\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(MEAN(quantity_issued ~ window(size = 3))) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#arima",
    "href": "lab/epss_training/slides/epss_training.html#arima",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA",
    "text": "ARIMA\nCombines Autoregressive (AR) and Moving Average (MA) components with differencing.\n\nAR: autoregressive (lagged observations as inputs)\nI: integrated (differencing to make series stationary)\nMA: moving average (lagged errors as inputs)\n\n\nThe ARIMA model is given by:\n\\((1 - \\phi_1 B - \\dots - \\phi_p B^p)(1 - B)^d y_t = c + (1 + \\theta_1 B + \\dots + \\theta_q B^q) \\epsilon_t\\)\nWhere: \\(B\\): Backshift operator, \\(\\phi\\): AR coefficients, \\(\\theta\\): MA coefficients, \\(d\\): Differencing order, \\(p\\): AR order, \\(q\\): MA order and \\(\\epsilon_t\\): White noise"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#arima-1",
    "href": "lab/epss_training/slides/epss_training.html#arima-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA",
    "text": "ARIMA\nAssumptions\n\nSeries is stationary\nLinear relationship between past values and errors\nWhite noise errors\nNo missing values in series\n\nStrengths & Weaknesses\n✓ Flexible for various time series patterns\n✓ Perform well for short term horizons\n✗ Requires stationarity for optimal performance\n✗ The parameters are often not easily interpretable in terms of trend or seasonality"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#arima-2",
    "href": "lab/epss_training/slides/epss_training.html#arima-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA",
    "text": "ARIMA\nA stationary series is:\n\nroughly horizontal\n\nconstant variance\n\nno patterns predictable in the long-term"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#seasonal-arima-models",
    "href": "lab/epss_training/slides/epss_training.html#seasonal-arima-models",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Seasonal ARIMA models",
    "text": "Seasonal ARIMA models\n\n\n\nARIMA\n\\(~\\underbrace{(p, d, q)}\\)\n\\(\\underbrace{(P, D, Q)_{m}}\\)\n\n\n\n\n\n\\({\\uparrow}\\)\n\\({\\uparrow}\\)\n\n\n\nNon-seasonal part\nSeasonal part of\n\n\n\nof the model\nof the model\n\n\n\n\n\\(m\\): number of observations per year.\n\\(d\\): first differences, \\(D\\): seasonal differences\n\\(p\\): AR lags, \\(q\\): MA lags\n\\(P\\): seasonal AR lags, \\(Q\\): seasonal MA lags\n\nSeasonal and non-seasonal terms combine multiplicatively."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#arima-automatic-modelling",
    "href": "lab/epss_training/slides/epss_training.html#arima-automatic-modelling",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA automatic modelling",
    "text": "ARIMA automatic modelling\n\nPlot the data. Identify any unusual observations.\nIf necessary, transform the data (e.g., Box-Cox transformation) to stabilize the variance.\nUse ARIMA() to automatically select a model.\nCheck the residuals from your chosen model and if they do not look like white noise, try a modified model.\nOnce the residuals look like white noise, calculate forecasts."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#arima-automatic-modelling-1",
    "href": "lab/epss_training/slides/epss_training.html#arima-automatic-modelling-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ARIMA automatic modelling",
    "text": "ARIMA automatic modelling\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(ARIMA(quantity_issued)) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#ets",
    "href": "lab/epss_training/slides/epss_training.html#ets",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nETS stands for Exponential Smoothing and is based on a state space framework that decomposes a time series into three components:\n\n\n\n\n\n\n\n\n\nGeneral Notation\n\nE T S\nExponenTial Smoothing\n\n\n\n\n\n↗\n↑\n↖\n\n\n\nError\nTrend\nSeason\n\n\n\n\nError: Additive (\"A\") or multiplicative (\"M\")\nTrend: None (\"N\"), additive (\"A\"), multiplicative (\"M\"), or damped (\"Ad\" or \"Md\").\nSeasonality: None (\"N\"), additive (\"A\") or multiplicative (\"M\")\n\n For example, ETS(A,N,N) is the simple exponential smoothing model (no trend or seasonality) with additive errors."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#ets-1",
    "href": "lab/epss_training/slides/epss_training.html#ets-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nHow do we combine these elements?\nAdditively?\n\\(y_t = \\ell_{t-1} + b_{t-1} + s_{t-m} + \\varepsilon_t\\)\n\nMultiplicatively?\n\\(y_t = \\ell_{t-1}b_{t-1}s_{t-m}(1 + \\varepsilon_t)\\)\n\nPerhaps a mix of both?\n\\(y_t = (\\ell_{t-1} + b_{t-1}) s_{t-m} + \\varepsilon_t\\)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#ets-2",
    "href": "lab/epss_training/slides/epss_training.html#ets-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nHow do the level, trend and seasonal components evolve over time?"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#ets-3",
    "href": "lab/epss_training/slides/epss_training.html#ets-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS",
    "text": "ETS\nAssumptions\n\nDecomposable patterns\nRecent observations more important\nConsistent error structure (additive/multiplicative)\n\nStrengths & Weaknesses\n✓ They can be adapted to various data characteristics with different error, trend, and seasonal formulations\n✓ Often very effective when the underlying components are stable\n✗ Parameter estimates (including smoothing parameters and initial states) can affect the forecasts\n✗ May struggle to capture sudden shifts or non-standard patterns if the smoothing parameters are constant"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#ets-automatic-modelling",
    "href": "lab/epss_training/slides/epss_training.html#ets-automatic-modelling",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS automatic modelling",
    "text": "ETS automatic modelling\n\nApply each model that is appropriate to the data.\nOptimize parameters and initial values using MLE (or some other criterion).\nSelect best method using AICc.\nUse ETS() to automatically select a model.\nProduce forecasts using best method."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#ets-automatic-modelling-1",
    "href": "lab/epss_training/slides/epss_training.html#ets-automatic-modelling-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "ETS automatic modelling",
    "text": "ETS automatic modelling\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(ETS(quantity_issued)) |&gt; \n  forecast(h = 12) |&gt; \n  autoplot(med_tsb_filter |&gt; \n             filter(hub_id == 'hub_1' & product_id == 'product_5'), level = NULL) +\n  labs(y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#model-fitting-in-fable",
    "href": "lab/epss_training/slides/epss_training.html#model-fitting-in-fable",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Model fitting in Fable",
    "text": "Model fitting in Fable\n\nThe model() function trains models on data.\nIt returns a mable object.\nA mable is a model table, each cell corresponds to a fitted model.\n\n\n# Fit the models\n\nfit_all &lt;- med_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(\n    naive = NAIVE(quantity_issued),\n    snaive = SNAIVE(quantity_issued ~ lag('year')),\n    mean = MEAN(quantity_issued ~ window(size = 3)),\n    arima = ARIMA(quantity_issued),\n    ets = ETS(quantity_issued)\n    )\n\nfit_all\n\n# A mable: 1 x 7\n# Key:     hub_id, product_id [1]\n  hub_id product_id   naive   snaive    mean                     arima\n  &lt;chr&gt;  &lt;chr&gt;      &lt;model&gt;  &lt;model&gt; &lt;model&gt;                   &lt;model&gt;\n1 hub_1  product_5  &lt;NAIVE&gt; &lt;SNAIVE&gt;  &lt;MEAN&gt; &lt;ARIMA(0,1,1)(0,0,2)[12]&gt;\n# ℹ 1 more variable: ets &lt;model&gt;"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#extract-information-from-mable",
    "href": "lab/epss_training/slides/epss_training.html#extract-information-from-mable",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Extract information from mable",
    "text": "Extract information from mable\n\nfit_all  |&gt;  select(snaive) |&gt;  report()\nfit_all |&gt;  tidy()\nfit_all  |&gt;  glance()\n\n\nThe report() function gives a formatted model-specific display.\nThe tidy() function is used to extract the coefficients from the models.\nWe can extract information about some specific model using the filter() and select()functions."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#producing-forecasts",
    "href": "lab/epss_training/slides/epss_training.html#producing-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Producing forecasts",
    "text": "Producing forecasts\n\nThe forecast() function is used to produce forecasts from estimated models.\nh can be specified with:\n\na number (the number of future observations)\nnatural language (the length of time to predict)\nprovide a dataset of future time periods"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#producing-forecasts-1",
    "href": "lab/epss_training/slides/epss_training.html#producing-forecasts-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Producing forecasts",
    "text": "Producing forecasts\n\nfit_all_fc &lt;- fit_all |&gt; \n  forecast(h = 'year')\n\n#h = \"year\" is equivalent to setting h = 12.\n\nfit_all_fc\n\n# A fable: 60 x 6 [1M]\n# Key:     hub_id, product_id, .model [5]\n   hub_id product_id .model     date\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;mth&gt;\n 1 hub_1  product_5  naive  2023 Jul\n 2 hub_1  product_5  naive  2023 Aug\n 3 hub_1  product_5  naive  2023 Sep\n 4 hub_1  product_5  naive  2023 Oct\n 5 hub_1  product_5  naive  2023 Nov\n 6 hub_1  product_5  naive  2023 Dec\n 7 hub_1  product_5  naive  2024 Jan\n 8 hub_1  product_5  naive  2024 Feb\n 9 hub_1  product_5  naive  2024 Mar\n10 hub_1  product_5  naive  2024 Apr\n# ℹ 50 more rows\n# ℹ 2 more variables: quantity_issued &lt;dist&gt;, .mean &lt;dbl&gt;"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#visualising-forecasts",
    "href": "lab/epss_training/slides/epss_training.html#visualising-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Visualising forecasts",
    "text": "Visualising forecasts\n\nfit_all_fc |&gt; \n  autoplot(level = NULL) +\n  autolayer(med_tsb_filter |&gt; \n              filter_index(\"2022 JAn\" ~ .) |&gt; \n              filter(hub_id == 'hub_1' & product_id == 'product_5'), color = 'black') +\n  labs(title = \"Forecasts for monthly quantity issued\", y = \"Quantity issued\", x = \"Date\") +\n  theme_minimal() +\n  theme(panel.border = element_rect(color = \"lightgrey\", fill = NA)) +\n  guides(colour=guide_legend(title=\"Forecast\"))"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#what-is-wrong-with-point-forecasts",
    "href": "lab/epss_training/slides/epss_training.html#what-is-wrong-with-point-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "What is wrong with point forecasts?",
    "text": "What is wrong with point forecasts?\nA point forecast is a single-value prediction representing the most likely future outcome, based on current data and models.\nThe disadvantage of point forecast;\n✗ It ignores additional information in future.\n✗ It does not explain uncertainties around future.\n✗ It can not deal with assymmetric."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts",
    "href": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nInterval forecasts: A prediction interval is an interval within which power generation may lie, with a certain probability."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-1",
    "href": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nQuantile forecasts: A quantile forecast provides a value that the future observation is expected to be below with a specified probability."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-2",
    "href": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nDistribution forecasts: A comprehensive probabilistic forecast capturing the full range of potential outcomes across all time horizons."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-3",
    "href": "lab/epss_training/slides/epss_training.html#types-of-probabilistic-forecasts-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Types of probabilistic forecasts",
    "text": "Types of probabilistic forecasts\nScenario forecasts: A spectrum of potential futures derived from probabilistic modeling to inform decision- making under uncertainty."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#forecast-distributions-from-bootstrapping",
    "href": "lab/epss_training/slides/epss_training.html#forecast-distributions-from-bootstrapping",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Forecast distributions from bootstrapping",
    "text": "Forecast distributions from bootstrapping\nWhen a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance.\n\nA one-step forecast error is defined as\n\n\\(e_t = y_t - \\hat{y}_{t|t-1}\\), \\(y_t = \\hat{y}_{t|t-1} + e_t\\)\n\nSo we can simulate the next observation of a time series using\n\n\\(y_{T+1} = \\hat{y}_{T+1|T} + e_{T+1}\\)\n\nAdding the new simulated observation to our data set, we can repeat the process to obtain\n\n\\(y_{T+2} = \\hat{y}_{T+2|T+1} + e_{T+2}\\)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#generate-different-futures-forecast",
    "href": "lab/epss_training/slides/epss_training.html#generate-different-futures-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Generate different futures forecast",
    "text": "Generate different futures forecast\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  generate(h = 12, bootstrap = TRUE, times = 5)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#generate-probabilistic-forecast",
    "href": "lab/epss_training/slides/epss_training.html#generate-probabilistic-forecast",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Generate probabilistic forecast",
    "text": "Generate probabilistic forecast\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  forecast(h = 12, bootstrap = TRUE, times = 1000)\n\n# A fable: 12 x 6 [1M]\n# Key:     hub_id, product_id, .model [1]\n   hub_id product_id .model     date quantity_issued  .mean\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;mth&gt;          &lt;dist&gt;  &lt;dbl&gt;\n 1 hub_1  product_5  snaive 2023 Jul    sample[1000]  3790.\n 2 hub_1  product_5  snaive 2023 Aug    sample[1000] 32090.\n 3 hub_1  product_5  snaive 2023 Sep    sample[1000] 13039.\n 4 hub_1  product_5  snaive 2023 Oct    sample[1000] 13203.\n 5 hub_1  product_5  snaive 2023 Nov    sample[1000] 28567.\n 6 hub_1  product_5  snaive 2023 Dec    sample[1000] 26470.\n 7 hub_1  product_5  snaive 2024 Jan    sample[1000] 11766.\n 8 hub_1  product_5  snaive 2024 Feb    sample[1000] 25393.\n 9 hub_1  product_5  snaive 2024 Mar    sample[1000] 11540.\n10 hub_1  product_5  snaive 2024 Apr    sample[1000] 11942.\n11 hub_1  product_5  snaive 2024 May    sample[1000] 16975.\n12 hub_1  product_5  snaive 2024 Jun    sample[1000]  5496."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#prediction-intervals",
    "href": "lab/epss_training/slides/epss_training.html#prediction-intervals",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nForecast intervals can be extracted using the hilo() function.\n\nmed_tsb_filter |&gt; \n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(snaive = SNAIVE(quantity_issued ~ lag(\"year\"))) |&gt; \n  forecast(h = 12, bootstrap = TRUE, times = 1000) |&gt; \n  hilo(level = 75) |&gt; \n  unpack_hilo(\"75%\")\n\n# A tsibble: 12 x 8 [1M]\n# Key:       hub_id, product_id, .model [1]\n   hub_id product_id .model     date quantity_issued  .mean `75%_lower`\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;mth&gt;          &lt;dist&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1 hub_1  product_5  snaive 2023 Jul    sample[1000]  3509.     -8015. \n 2 hub_1  product_5  snaive 2023 Aug    sample[1000] 32690.     20230. \n 3 hub_1  product_5  snaive 2023 Sep    sample[1000] 12713.       850. \n 4 hub_1  product_5  snaive 2023 Oct    sample[1000] 13433.      1624. \n 5 hub_1  product_5  snaive 2023 Nov    sample[1000] 29074.     16981. \n 6 hub_1  product_5  snaive 2023 Dec    sample[1000] 25767.     14190. \n 7 hub_1  product_5  snaive 2024 Jan    sample[1000] 11377.       -48.1\n 8 hub_1  product_5  snaive 2024 Feb    sample[1000] 25310.     13406. \n 9 hub_1  product_5  snaive 2024 Mar    sample[1000] 11876.      -410. \n10 hub_1  product_5  snaive 2024 Apr    sample[1000] 12137.       -44.1\n11 hub_1  product_5  snaive 2024 May    sample[1000] 16901.      4829. \n12 hub_1  product_5  snaive 2024 Jun    sample[1000]  5407.     -6448. \n# ℹ 1 more variable: `75%_upper` &lt;dbl&gt;"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#forecast-accuracy-evaluation-using-test-sets",
    "href": "lab/epss_training/slides/epss_training.html#forecast-accuracy-evaluation-using-test-sets",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Forecast accuracy evaluation using test sets",
    "text": "Forecast accuracy evaluation using test sets\n\nWe mimic the real life situation\nWe pretend we don’t know some part of data (new data)\nIt must not be used for any aspect of model training\nForecast accuracy is computed only based on the test set\n\nTraining and test sets"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\nIn order to evaluate the performance of a forecasting model, we compute its forecast accuracy.\nForecast accuracy is compared by measuring errors based on the test set.\nIdeally it should allow comparing benefits from improved accuracy with the cost of obtaining the improvement."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-1",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\nForecast Error\n\\(e_{T+h} = y_{T+h} - \\hat{y}_{T+h\\mid T}\\)\nwhere\n- \\(y_{T+h}\\) is the \\((T+h)^\\text{th}\\) observation \\((h=1,\\dots,H)\\), and\n- \\(\\hat{y}_{T+h\\mid T}\\) is the forecast based on data up to time \\(T\\).\n\nRead more on How to choose appropriate error measure by Ivan Svetunkov."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-2",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\n\n\n\n\n\n\n\nMeasure\nFormula\nNotes\n\n\n\n\nMAE  (Mean Absolute Error)\n\\(\\text{MAE} = \\text{mean}(|e_{T+h}|)\\)\nScale dependent\n\n\nMSE  (Mean Squared Error)\n\\(\\text{MSE} = \\text{mean}(e_{T+h}^2)\\)\nScale dependent\n\n\nMAPE  (Mean Absolute Percentage Error)\n\\(\\text{MAPE} = 100\\,\\text{mean}(|e_{T+h}|/|y_{T+h}|)\\)\nScale independent; use if \\(y_t \\gg 0\\) and \\(y\\) has a natural zero\n\n\nRMSE  (Root Mean Squared Error)\n\\(\\text{RMSE} = \\sqrt{\\text{mean}(e_{T+h}^2)}\\)\nScale dependent"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-3",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\n\n\n\n\n\n\n\nMeasure\nFormula\nNotes\n\n\n\n\nMASE  (Mean Absolute Scaled Error)\n\\(\\text{MASE} = \\text{mean}(|e_{T+h}|/Q)\\)\nNon-seasonal: \\(Q = \\frac{1}{T-1}\\sum_{t=2}^T |y_t-y_{t-1}|\\)  Seasonal: \\(Q = \\frac{1}{T-m}\\sum_{t=m+1}^T |y_t-y_{t-m}|\\), where \\(m\\) is the seasonal frequency\n\n\nRMSSE  (Root Mean Squared Scaled Error)\n\\(\\text{RMSSE} = \\sqrt{\\text{mean}(e_{T+h}^2/Q)}\\)\nNon-seasonal: \\(Q = \\frac{1}{T-1}\\sum_{t=2}^T (y_t-y_{t-1})^2\\)  Seasonal: \\(Q = \\frac{1}{T-m}\\sum_{t=m+1}^T (y_t-y_{t-m})^2\\), where \\(m\\) is the seasonal frequency"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-4",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\nCreate train and test sets.\n\nf_horizon &lt;- 12 # forecast horizon\n\ntrain &lt;- med_tsb_filter |&gt; # create train set\n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt; \n  filter_index(. ~ '2022 June')\n\nfit_all &lt;- train |&gt; # model fitting\n  filter(hub_id == 'hub_1' & product_id == 'product_5') |&gt;\n  model(\n    naive = NAIVE(quantity_issued),\n    snaive = SNAIVE(quantity_issued ~ lag('year')),\n    mean = MEAN(quantity_issued ~ window(size = 3)),\n    arima = ARIMA(quantity_issued),\n    ets = ETS(quantity_issued)\n    )\n\nfit_all_fc &lt;- fit_all |&gt; # forecasting\n  forecast(h = f_horizon)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-5",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-point-forecast-accuracy-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating point forecast accuracy",
    "text": "Evaluating point forecast accuracy\n\nfit_all_fc |&gt; \n  accuracy(med_tsb_filter |&gt;\n             filter(hub_id == 'hub_1' & product_id == 'product_5'),\n           measures = list(point_accuracy_measures))\n\n# A tibble: 5 × 12\n  .model hub_id product_id .type     ME   RMSE    MAE    MPE  MAPE  MASE RMSSE\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 arima  hub_1  product_5  Test   5787. 10679.  8732.   5.08  59.3  1.23  1.21\n2 ets    hub_1  product_5  Test   5400. 11163.  8681.   5.33  57.0  1.23  1.27\n3 mean   hub_1  product_5  Test   8568. 12274.  9664.  29.9   54.3  1.37  1.39\n4 naive  hub_1  product_5  Test  -3627.  9508.  8803. -76.1   94.1  1.24  1.08\n5 snaive hub_1  product_5  Test   5237. 12571. 11346.  -1.06  85.6  1.60  1.43\n# ℹ 1 more variable: ACF1 &lt;dbl&gt;"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nCoverage\n\nMeasures how often the true value falls within a prediction interval\nTypically assessed for specific confidence levels (e.g., 95% interval)\n\nExample: A 95% prediction interval should contain the true value 95% of the time."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-1",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nSharpness\n\nRefers to the width of prediction intervals\nMeasures how precise or focused the forecast is\n\nExample: A forecast predicting monthly sales qty between 2500-5000 is sharper than 500-10000."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-2",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nQuantile score/ Pin ball loss\n\nAssesses entire prediction interval, not just point forecast\nPenalizes too narrow and too wide intervals\nInterpretation: Lower values indicate better calibrated intervals\n\n\\[\nQ_{p,t} =\n\\begin{cases}\n2(1 - p)(f_{p,t} - y_t), & \\text{if } y_t &lt; f_{p,t}, \\\\[1mm]\n2p(y_t - f_{p,t}),       & \\text{if } y_t \\geq f_{p,t}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-3",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nCRPS (Continuous Ranked Probability Score)\n\nProper scoring rule\nMeasures accuracy of full predictive distribution\nGeneralizes absolute error to probabilistic forecasts\nInterpretation: Lower CRPS = better forecast\nAdvantage: Sensitive to distance, rewards sharp and calibrated forecasts\n\n\\(\\large \\text{CRPS} = \\text{mean}(p_j),\\)\nwhere\n\\(p_j = \\int_{-\\infty}^{\\infty} \\left(G_j(x) - F_j(x)\\right)^2dx,\\)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-4",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\nCRPS (Continuous Ranked Probability Score)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-5",
    "href": "lab/epss_training/slides/epss_training.html#evaluating-probabilistic-forecast-accuracy-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Evaluating probabilistic forecast accuracy",
    "text": "Evaluating probabilistic forecast accuracy\n\nfit_all_fc |&gt; \n  accuracy(med_tsb_filter |&gt;\n             filter(hub_id == 'hub_1' & product_id == 'product_5'),\n           measures = list(distribution_accuracy_measures)) |&gt; \n  select(-percentile)\n\n# A tibble: 5 × 5\n  .model hub_id product_id .type  CRPS\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt;\n1 arima  hub_1  product_5  Test  6311.\n2 ets    hub_1  product_5  Test  6648.\n3 mean   hub_1  product_5  Test  7081.\n4 naive  hub_1  product_5  Test  8034.\n5 snaive hub_1  product_5  Test  7823."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#feature-engineering",
    "href": "lab/epss_training/slides/epss_training.html#feature-engineering",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature engineering",
    "text": "Feature engineering\nIn this training, we only do a basic feature engineering.\n\n# Load data\nfrom google.colab import files\nuploaded = files.upload()\ndf = pd.read_csv('med_tsb_filter.csv')\n\n# Make the yearmonth as date format\ndf['date'] = pd.to_datetime(df['date']) + pd.offsets.MonthEnd(0)\n\n# Feature Engineering\ndf['month'] = df['date'].dt.month  # create month feature\n\n# categorical encoding\nenc = OrdinalEncoder()\ndf[['hub_id_cat', 'product_id_cat']] = enc.fit_transform(df[['hub_id', 'product_id']])\n\n# Create unique identifier for series\ndf['unique_id'] = df['hub_id'] + '_' + df['product_id']"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#feature-engineering-1",
    "href": "lab/epss_training/slides/epss_training.html#feature-engineering-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature engineering",
    "text": "Feature engineering\n\n# Create series and exogenous data\nseries = df[['date', 'unique_id', 'quantity_issued']]\nexog = df[['date', 'unique_id', 'month', 'hub_id_cat', 'product_id_cat']]\n\n# Transform series and exog to dictionaries\n\nseries_dict = series_long_to_dict(\n    data      = series,\n    series_id = 'unique_id',\n    index     = 'date',\n    values    = 'quantity_issued',\n    freq      = 'M'\n)\n\nexog_dict = exog_long_to_dict(\n    data      = exog,\n    series_id = 'unique_id',\n    index     = 'date',\n    freq      = 'M'\n)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#feature-engineering-2",
    "href": "lab/epss_training/slides/epss_training.html#feature-engineering-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Feature engineering",
    "text": "Feature engineering\n\n# Partition data in train and test\nend_train = '2022-06-30'\nstart_test = pd.to_datetime(end_train) + pd.DateOffset(months=1)  # Add 1 month\n\nseries_dict_train = {k: v.loc[:end_train] for k, v in series_dict.items()}\nexog_dict_train = {k: v.loc[:end_train] for k, v in exog_dict.items()}\nseries_dict_test = {k: v.loc[start_test:] for k, v in series_dict.items()}\nexog_dict_test = {k: v.loc[start_test:] for k, v in exog_dict.items()}"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#xgboost",
    "href": "lab/epss_training/slides/epss_training.html#xgboost",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\nExtreme Gradient Boosting (XGBoost) is a scalable tree-based gradient boosting machine learning algorithm.\n\\(\\hat{y}_{t+h|t} = \\sum_{k=1}^K f_k(\\mathbf{x}_t), \\quad f_k \\in \\mathcal{F}\\)\nWhere: \\(K\\) = number of trees, \\(f_k\\) = tree function, \\(\\mathbf{x}_t\\) = feature vector (lags, calendar features, etc.)\n\n\n\nsource: Rui Guo et al."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#xgboost-1",
    "href": "lab/epss_training/slides/epss_training.html#xgboost-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\nAssumptions\n\nPredictive patterns can be captured through feature engineering\nRelationships between features and target are stable\nNo strong temporal dependencies beyond engineered features\n\nStrengths & Weaknesses\n✓ Handles non-linear relationships well\n✓ Provides feature importance metrics\n✗ Requires careful parameter tuning\n✗ Less interpretable than linear models"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#xgboost-2",
    "href": "lab/epss_training/slides/epss_training.html#xgboost-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Fit xgboost forecaster\nregressor_xgb = XGBRegressor(tree_method = 'hist',\n                             enable_categorical = True)\n\nforecaster_xgb = ForecasterRecursiveMultiSeries(\n                 regressor          = regressor_xgb,\n                 transformer_series = None,\n                 lags               = 4,\n                 dropna_from_series = False\n             )\n\nforecaster_xgb.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True)\n\nforecaster_xgb"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#xgboost-3",
    "href": "lab/epss_training/slides/epss_training.html#xgboost-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Feature importance plot for XGB\nplt.figure(figsize=(10, 6))\nfeat_xgb = forecaster_xgb.get_feature_importances()\nsns.barplot(x='importance', y='feature', data=feat_xgb.sort_values('importance', ascending=False).head(10))\nplt.title('XGBoost Feature Importance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#xgboost-4",
    "href": "lab/epss_training/slides/epss_training.html#xgboost-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# XGB predictions and plot\nboot = 100\npredictions_xgb = forecaster_xgb.predict_bootstrapping(steps=12, exog=exog_dict_test, n_boot=boot)\n\n# Create prediction DF and plot example series\nexample_series = list(series_dict_test.keys())[2]\nxgb_pred_test = predictions_xgb[example_series].copy()\n\n# Calculate statistics\nmean_pred = xgb_pred_test.mean(axis=1)\nlower_pred = xgb_pred_test.quantile(0.025, axis=1)\nupper_pred = xgb_pred_test.quantile(0.975, axis=1)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#xgboost-5",
    "href": "lab/epss_training/slides/epss_training.html#xgboost-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(series_dict_test[example_series], label='Actual', color='black')\nplt.plot(mean_pred, label='XGB Prediction', color='blue')\nplt.fill_between(mean_pred.index, \n                 lower_pred, \n                 upper_pred,\n                 color='blue', alpha=0.2)\nplt.title('XGBoost Forecast with 95% Prediction Intervals')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#xgboost-6",
    "href": "lab/epss_training/slides/epss_training.html#xgboost-6",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "XGBoost",
    "text": "XGBoost\n\n# Create prediciton df\n\npred_id = list(predictions_xgb.keys())\n\n# Create an empty DataFrame\nxgb_pred = pd.DataFrame(columns=['date', 'unique_id', 'model'] + [f'X_{i}' for i in range(boot)])\n\nfor i in pred_id:\n    xgb_pred_test = predictions_xgb[i]\n    xgb_pred_test = xgb_pred_test.reset_index()\n    xgb_pred_test.columns = ['date'] + [f'X_{i}' for i in range(boot)]\n    xgb_pred_test['unique_id'] = i\n    xgb_pred_test['model'] = 'xgb'\n    xgb_pred = pd.concat([xgb_pred, xgb_pred_test])\n\nxgb_pred.head()"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lightgbm",
    "href": "lab/epss_training/slides/epss_training.html#lightgbm",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\nLight Gradient Boosting Machine (LightGBM) uses leaf-wise tree growth for efficiency whereas other boosting methods divide the tree level‐wise.\n\n\n\nsource: Sheng Dong et al."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lightgbm-1",
    "href": "lab/epss_training/slides/epss_training.html#lightgbm-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\nAssumptions\n\nSimilar to XGBoost but more efficient with large datasets\nHandles categorical features natively\n\nStrengths & Weaknesses\n✓ Faster training speed\n✓ Lower memory usage\n✗ Sensitive to small datasets\n✗ May overfit with noisy data"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lightgbm-2",
    "href": "lab/epss_training/slides/epss_training.html#lightgbm-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Fit lightgbm forecaster\n\nregressor_lgbm = LGBMRegressor(\n                boosting_type = 'gbdt',\n                metric = 'mae',\n                learning_rate = 0.1,\n                num_iterations = 200,\n                n_estimators = 100,\n                objective = 'poisson')\n\nforecaster_lgbm = ForecasterRecursiveMultiSeries(\n                 regressor          = regressor_lgbm, \n                 transformer_series = None,\n                 lags               = 4,  \n                 dropna_from_series = False\n             )\n\nforecaster_lgbm.fit(series=series_dict_train, exog=exog_dict_train, suppress_warnings=True)\n\nforecaster_lgbm"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lightgbm-3",
    "href": "lab/epss_training/slides/epss_training.html#lightgbm-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Feature importance plot for LGBM\nplt.figure(figsize=(10, 6))\nfeat_lgbm = forecaster_lgbm.get_feature_importances()\nsns.barplot(x='importance', y='feature', data=feat_lgbm.sort_values('importance', ascending=False).head(10))\nplt.title('LightGBM Feature Importance')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lightgbm-4",
    "href": "lab/epss_training/slides/epss_training.html#lightgbm-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# LGBM predictions and plot\nboot = 100\npredictions_lgbm = forecaster_lgbm.predict_bootstrapping(steps=12, exog=exog_dict_test, n_boot=boot)\n\n# Create prediction DF and plot example series\nexample_series = list(series_dict_test.keys())[2]\nlgbm_pred_test = predictions_lgbm[example_series].copy()\n\n# Calculate statistics\nmean_pred = lgbm_pred_test.mean(axis=1)\nlower_pred = lgbm_pred_test.quantile(0.025, axis=1)\nupper_pred = lgbm_pred_test.quantile(0.975, axis=1)"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lightgbm-5",
    "href": "lab/epss_training/slides/epss_training.html#lightgbm-5",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(series_dict_test[example_series], label='Actual', color='black')\nplt.plot(mean_pred, label='LGBM Prediction', color='blue')\nplt.fill_between(mean_pred.index, \n                 lower_pred, \n                 upper_pred,\n                 color='blue', alpha=0.2)\nplt.title('LGBM Forecast with 95% Prediction Intervals')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#lightgbm-6",
    "href": "lab/epss_training/slides/epss_training.html#lightgbm-6",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "LightGBM",
    "text": "LightGBM\n\n# Create prediciton df\n\npred_id = list(predictions_lgbm.keys())\n\n# Create an empty DataFrame\nlgbm_pred = pd.DataFrame(columns=['date', 'unique_id', 'model'] + [f'X_{i}' for i in range(boot)])\n\nfor i in pred_id:\n    lgbm_pred_test = predictions_lgbm[i]\n    lgbm_pred_test = lgbm_pred_test.reset_index()\n    lgbm_pred_test.columns = ['date'] + [f'X_{i}' for i in range(boot)]\n    lgbm_pred_test['unique_id'] = i\n    lgbm_pred_test['model'] = 'lgbm'\n    lgbm_pred = pd.concat([lgbm_pred, lgbm_pred_test])\n\nlgbm_pred.head()"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#model-evaluation",
    "href": "lab/epss_training/slides/epss_training.html#model-evaluation",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Model evaluation",
    "text": "Model evaluation\n\n# Calculate metrics for both models\nxgb_mase, xgb_rmse, xgb_qs, xgb_crps = calculate_metrics(predictions_xgb, series_dict_test, series_dict_train)\nlgbm_mase, lgbm_rmse, lgbm_qs, lgbm_crps = calculate_metrics(predictions_lgbm, series_dict_test, series_dict_train)\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nAverage MASE\nAverage RMSE\nAverage Quantile Score\nAverage CRPS\n\n\n\n\nXGBoost\n1.218\n4878.992\n1151.738\n3548.056\n\n\nLightGBM\n1.061\n4953.944\n310.234\n3498.346\n\n\n\n\nNote: We can improve the performance of XGBoost and LightGBM through better feature engineering and hyperparameter tuning."
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#timegpt",
    "href": "lab/epss_training/slides/epss_training.html#timegpt",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\nFoundational time series model for time series forecasting by Nixtla (Read more).\nAssumptions\n\nNo strict stationarity requirements\nAutomatically handles multiple series\n\nStrengths & Weaknesses\n✓ Zero configuration needed\n✓ Handles complex patterns\n✗ Requires API access\n✗ Black-box model"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#timegpt-1",
    "href": "lab/epss_training/slides/epss_training.html#timegpt-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\nGet API key from Nixtla\n\nVisit https://nixtla.io/\nSign up for free account\nNavigate to API Keys section\nCreate new key and copy it\n\n\n!pip install nixtla\n\n# Load libraries\nfrom nixtla import NixtlaClient\n\n# Initialize Nixtla client\nnixtla_client = NixtlaClient(api_key='your_api_key_here')"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#timegpt-2",
    "href": "lab/epss_training/slides/epss_training.html#timegpt-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\n\n# Since we already have done the feature engineering, we dont need to do it again\n# Create unique identifier and rename columns for TimeGPT\ndf_timegpt = df.rename(columns={'date': 'ds', 'quantity_issued': 'y'}).drop(columns=['hub_id', 'product_id'])\n\n# Split data into train-test\nend_train = '2022-06-30'\ntrain_df = df_timegpt[df_timegpt['ds'] &lt;= end_train]\ntest_df = df_timegpt[df_timegpt['ds'] &gt; end_train]"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#timegpt-3",
    "href": "lab/epss_training/slides/epss_training.html#timegpt-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\n\n# TimeGPT Base Model\ntimegpt_fcst =  nixtla_client.forecast(\n    df=train_df,\n    h=len(test_df['ds'].unique()),\n    freq='M',\n    level=[90, 95]  # 90% and 95% prediction intervals\n)\n\nnixtla_client.plot(train_df, timegpt_fcst, time_col='ds', target_col='y')"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#timegpt-4",
    "href": "lab/epss_training/slides/epss_training.html#timegpt-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "TimeGPT",
    "text": "TimeGPT\n\n# TimeGPT with Exogenous Variables\n\n# Prepare exogenous data\nexog_features = ['month', 'hub_id_cat', 'product_id_cat']\n\n# Future exogenous variables (from your test set)\nfuture_exog = test_df[['unique_id', 'ds'] + exog_features]\n\ntimegpt_reg_fcst = nixtla_client.forecast(\n    df=train_df,\n    X_df=future_exog,\n    h=len(test_df['ds'].unique()),\n    freq='M',\n    level=[90, 95]\n)\n\nnixtla_client.plot(train_df, timegpt_reg_fcst, time_col='ds', target_col='y')"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#model-evaluation-1",
    "href": "lab/epss_training/slides/epss_training.html#model-evaluation-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Model evaluation",
    "text": "Model evaluation\n\n# Calculate metrics for both models\nxgb_mase, xgb_rmse, xgb_qs, xgb_crps = calculate_metrics(predictions_xgb, series_dict_test, series_dict_train)\nlgbm_mase, lgbm_rmse, lgbm_qs, lgbm_crps = calculate_metrics(predictions_lgbm, series_dict_test, series_dict_train)\n\n# Calculate metrics for base model\nbase_metrics = calculate_metrics(timegpt_fcst, test_df, train_df)\n\n# Calculate metrics for regressor model\nreg_metrics = calculate_metrics(timegpt_reg_fcst, test_df, train_df)\n\n\n\n\n\n\n\n\n\n\n\nModel\nAverage MASE\nAverage RMSE\nAverage Quantile Score\n\n\n\n\nTimeGPT Base Model\n1.019\n4261.135\n41.892\n\n\nTimeGPT Regressor Model\n1.125\n5016.053\n57.571"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context",
    "href": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nA population-based contraceptive needs estimation model combining:\n\nPopulation dynamics\nFamily planning indicators\nMethod/brand distribution factors"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-1",
    "href": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-1",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\n\\[\\begin{equation}\n\\begin{split}\ny_{i,t} = & \\left(\\sum_{j=15}^{50} \\text{mCPR}_{t,j} \\times \\text{WomenPopulation}_{t,j}\\right) \\\\\n           & \\times \\text{MethodMix}_{t,i} \\times \\text{CYP}_{t,i} \\times \\text{BrandMix}_{t,i} \\times \\text{SourceShare}_t\n\\end{split}\n\\end{equation}\\]\n\n\\(i\\): Contraceptive product\n\\(t\\): Time period (year)\n\\(\\text{mCPR}\\): Modern Contraceptive Prevalence Rate (%)\n\\(\\text{WomenPopulation}\\): Women aged 15-49\n\\(\\text{MethodMix}\\): Contraceptive method distribution\n\\(\\text{CYP}\\): Couple-Years of Protection factor\n\\(\\text{BrandMix}\\): Brand preference distribution\n\\(\\text{SourceShare}\\): Provider type distribution"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-2",
    "href": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-2",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nAssumptions\n\nStable demographic patterns during forecast period\nConsistent reporting of family planning indicators\nAccurate CYP values for different methods\nHistorical brand/source mixes remain valid\nLinear relationship between population and needs\nProper spatial distribution via site coordinates\nValid monthly weight distribution"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-3",
    "href": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-3",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nStrengths & Weaknesses\n✓ Directly ties to population dynamics\n✓ Incorporates multiple programmatic factors\n✓ Enables spatial allocation to health sites\n✓ Aligns with public health planning frameworks\n✗ Sensitive to input data quality\n✗ Static assumptions about behavior patterns\n✗ Limited responsiveness to sudden changes\n✗ Provides national level need"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-4",
    "href": "lab/epss_training/slides/epss_training.html#demographic-forecasting-method-fpsc-context-4",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Demographic forecasting method (FPSC Context)",
    "text": "Demographic forecasting method (FPSC Context)\nFemale condom needs (age 20-15) for 2020 (example)\n\n\n\n\n\n\n\n\n\nParameter\nValue\nUnit\nSources\n\n\n\n\nA. Women population 20 – 25\n345,000\n# of people\nWorldPop\n\n\nB. Percentage of women who will use a contraceptive product (mCPR)\n68%\n%\nPMA DataLab\n\n\nC. Percentage of the method mix\n26%\n%\nPMA DataLab\n\n\nD. CYP\n120\n-\nUSAID\n\n\nE. Percentage of brand A mix\n45%\n%\nAuthor calculation\n\n\nF. Percentage of source mix\n13%\n%\nUSAID\n\n\nTotal requirement (A × B × C × D × E × F)\n629,694\n# of products\n-"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#next-steps-and-further-learning",
    "href": "lab/epss_training/slides/epss_training.html#next-steps-and-further-learning",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Next steps and further learning",
    "text": "Next steps and further learning\n\nForecasting for social good learning labs\nForecasting: Principles and Practice\nForecasting and Analytics with the Augmented Dynamic Adaptive Model (ADAM)\nNixtla\nSKTIME\nskforecast"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#post-training-feedback",
    "href": "lab/epss_training/slides/epss_training.html#post-training-feedback",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Post-Training Feedback",
    "text": "Post-Training Feedback"
  },
  {
    "objectID": "lab/epss_training/slides/epss_training.html#thank-you",
    "href": "lab/epss_training/slides/epss_training.html#thank-you",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "Thank you!",
    "text": "Thank you!\n\n\nScan the QR Code and follow us on LinkedIn…"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html",
    "href": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "In this lab, we explore a discrete-time Markov chain modeling brand switching between two paracetamol brands:\n\nBrand A: locally manufactured (state 1)\nBrand B: imported (state 2)\n\nPatients switch weekly according to the transition matrix:\n\n\n\nFrom  To\nBrand A\nBrand B\n\n\n\n\nBrand A\n0.92\n0.08\n\n\nBrand B\n0.15\n0.85\n\n\n\nWe will:\n\nCompute expected market shares after the first three weeks.\nWrite a function to calculate the steady-state distribution.\nPlot convergence over time.\nVerify independence of the steady state from initial shares.\nAdd simulation and sensitivity tasks.\n\n\n\nSet the initial market-share vector \\(q^{(0)} = (0.5, 0.5)\\). Manually compute \\(q^{(1)}, q^{(2)}, q^{(3)}\\) using matrix multiplication.\n\n# Define transition matrix P\nP &lt;- matrix(c(0.92, 0.08,\n              0.15, 0.85), \n            nrow = 2, byrow = TRUE,\n            dimnames = list(c(\"A\", \"B\"), c(\"A\", \"B\")))\n# Initial distribution\nq0 &lt;- c(A = 0.5, B = 0.5)\n# Compute first three periods\nq_list &lt;- accumulate(1:3, ~ .x %*% P, .init = q0)[-1]\n# Display results\nresults1 &lt;- tibble(week = 1:3,\n       A = map_dbl(q_list, 1),\n       B = map_dbl(q_list, 2))\nknitr::kable(results1, digits=6)\n\n\n\n\nweek\nA\nB\n\n\n\n\n1\n0.535000\n0.465000\n\n\n2\n0.561950\n0.438050\n\n\n3\n0.582702\n0.417298\n\n\n\n\n\n\n\n\nImplement a function steady_state(P) that returns the long-run distribution via eigen decomposition. Verify it on \\(P\\) above.\n\nsteady_state &lt;- function(P) {\n  eig &lt;- eigen(t(P))\n  vec &lt;- Re(eig$vectors[, which.min(Mod(eig$values - 1))])\n  pi &lt;- vec / sum(vec)\n  pi\n}\n# Test\ndist_ss &lt;- steady_state(P)\nknitr::kable(t(round(dist_ss, 6)), col.names = c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.652174\n0.347826\n\n\n\n\n\n\n\n\nSimulate the expected market-share evolution for 30 weeks and plot \\(q^{(n)}\\) to illustrate convergence.\n\ntotal_weeks &lt;- 30\npi_mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\ncolnames(pi_mat) &lt;- c(\"A\", \"B\")\npi_mat[1, ] &lt;- q0\nfor (i in 2:(total_weeks + 1)) {\n  pi_mat[i, ] &lt;- pi_mat[i - 1, ] %*% P\n}\ndata_conv &lt;- as_tibble(pi_mat) |&gt;\n  mutate(week = 0:total_weeks) |&gt;\n  pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\nsteady_vals &lt;- steady_state(P)\n\ndata_conv |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Market-Share Convergence over Time\",\n       x = \"Week\",\n       y = \"Expected Market Share\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nThe convergence plot shows both shares approaching the dashed lines at (0.652174, 0.347826) by around week 20.\n\n\n\nUsing initial distributions \\((0.5, 0.5), (0.75, 0.25), (0.25, 0.75), (0.9, 0.1)\\), confirm that all converge to the same steady state.\n\ninit_list &lt;- list(\n  \"0.5,0.5\" = c(0.5, 0.5),\n  \"0.75,0.25\" = c(0.75, 0.25),\n  \"0.25,0.75\" = c(0.25, 0.75),\n  \"0.9,0.1\" = c(0.9, 0.1)\n)\nframe_list &lt;- map2_dfr(init_list, names(init_list), function(init_dist, label) {\n  mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\n  colnames(mat) &lt;- c(\"A\", \"B\")\n  mat[1, ] &lt;- init_dist\n  for (i in 2:(total_weeks + 1)) mat[i, ] &lt;- mat[i - 1, ] %*% P\n  as_tibble(mat) |&gt;\n    mutate(week = 0:total_weeks, init = label) |&gt;\n    pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\n})\nframe_list |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line() +\n  facet_wrap(~ init) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Convergence Across Different Initial Distributions\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nRegardless of initial shares, all paths converge to approximately (0.652174, 0.347826) by week 30.\n\n\n\nSimulate the brand preference for 10,000 patients over 30 weeks by sampling transitions. Compare empirical frequencies at week 30 to the theoretical steady state.\n\nset.seed(123)\nn_patients &lt;- 10000\nn_weeks &lt;- 30\n# Initialize state 1=A, 2=B randomly according to q0\nstates &lt;- sample(c(1,2), n_patients, replace = TRUE, prob = q0)\nfor (t in 1:n_weeks) {\n  probs &lt;- P[states, ]\n  states &lt;- apply(probs, 1, function(pr) sample(c(1,2), 1, prob = pr))\n}\nempirical &lt;- prop.table(table(states))\nknitr::kable(t(round(empirical, 4)), col.names=c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.6446\n0.3554\n\n\n\n\n\nEmpirical shares at week 30 (approx):\n\nBrand A: 0.6518\nBrand B: 0.3482\n\nClose to theoretical (0.6522, 0.3478).\n\n\n\nVary the promotion effect: change \\(p_{21}\\) (Brand B → A) from 0.15 to 0.20 and 0.10. Compute new steady states and discuss impact.\n\nfor (p21 in c(0.20, 0.10)) {\n  P_var &lt;- matrix(c(0.92, 0.08,\n                    p21, 1-p21), nrow=2, byrow=TRUE)\n  ss &lt;- steady_state(P_var)\n  cat(\"p21 =\", p21, \"→ steady state:\", round(ss,4), \"\\n\")\n}\n\np21 = 0.2 → steady state: 0.7143 0.2857 \np21 = 0.1 → steady state: 0.5556 0.4444 \n\n\nIncreasing the promotion increases the long-run share of Brand A, while decreasing it lowers that share."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-1-initial-evolution",
    "href": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-1-initial-evolution",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Set the initial market-share vector \\(q^{(0)} = (0.5, 0.5)\\). Manually compute \\(q^{(1)}, q^{(2)}, q^{(3)}\\) using matrix multiplication.\n\n# Define transition matrix P\nP &lt;- matrix(c(0.92, 0.08,\n              0.15, 0.85), \n            nrow = 2, byrow = TRUE,\n            dimnames = list(c(\"A\", \"B\"), c(\"A\", \"B\")))\n# Initial distribution\nq0 &lt;- c(A = 0.5, B = 0.5)\n# Compute first three periods\nq_list &lt;- accumulate(1:3, ~ .x %*% P, .init = q0)[-1]\n# Display results\nresults1 &lt;- tibble(week = 1:3,\n       A = map_dbl(q_list, 1),\n       B = map_dbl(q_list, 2))\nknitr::kable(results1, digits=6)\n\n\n\n\nweek\nA\nB\n\n\n\n\n1\n0.535000\n0.465000\n\n\n2\n0.561950\n0.438050\n\n\n3\n0.582702\n0.417298"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-2-steady-state-distribution-function",
    "href": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-2-steady-state-distribution-function",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Implement a function steady_state(P) that returns the long-run distribution via eigen decomposition. Verify it on \\(P\\) above.\n\nsteady_state &lt;- function(P) {\n  eig &lt;- eigen(t(P))\n  vec &lt;- Re(eig$vectors[, which.min(Mod(eig$values - 1))])\n  pi &lt;- vec / sum(vec)\n  pi\n}\n# Test\ndist_ss &lt;- steady_state(P)\nknitr::kable(t(round(dist_ss, 6)), col.names = c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.652174\n0.347826"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-3-convergence-plot",
    "href": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-3-convergence-plot",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Simulate the expected market-share evolution for 30 weeks and plot \\(q^{(n)}\\) to illustrate convergence.\n\ntotal_weeks &lt;- 30\npi_mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\ncolnames(pi_mat) &lt;- c(\"A\", \"B\")\npi_mat[1, ] &lt;- q0\nfor (i in 2:(total_weeks + 1)) {\n  pi_mat[i, ] &lt;- pi_mat[i - 1, ] %*% P\n}\ndata_conv &lt;- as_tibble(pi_mat) |&gt;\n  mutate(week = 0:total_weeks) |&gt;\n  pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\nsteady_vals &lt;- steady_state(P)\n\ndata_conv |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Market-Share Convergence over Time\",\n       x = \"Week\",\n       y = \"Expected Market Share\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nThe convergence plot shows both shares approaching the dashed lines at (0.652174, 0.347826) by around week 20."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-4-independence-from-initial-shares",
    "href": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-4-independence-from-initial-shares",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Using initial distributions \\((0.5, 0.5), (0.75, 0.25), (0.25, 0.75), (0.9, 0.1)\\), confirm that all converge to the same steady state.\n\ninit_list &lt;- list(\n  \"0.5,0.5\" = c(0.5, 0.5),\n  \"0.75,0.25\" = c(0.75, 0.25),\n  \"0.25,0.75\" = c(0.25, 0.75),\n  \"0.9,0.1\" = c(0.9, 0.1)\n)\nframe_list &lt;- map2_dfr(init_list, names(init_list), function(init_dist, label) {\n  mat &lt;- matrix(NA, nrow = total_weeks + 1, ncol = 2)\n  colnames(mat) &lt;- c(\"A\", \"B\")\n  mat[1, ] &lt;- init_dist\n  for (i in 2:(total_weeks + 1)) mat[i, ] &lt;- mat[i - 1, ] %*% P\n  as_tibble(mat) |&gt;\n    mutate(week = 0:total_weeks, init = label) |&gt;\n    pivot_longer(cols = c(\"A\", \"B\"), names_to = \"Brand\", values_to = \"Share\")\n})\nframe_list |&gt;\n  ggplot(aes(x = week, y = Share, color = Brand)) +\n  geom_line() +\n  facet_wrap(~ init) +\n  geom_hline(yintercept = steady_vals, linetype = \"dashed\") +\n  labs(title = \"Convergence Across Different Initial Distributions\") +\n  scale_colour_manual(name   = \"Brand\", values = c('A' = \"#E69F00\", 'B' = \"#0072B2\")) +\n  theme_few(base_size = 10) +\n  theme(\n    strip.text         = element_text(face = \"bold\"),\n    panel.grid.minor   = element_blank(),\n    panel.border       = element_rect(colour = \"lightgrey\", fill = NA),\n    panel.spacing      = unit(0.5, \"lines\")\n  )\n\n\n\n\n\n\n\n\nRegardless of initial shares, all paths converge to approximately (0.652174, 0.347826) by week 30."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-5-simulation-challenge",
    "href": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-5-simulation-challenge",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Simulate the brand preference for 10,000 patients over 30 weeks by sampling transitions. Compare empirical frequencies at week 30 to the theoretical steady state.\n\nset.seed(123)\nn_patients &lt;- 10000\nn_weeks &lt;- 30\n# Initialize state 1=A, 2=B randomly according to q0\nstates &lt;- sample(c(1,2), n_patients, replace = TRUE, prob = q0)\nfor (t in 1:n_weeks) {\n  probs &lt;- P[states, ]\n  states &lt;- apply(probs, 1, function(pr) sample(c(1,2), 1, prob = pr))\n}\nempirical &lt;- prop.table(table(states))\nknitr::kable(t(round(empirical, 4)), col.names=c(\"Brand A\", \"Brand B\"))\n\n\n\n\nBrand A\nBrand B\n\n\n\n\n0.6446\n0.3554\n\n\n\n\n\nEmpirical shares at week 30 (approx):\n\nBrand A: 0.6518\nBrand B: 0.3482\n\nClose to theoretical (0.6522, 0.3478)."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-6-sensitivity-analysis",
    "href": "lab/dl4sg_marcov/materials/lab/01_markov_intro_lab.html#task-6-sensitivity-analysis",
    "title": "Lab: Brand Switching Markov Chain Example",
    "section": "",
    "text": "Vary the promotion effect: change \\(p_{21}\\) (Brand B → A) from 0.15 to 0.20 and 0.10. Compute new steady states and discuss impact.\n\nfor (p21 in c(0.20, 0.10)) {\n  P_var &lt;- matrix(c(0.92, 0.08,\n                    p21, 1-p21), nrow=2, byrow=TRUE)\n  ss &lt;- steady_state(P_var)\n  cat(\"p21 =\", p21, \"→ steady state:\", round(ss,4), \"\\n\")\n}\n\np21 = 0.2 → steady state: 0.7143 0.2857 \np21 = 0.1 → steady state: 0.5556 0.4444 \n\n\nIncreasing the promotion increases the long-run share of Brand A, while decreasing it lowers that share."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "",
    "text": "2025-05-15"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#assumptions",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#assumptions",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Assumptions",
    "text": "Assumptions\n\nYou should be familiar with basic probability and random variables.\nYou are expected to be comfortable with R (or Python) for basic simulation and matrix operations.\nThis is not a theory-heavy workshop—we will use simple examples to build intuition, not derive theorems."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-cover",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-cover",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "What we will cover",
    "text": "What we will cover\n\nKey concepts in Markov chains\nTransition matrices and system evolution\nSteady-state distributions and their interpretations\nApplications of Markov models in healthcare supply chains\nCode demonstrations and simulations"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-not-cover",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#what-we-will-not-cover",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "What we will not cover",
    "text": "What we will not cover\n\nProofs of Markov chain convergence theorems\nAdvanced topics like Hidden Markov Models or Semi-Markov processes\nContinuous-time Markov processes in full generality\nFormal classification of all chain types (e.g., reducibility, ergodicity)\nMarcov process with rewards"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#materials",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#materials",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Materials",
    "text": "Materials\n\nYou can find the workshop materials here.\n\n\nNote: These materials are based on my learnings at NATCOR Taught Course Centre: Stochastic Modelling Course."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#outline",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#outline",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Outline",
    "text": "Outline\n\nWhy stochastic modeling?\nWhat are Markov processes?\nBrand switching as a DTMC example\nCode walk-throughs in R"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#why-stochastic-modelling-in-healthcare-supply-chains",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#why-stochastic-modelling-in-healthcare-supply-chains",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Why stochastic modelling in healthcare supply chains?",
    "text": "Why stochastic modelling in healthcare supply chains?\n\n\nDemand is unpredictable: Patient arrivals, seasonal outbreaks, and changing usage patterns\nSupply is uncertain: Delivery delays, stock losses, funding gaps, partial shipments\nHelps quantify risks: Probability of stockouts, unmet demand, cold chain failures\nEnables simulation of long-run behavior: Understand steady-state stock levels, refill patterns, or equipment uptime\nUseful for evaluating interventions: E.g., What if we promote a local brand? Add a backup supplier? Use mobile delivery?"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states",
    "text": "System states\nWe want to model the behaviour of a system which can change its state from one period to the next.\nFor example:\n\n\n\n\n\n\n\n\n\n\nSystem\nStates\nTime unit\n\n\n\n\nWeather\nSunny, cloudy, rainy, …\nDay or hour\n\n\nNo. of people in a healtcare centre\n0, 1, 2, 3, …\nMinute or second\n\n\nStatus of job application\n“In preparation”, “Submitted”, “Invited for interview”, …\nDay or hour?"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states: weather example",
    "text": "System states: weather example\nWe are interested in the transitions between different states. Suppose there are only 3 possible states in weather:\n\nFrom any of the 3 states we can get to any of the other states in a single transition (or stay in the same state). A sample trajectory of the system could be:\n\\([Sunny,  Cloudy,  Cloudy,  Rainy,  Sunny,  Cloudy,  Rainy,  Rainy, …]\\)"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states: weather example cont.",
    "text": "System states: weather example cont.\nFor convenience, let’s give each of the 3 states a number: \\(0 – Sunny\\), \\(1 – Cloudy\\), \\(2 – Rainy\\)\n\nLet \\(S\\) be the set of system states. So in our example: \\(S = {0, 1, 2}\\)\n\nLet \\(X_n\\) be the state of the system after \\(n\\) time units (e.g. days).\n\nFor example:"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.-1",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#system-states-weather-example-cont.-1",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "System states: weather example cont.",
    "text": "System states: weather example cont.\nSuppose that the system begins as follows:\n\\(X_0 = 0, X_1 = 1, X_2 = 1, X_3 = 2\\)\nGiven the above, what is the probability that \\(X_4 = 0\\), i.e. it is sunny after 4 days? We can write this as a conditional probability:\n\\[\nPr(X_4 = 0\\,|\\,\\underbrace{X_0 = 0,\\,X_1 = 1,\\, X_2 = 1,\\,X_3 = 2)}_{\\text{entire history of the process}}\n\\] Suppose we assume that the probability that \\(X_4 = 0\\) depends only on the value of \\(X_3\\), and not on \\(X_2\\), \\(X_1\\) or \\(X_0\\). We can then write:\n\\[\nPr(X_4 = 0\\,|\\,\\underbrace{X_0 = 0,\\,X_1 = 1,\\, X_2 = 1,\\,X_3 = 2)}_{\\text{entire history of the process}} = Pr(X_4 = 0\\,|\\,\\underbrace{X_3 = 2)}_{\\text{current state}}\n\\]"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#markov-process-definition",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#markov-process-definition",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Markov process: definition",
    "text": "Markov process: definition\nA stochastic process with the Markov Property:\n\n\\((X_1, X_2, X_3, ...)\\)\n\nin which the probability distribution for state \\(X_{n+1}\\) depends only on the state \\(X_n\\), and not on any of the states from \\(X_0\\) up to \\(X_{n-1}\\) (for all \\(n \\ge 0\\)).\n\nWriting this more mathematically, we can say:  \\[\n\\Pr\\bigl(X_{n+1} = j \\;\\bigm|\\; X_0 = x_0,\\; X_1 = x_1,\\;\\dots,\\; X_n = x_n\\bigr)\n\\;=\\;\n\\Pr\\bigl(X_{n+1} = j \\;\\bigm|\\; X_n = x_n\\bigr)\n\\]\nKey idea: “The future depends only on the present, not on the past.”\nSo the probability distribution for the next state depends only on the current state, not on the history of previous states. Can be discrete or continuous in time."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#what-is-a-good-example-of-a-markov-chain",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#what-is-a-good-example-of-a-markov-chain",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "What is a good example of a Markov chain?",
    "text": "What is a good example of a Markov chain?\n\n\n\n\n\nFor example, in Snakes and Ladders,\nif \\(X_n = 50\\) then:\n\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=67 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=52 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=53 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=34 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=55 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)\n\\(\\displaystyle \\Pr\\bigl(X_{n+1}=56 \\mid X_n=50\\bigr) = \\tfrac{1}{6}\\)"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#assumptions-of-markov-chain-models",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#assumptions-of-markov-chain-models",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Assumptions of Markov chain models",
    "text": "Assumptions of Markov chain models\n\nRemember, simple Markov chain models rely upon some important assumptions:\n\nThe Markov chain is in exactly one state on any particular time step\nThe probability distribution for the next state only depends on the current state (i.e., Markov property)\nThe transition probabilities are the same on every time step"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#transition-probability-matrix",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#transition-probability-matrix",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Transition probability matrix",
    "text": "Transition probability matrix\nLet \\(S = \\{s_1, s_2, \\dots, s_n\\}\\) be the finite state space. The one-step transition probabilities are:\n\\[\np_{ij} = \\Pr(X_{t+1} = s_j \\mid X_t = s_i)\n\\]\nWe collect these into the transition matrix:\n\\[\nP = [p_{ij}], \\quad \\text{where each row sums to 1}\n\\]\nThe \\(n\\)-step transition matrix is:\n\\[\nP^{(n)} = P^n = \\underbrace{P \\cdot P \\cdot \\dots \\cdot P}_{n\\text{ times}}\n\\]\nWe can use the Chapman-Kolmogorov Equation:\n\\[\nP^{(n)} = P^{(k)} P^{(n-k)}\n\\]"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#steady-state-distribution",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#steady-state-distribution",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Steady-state distribution",
    "text": "Steady-state distribution\n\nWhen a Markov chain evolves over time, the probability distribution of states may converge to a fixed vector — called the steady-state distribution, \\(\\pi\\).\nA steady-state distribution exists if the chain is:\n\nIrreducible: all states communicate\nAperiodic: not cyclic\nPositive recurrent: expected return time is finite\n\nSteady-state equation: \\(\\pi P = \\pi, \\quad \\sum_i \\pi_i = 1\\)\nThis means:\n\n\\(\\pi_i\\): long-run proportion of time spent in state \\(i\\)\nIt’s a left eigenvector of \\(P\\) corresponding to eigenvalue 1"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-infinitely-many-states",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-infinitely-many-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Classifying states: infinitely many states",
    "text": "Classifying states: infinitely many states\n\nSuppose we have a Markov chain defined on the infinite state space \\({0, 1, 2, …}\\). If it is in state 0 then it moves to state \\(1\\) with probability \\(1\\). If it is in any other state then it moves up with probability \\(p\\) and moves down with probability \\(1-p\\), where \\(0&lt;p&lt;1\\).\n\nKey point: if a Markov chain has infinitely many states, a steady-state distribution might not exist."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-periodic-states",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-periodic-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Classifying states: periodic states",
    "text": "Classifying states: periodic states\n\nSuppose we have a Markov chain which just goes back and forth between states 1 and 2. We call this a periodic Markov chain with a period of 2, because it can only return to the same state after an even number of time steps.\n\nKey point: even if a steady-state distribution exists, the Markov chain might not “converge” to the steady-state distribution unless it actually starts there."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-absorbing-states",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#classifying-states-absorbing-states",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Classifying states: absorbing states",
    "text": "Classifying states: absorbing states\nThis time, we assume that if you’re in state 1 you stay there forever – and the same applies to state 2. In this case we call states 1 and 2 absorbing states.\n\nKey point: if states do not all “communicate” with each other (meaning that you cannot necessarily find a path from one state to another), there could be multiple steady-state distributions."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A",
    "text": "Lets promote Brand A\nAssume patients switch weekly between two brands according to the probabilities shown in the table below:\n\n\n\n\nFrom  To\nBrand A\nBrand B\n\n\n\n\nBrand A\n0.92\n0.08\n\n\nBrand B\n0.15\n0.85\n\n\n\n\n\n\n\n\n\nLet \\(X_n\\) denote the preferred brand (either A or B) of a randomly-chosen customer after \\(n\\) weeks. From the table:\n\\(\\Pr(X_{n+1}=1 \\mid X_n = 1) = 0.92\\)\n\\(\\Pr(X_{n+1}=2 \\mid X_n = 1) = 0.08\\)\n\\(\\Pr(X_{n+1}=1 \\mid X_n = 2) = 0.15\\)\n\\(\\Pr(X_{n+1}=2 \\mid X_n = 2) = 0.85\\)"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\n\n\n\n−+\n10:00\n\n\n\n\nWe can represent this situation using a discrete-time Markov chain:\n\nWe are using some shorthand notation: \\(p_{ij} = Pr(X_{n+1} = j \\,|\\, X_n = i)\\)"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-1",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-1",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nSuppose that after zero weeks, both brands have a 50% market share. This means a randomly-chosen patient has a 50% chance of preferring Brand A.\nSo: \\(Pr(X_0 = 1) = 0.5\\) and \\(Pr(X_0 = 2) = 0.5\\)\nUsing the switching probabilities and invoking the law of total probability, we can calculate the preferred brand of a randomly-chosen patient after 1 week:\n\\(Pr(X_1 = 1) = \\underbrace{Pr(X_0 = 1)Pr(X_1 = 1|X_0 = 1)}_{\\text{(begin in state 1)}} + \\underbrace{Pr(X_0 = 2)Pr(X_1 = 1|X_0 = 2)}_{\\text{(begin in state 2)}} \\\\ =(0.5×0.92)+(0.5×0.15)=0.535\\)\n\n\\(Pr(X_1 = 2) = \\underbrace{Pr(X_0 = 1)Pr(X_1 = 2|X_0 = 1)}_{\\text{(begin in state 1)}} + \\underbrace{Pr(X_0 = 2)Pr(X_1 = 2|X_0 = 2)}_{\\text{(begin in state 2)}} \\\\=(0.5×0.08)+(0.5×0.85)=0.465\\)"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-2",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-2",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nWe can make these calculations look neater by using matrix-vector notation. Let \\(p_{ij}\\) denote the probability of switching from \\(i\\) to \\(j\\). Obviously, this implies:\n\\(p_{ij}\\ge0,\\;\\forall\\,i,j\\in S\\), \\(\\sum_{j\\in S} p_{ij} = 1,\\;\\forall\\,i\\in S.\\)\nLet \\(P\\) denote the transition matrix:\n\\[\n\\mathbf{P} \\;=\\;\n\\begin{pmatrix}\np_{11} & p_{12} \\\\\np_{21} & p_{22}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n0.92 & 0.08 \\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\]\nAlso let \\(q^{(0)}\\) be the vector of initial market shares:\n\\(q^{(0)}=(0.5, 0.5)\\)\nTo get the expected market shares after one week, we multiply \\(q^{(0)}\\) by \\(P\\) to get \\(q^{(1)}\\)\n\\[\n\\mathbf{q}^{(0)} \\mathbf{P}\n\\;=\\;\n(0.5,\\;0.5)\\,\n\\begin{pmatrix}\n0.92 & 0.08\\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\;=\\;\n(0.535,\\;0.465)\n\\;=\\;\n\\mathbf{q}^{(1)}\n\\]"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-3",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-3",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nTo find the expected market shares after two weeks, we repeat the process, starting from the expected market shares after one week. This means we need to calculate\n\\[\n\\overset{\\substack{\\text{expected market shares}\\\\\\text{after one week}}}\n         {\\mathbf{q}^{(1)}}\n\\;\\times\\;\n\\overset{\\text{transition matrix}}{P}\n\\;\\;=\\;\\;\n\\overset{\\substack{\\text{expected market shares}\\\\\\text{after two weeks}}}\n         {\\mathbf{q}^{(2)}}\n\\]\n\\[\n\\mathbf{q}^{(1)} \\mathbf{P}\n\\;=\\;\n(0.535,\\;0.465)\\,\n\\begin{pmatrix}\n0.92 & 0.08\\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\;=\\;\n(0.56195,\\;0.43805)\n\\;=\\;\n\\mathbf{q}^{(2)}\n\\]\nSimilarly, to find the expected market shares after three weeks:\n\\[\n\\mathbf{q}^{(2)} \\mathbf{P}\n\\;=\\;\n(0.56195,\\;0.43805)\\,\n\\begin{pmatrix}\n0.92 & 0.08\\\\\n0.15 & 0.85\n\\end{pmatrix}\n\\;=\\;\n(0.5827015,\\;0.4172985)\n\\;=\\;\n\\mathbf{q}^{(3)}\n\\]"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-4",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-4",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\n\nIn general, to find the expected market shares after \\(n\\) weeks, we calculate\n\\(\\mathbf{q}^{(n-1)}\\,\\mathbf{P} = \\mathbf{q}^{(n)}\\)\n\nThis is the same as:\n\\(\\mathbf{q}^{(0)}\\underbrace{\\mathbf{P}\\,\\mathbf{P}\\,\\cdots\\,\\mathbf{P}}_{\\text{(n times)}} \\;=\\; \\mathbf{q}^{(0)}\\,\\mathbf{P}^n\\)\n\ni.e. the vector of initial market shares multiplied by \\(\\mathbf{P}\\) to the power \\(n\\)."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-5",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-5",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\n\n\n\n−+\n10:00\n\n\n\n\nRecall that: \\(\\mathbf{q}^{(n)}\\) = vector of state probabilities after \\(n\\) time steps (this is the vector of expected market shares in our task) and \\(\\mathbf{P}\\) = transition matrix.\n\nWe have already seen that \\(\\mathbf{q}^{(n)}\\) appears to converge towards a limit as \\(n\\) increases. If we use \\(\\boldsymbol{\\pi}\\) to denote the limiting vector, i.e. \\(\\boldsymbol{\\pi} \\;=\\;\\lim_{n \\to \\infty}\\mathbf{q}^{(n)},\\) then we can take limits to obtain:\n\\[\n\\boldsymbol{\\pi}P = \\boldsymbol{\\pi}\n\\]\nNote: \\(\\boldsymbol{\\pi} = (\\pi_1,\\pi_2)\\), where the numbers \\(\\pi_1\\) and \\(\\pi_2\\) are “unknowns.”"
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-6",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-6",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nBy solving the equations \\(\\boldsymbol{\\pi} = (\\pi_1,\\pi_2)\\) (where \\(\\boldsymbol{\\pi}\\) is a ‘vector of unknowns’) we can calculate the steady-state distribution of the Markov chain.\nRecall the market shares example from earlier. Let \\(\\pi_1\\) and \\(\\pi_2\\) denote the (unknown) steady-state expected market shares for brands A and B respectively. We have:\n\\[\n\\boldsymbol{\\pi}P = \\boldsymbol{\\pi}\n\\quad\\Longleftrightarrow\\quad\n(\\pi_1, \\pi_2)\n\\begin{pmatrix}\n0.92 & 0.08 \\\\\n0.15 & 0.85\n\\end{pmatrix}\n=\n(\\pi_1, \\pi_2).\n\\]\nThis gives us a couple of linear equations in \\(\\pi_1\\) and \\(\\pi_2\\):\n\\[\n\\;\\Longleftrightarrow\\;\n\\begin{cases}\n\\pi_1 = 0.92\\,\\pi_1 + 0.15\\,\\pi_2\\\\[6pt]\n\\pi_2 = 0.08\\,\\pi_1 + 0.85\\,\\pi_2\n\\end{cases}\n\\;\\Longleftrightarrow\\;\n\\begin{cases}\n0.08\\,\\pi_1 - 0.15\\,\\pi_2 = 0\\\\[6pt]\n0.08\\,\\pi_1 - 0.15\\,\\pi_2 = 0\n\\end{cases}\n\\quad\\text{(These equations are the same!)}\n\\]\nTo solve these equations we will also have to use the fact that \\(\\pi_1 + \\pi_2 = 1\\)."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-7",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-7",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nLet’s replace one of the two identical equations with \\(\\pi_1 + \\pi_2 = 1\\). Then we have:\n\\[\n\\begin{cases}\n0.08\\,\\pi_1 - 0.15\\,\\pi_2 = 0\\\\[6pt]\n\\pi_1 + \\pi_2 = 1\n\\end{cases}\n\\]\nIf we substitute \\(\\pi_2 = 1 - \\pi_1\\) into the first equation, this gives:\n\\[\n0.08\\,\\pi_1 \\;-\\; 0.15\\,(1 - \\pi_1) \\;=\\; 0\n\\;\\Longleftrightarrow\\;\n0.23\\,\\pi_1 \\;=\\; 0.15\n\\]\nTherefore;\n\\[\n\\pi_1 \\;=\\;\\frac{0.15}{0.23}\\approx 0.652,\n\\qquad\n\\pi_2 \\;=\\;1 - \\pi_1\\approx 0.348\n\\]\nSo the steady-state expected market shares are roughly: 65.2% for Brand A and 34.8% for Brand B."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-8",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-8",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nIf we use a line graph to show how the expected market shares change with time, we find that both of them appear to converge to fixed values.\n\n\n\n\n\n\n\n\n\nThe market share for Brand A converges to about 65.2%, and the market share for Brand B converges to about 34.8%. We call (0.652, 0.348) the steady-state distribution in this example."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-9",
    "href": "lab/dl4sg_marcov/materials/dl4sg_markov.html#lets-promote-brand-a-cont.-9",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "Lets promote Brand A cont.",
    "text": "Lets promote Brand A cont.\nChanging the initial probability vector \\(\\mathbf{q}^{(0)}\\) has no effect on the steady-state distribution."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/00_dtcm.html",
    "href": "lab/dl4sg_marcov/materials/lab/00_dtcm.html",
    "title": "Home",
    "section": "",
    "text": "This exercise is based on the brand switching example discussed in the slides. It models consumer behavior using a discrete-time Markov chain with two states representing two brands."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/00_dtcm.html#introduction",
    "href": "lab/dl4sg_marcov/materials/lab/00_dtcm.html#introduction",
    "title": "Home",
    "section": "",
    "text": "This exercise is based on the brand switching example discussed in the slides. It models consumer behavior using a discrete-time Markov chain with two states representing two brands."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/00_dtcm.html#excel-based-exploration",
    "href": "lab/dl4sg_marcov/materials/lab/00_dtcm.html#excel-based-exploration",
    "title": "Home",
    "section": "Excel-Based Exploration",
    "text": "Excel-Based Exploration\n\nInitial Setup\nOpen the “Brand switching example – spreadsheet” Excel file.\n\nInitial market shares are in cells B2 (Brand 1) and C2 (Brand 2) — both start at 50%.\nTransition probabilities are found in:\n\nE2: \\(\\Pr(\\text{Brand 1 next week} \\mid \\text{Brand 1 this week})\\)\nF2: \\(\\Pr(\\text{Brand 2 next week} \\mid \\text{Brand 1 this week})\\)\nE3: \\(\\Pr(\\text{Brand 1 next week} \\mid \\text{Brand 2 this week})\\)\nF3: \\(\\Pr(\\text{Brand 2 next week} \\mid \\text{Brand 2 this week})\\)\n\n\nCells B3 and C3 contain formulas to calculate expected market shares after one week.\n\n\nTask 1A\nCopy the formulas in B3 and C3 down for at least 30 rows.\n\nQuestion: How long does it take until the expected market share for Brand 1 exceeds 60%?\n\nAnswer: _________\n\n\nTask 1B\nChange the initial market shares:\n\nCase 1: 30% (Brand 1) and 70% (Brand 2)\nCase 2: 15% (Brand 1) and 85% (Brand 2)\n\n\nQuestion: In each case, how many weeks does it take for Brand 1 to overtake Brand 2?\n\nAnswers: - Case 1: _________\n\nCase 2: _________\n\nRegardless of starting conditions, the system quickly converges to a steady-state distribution.\n\n\nTask 1C\nInspect further down the spreadsheet and record the steady-state expected market shares for both brands to 4 decimal places.\nAnswer:\n\nBrand 1: _________\nBrand 2: _________\n\nThese values are determined by the transition matrix, not the initial shares."
  },
  {
    "objectID": "lab/dl4sg_marcov/materials/lab/00_dtcm.html#using-r-to-find-steady-state-distribution",
    "href": "lab/dl4sg_marcov/materials/lab/00_dtcm.html#using-r-to-find-steady-state-distribution",
    "title": "Home",
    "section": "Using R to Find Steady-State Distribution",
    "text": "Using R to Find Steady-State Distribution\nWe now use R to compute the steady-state distribution algebraically.\nR Function to Solve for Steady-State\n\nlibrary(tidyverse)\n\nsteady_state_dist &lt;- function(P) {\n  # Calculate the steady-state distribution of a discrete-time Markov chain\n  # defined by the transition matrix P\n  \n  # Parameters:\n  # P : matrix\n  #   The transition matrix of the Markov chain of interest.\n  \n  # Returns:\n  # A vector representing the steady-state probabilities for the Markov chain.\n  \n  # Check if P is a square matrix\n  if (!is.matrix(P) || nrow(P) != ncol(P)) {\n    stop(\"P must be a square matrix\")\n  }\n  \n  # Number of states\n  dim &lt;- nrow(P)\n  \n  # Set up the system of equations\n  Q &lt;- P - diag(dim)  # P - I\n  ones &lt;- rep(1, dim) \n  Q &lt;- cbind(Q, ones)  # Append column of ones\n  QTQ &lt;- Q %*% t(Q)    # Compute Q * Q^T\n  bQT &lt;- rep(1, dim)   # Right-hand side vector\n  \n  # Solve the equations and return the solution\n  return(solve(QTQ, bQT))\n}\n\n\nTask 2A\nOpen the R quarto script 00_dtcm.qmd. This script can be used to find the steady-state distribution of a Markov chain, given the transition probability matrix. In our case, we are interested in using it to calculate the steady-state expected market shares for the two brands. Find the steady-state distribution in each of the following cases:\n\\[\nP_1 =\n\\begin{bmatrix}\n0.96 & 0.04 \\\\\n0.07 & 0.93\n\\end{bmatrix}\n\\]\n\n\\[\nP_2 =\n\\begin{bmatrix}\n0.75 & 0.25 \\\\\n0.22 & 0.78\n\\end{bmatrix}\n\\] \n\\[\nP_3 =\n\\begin{bmatrix}\n0.99 & 0.01 \\\\\n0.02 & 0.98\n\\end{bmatrix}\n\\]\n\n# Transition matrices\nmatrices &lt;- list(\n  matrix(c(__, __, __, __), 2, byrow = TRUE),\n  matrix(c(__, __, __, __), 2, byrow = TRUE),\n  matrix(c(__, __, __, __), 2, byrow = TRUE)\n)\n\n# Compute steady-state for each\nfor (P in matrices) {\n  pi_dist &lt;- steady_state_dist(P)\n  print(t(pi_dist))\n}\n\n\n🧠 Even with very similar transition probabilities, the resulting steady-state market shares can be surprisingly different.\n\n\n\nTask 2B – Extra Challenge: Algebraic derivation of steady-state\nLet the transition matrix be:\n\\[\nP = \\begin{bmatrix}\n1 - a & a \\\\\n2a & 1 - 2a\n\\end{bmatrix}, \\quad \\text{where } 0 &lt; a &lt; 0.5\n\\]\nWe want to show the steady-state vector is:\n\\[\n\\boldsymbol{\\pi} = \\left(\\frac{2}{3}, \\frac{1}{3}\\right)\n\\]\n\na_vals &lt;- c(__, __, __, __)\n\nfor (a in a_vals) {\n  P &lt;- matrix(c(__, __, __, __), 2, byrow = TRUE)\n  pi_dist &lt;- steady_state_dist(P)\n  print(t(pi_dist))\n}"
  },
  {
    "objectID": "lab/epss_training/index.html",
    "href": "lab/epss_training/index.html",
    "title": "Demand Forecasting Models for Contraceptive Supply Chain",
    "section": "",
    "text": "Demand Forecasting Models for Contraceptive Supply Chain\n  An introduction to time series forecasting\n  Harsha Chamara Hewage | 2025-03-17\n  \n    \n       Slides\n    \n    \n       GitHub Repo\n    \n  \n\n\n\n  Context\n  Accurate demand forecasting is essential for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory management, and distribution. This course aims to equip pharmaceutical officers at healthcare site levels with the knowledge and tools to adopt modern forecasting methods using R and Python.\n\n\n\n  Target Audience\n  \n    Pharmaceutical officers at healthcare site levels responsible for demand planning\n    Anyone interested in forecasting in the context of contraceptive supply chains\n  \n\n\n\n  Learning Outcomes\n  \n    \n      Day 1: Forecasting with R\n      \n        Familiarize with RStudio and R Notebooks.\n        Learn data wrangling and feature engineering.\n        Understand time series graphics.\n        Explore models: sNAIVE, Moving Average, ARIMA, ETS.\n        Evaluate model performance.\n      \n    \n    \n      Day 2: Advanced Forecasting with Python\n      \n        Familiarize with Google Colab and Python Notebooks.\n        Implement advanced models: LightGBM, XGBoost, and TimeGPT.\n        Evaluate advanced model performance.\n        Explore additional learning resources.\n      \n    \n  \n\n\n\n  Preparation & Prerequisites\n  No prior experience in time series is required. However, familiarity with the following is recommended:\n  \n    Writing R code and using tidyverse packages (dplyr, ggplot2). Learn R here.\n    Writing basic Python code. Learn Python here.\n    Basic statistical concepts such as mean, variance, and regression.\n    Please have a laptop capable of running both R and Python."
  },
  {
    "objectID": "lab/intro_python/index.html#who-is-the-course-for",
    "href": "lab/intro_python/index.html#who-is-the-course-for",
    "title": "A basic introduction to Python",
    "section": "Who is the course for?",
    "text": "Who is the course for?\nThis course is primarily aimed at learners who require a practical introduction to Python. It assumes no previous experience using Python."
  },
  {
    "objectID": "lab/intro_python/index.html#learning-objectives",
    "href": "lab/intro_python/index.html#learning-objectives",
    "title": "A basic introduction to Python",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nFamiliarize yourself with Python and an Integrated Development Environment (IDE) such as Jupyter Lab or VS Code, which we’ll use to interact with Python.\nUnderstand the basics of working with Python, including how to write and execute code.\nUse three different ways to work with Python: Python Shell, Python Scripts, and Jupyter Lab.\nLearn about Python’s basic data types and structures.\nInstall and import required libraries.\nFind resources for help when coding in Python."
  },
  {
    "objectID": "lab/intro_python/index.html#prerequisites",
    "href": "lab/intro_python/index.html#prerequisites",
    "title": "A basic introduction to Python",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nNo prior knowledge of Python is assumed.\nInstall Python and GitHub Desktop."
  },
  {
    "objectID": "lab/intro_python/index.html#course-topics",
    "href": "lab/intro_python/index.html#course-topics",
    "title": "A basic introduction to Python",
    "section": "Course Topics",
    "text": "Course Topics\n\nPython and IDE Essentials\n\nSection 1: Introduction to Python and IDEs\n\nIntroduction to Python\nInstalling Python\nSetting up an Integrated Development Environment (IDE)\nOverview of Jupyter Lab, VS Code, Google Colab, and Anaconda\n\n\n\nSection 2: Working with Jupyter Lab\n\nUnderstanding the basic syntax\nWriting and executing code in Python\nSetting up and organizing a Python project"
  },
  {
    "objectID": "talks/dl4sg_seminar/index.html#context",
    "href": "talks/dl4sg_seminar/index.html#context",
    "title": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains",
    "section": "Context",
    "text": "Context\nIn this presentation for the Data Lab for Social Good seminar, we explore how traditional forecasting methods often fail to capture unmet demand in contraceptive supply chains—particularly when stockouts censor the data. Our work combines temporal kernel forecasting, conformal prediction, and inventory-aware simulation to estimate true demand beyond what the system records.\nWe show how “no demand” often just means “no stock,” and how our approach can support smarter, more equitable inventory decisions. Our next steps include expanding the policy framework, incorporating external covariates, and testing the model in both lab and field settings with real demand planners.\nBecause unmet demand isn’t invisible—our systems just fail to see it."
  },
  {
    "objectID": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#section",
    "href": "talks/dl4sg_seminar/slides/dl4sg_seminar_wp2.html#section",
    "title": "Home",
    "section": "",
    "text": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#outline",
    "href": "talks/qff_london/slides/qff_london.html#outline",
    "title": "Home",
    "section": "Outline",
    "text": "Outline\n\n\nWhat was never counted...\nThe fundamental question\nWhat we are going to do\nNumerical experiment\nWhat’s NEXT"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#seen-the-unseen",
    "href": "talks/qff_london/slides/qff_london.html#seen-the-unseen",
    "title": "Home",
    "section": "Seen the UNSEEN",
    "text": "Seen the UNSEEN\n\n\nHuman story: What data misses\n\nNilu went to a pharmacy for Product A. It was not in stock and the system logs it as zero demand.\n\nWhen data is censored by stockouts or service interruptions… Forecasts fail… Not just by being wrong, but by being blind.\n\nThis creates broken trust and leads to UNMET DEMAND.\n\n\n\nAnalytical reality: Why this matters\n\nStockouts censor demand.\nObserved sales ≠ actual demand.\n\nInventory decisions based on this false signal?\nUnderstocking → more stockouts.\n\nForecasts don’t just underperform. They miss the whole story.\n\nContraceptives aren’t easily substitutable. A lost sale = a lost opportunity for care."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#contraceptive-products-arent-easily-substituted",
    "href": "talks/qff_london/slides/qff_london.html#contraceptive-products-arent-easily-substituted",
    "title": "Home",
    "section": "Contraceptive products aren’t easily SUBSTITUTED",
    "text": "Contraceptive products aren’t easily SUBSTITUTED\n\n\n\n\n*Percent of women who will get pregnant within the first year of typical use."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#the-big-picture",
    "href": "talks/qff_london/slides/qff_london.html#the-big-picture",
    "title": "Home",
    "section": "The BIG PICTURE",
    "text": "The BIG PICTURE\n\n\n\n\n\nIn reality…\nThere are more than\n218 million women\nlike Nilu still have an unmet need for family planning.\nUltimately, this results in dropouts, unwanted pregnancies, and almost 7 million hospitalizations each year in developing countries.\n\n\n\nImage generated using ChatGPT.\nSources: USAID, 2020; PATH, 2019; Mukasa et al., 2017; Gilda et al., 2016"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#why-this-is-critical",
    "href": "talks/qff_london/slides/qff_london.html#why-this-is-critical",
    "title": "Home",
    "section": "Why this is critical",
    "text": "Why this is critical\n\n\n\n\n\n\n\nFrequent stockouts are common in family planning supply chains, especially in developing countries, significantly impacting public health outcomes.\n\n\n\n\nDuring my recent field visit to Ethiopia, stockouts were repeatedly identified by demand planners as a major barrier to effective contraceptive supply management.\n\n\n\n\nTraditional forecasting methods fail under censorship.\n\n\n\n\nPrior research inadequately addresses demand estimation under conditions of frequent stockouts and interruptions, often leading to biased forecasts and suboptimal inventory decisions.\n\n\n\n\n\nImage generated using ChatGPT.\nSources: Bijvank et al., 2011; Karimi et al., 2021; Thanos et as., 2022 ; Trapero, 2024"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#key-definitions",
    "href": "talks/qff_london/slides/qff_london.html#key-definitions",
    "title": "Home",
    "section": "Key definitions",
    "text": "Key definitions\n\n\nStockouts: Periods when demand is higher than available inventory, leading to censored observations of demand.\nInterruptions: Periods when no products are issued despite available stock, thus artificially recorded as zero demand.\nCensored Demand: Demand occurring during periods when products are unavailable (stockouts or interruptions), thus not fully observable.\nTrue Demand: Actual demand that would have occurred if sufficient stock was available."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#censorship-scenarios",
    "href": "talks/qff_london/slides/qff_london.html#censorship-scenarios",
    "title": "Home",
    "section": "Censorship scenarios",
    "text": "Censorship scenarios\nHow can a demand forecasting and inventory optimization model that incorporates lost sales estimation and contextual field data enhance contraceptive supply chain performance and reduce stockouts in developing countries?\n\n\nFigure 1: Censorship scenarios in family planning supply chains."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#how-we-can-fill-the-gaps",
    "href": "talks/qff_london/slides/qff_london.html#how-we-can-fill-the-gaps",
    "title": "Home",
    "section": "How we can fill the gaps",
    "text": "How we can fill the gaps\n\n\n\n\n\n\n\n\nRQ1: How accurately can a Tobit Kalman Filter with conformal prediction estimate true demand under censorship?\n\n\n\n\n\nRQ2: How does demand reconstruction improve inventory performance compared to baseline planning methods?\n\n\n\n\nRQ3: How do planners adjust their orders in response to proposed model-generated recommendations?\n\n\n\n\n\nImage generated using ChatGPT."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#our-proposed-framework",
    "href": "talks/qff_london/slides/qff_london.html#our-proposed-framework",
    "title": "Home",
    "section": "Our proposed framework",
    "text": "Our proposed framework"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#first-stage-estimating-true-demand-under-censorship-using-tobit-kalman-filtering-and-conformal-prediction",
    "href": "talks/qff_london/slides/qff_london.html#first-stage-estimating-true-demand-under-censorship-using-tobit-kalman-filtering-and-conformal-prediction",
    "title": "Home",
    "section": "First stage: estimating true demand under censorship using tobit kalman filtering and conformal prediction",
    "text": "First stage: estimating true demand under censorship using tobit kalman filtering and conformal prediction"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#first-stage-estimating-true-demand-under-censorship-using-tobit-kalman-filtering-and-conformal-prediction-1",
    "href": "talks/qff_london/slides/qff_london.html#first-stage-estimating-true-demand-under-censorship-using-tobit-kalman-filtering-and-conformal-prediction-1",
    "title": "Home",
    "section": "First stage: estimating true demand under censorship using tobit kalman filtering and conformal prediction",
    "text": "First stage: estimating true demand under censorship using tobit kalman filtering and conformal prediction"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#experiment-setup",
    "href": "talks/qff_london/slides/qff_london.html#experiment-setup",
    "title": "Home",
    "section": "Experiment setup",
    "text": "Experiment setup"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#synthetic-data-exploration---example",
    "href": "talks/qff_london/slides/qff_london.html#synthetic-data-exploration---example",
    "title": "Home",
    "section": "Synthetic data exploration - example",
    "text": "Synthetic data exploration - example\n\nActual vs. observed demand for one representative series per type × category, with disruptions and censoring shaded."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#overall-forecasting-and-inventory-performance-across-models",
    "href": "talks/qff_london/slides/qff_london.html#overall-forecasting-and-inventory-performance-across-models",
    "title": "Home",
    "section": "Overall forecasting and inventory performance across models",
    "text": "Overall forecasting and inventory performance across models\n\n\n\n\nMethod\nMASE (mean)\nPin Ball Loss - q95 (mean)\nCSL (mean)\nLost Sales Rate (mean)\nInventory Coverage (mean)\n\n\n\n\nTKF CP\n0.87\n47.61\n0.86\n0.14\n5.25\n\n\nMoving Average\n1.06\n72.65\n0.82\n0.18\n19.6\n\n\nLinear Regression\n1.08\n73.86\n0.82\n0.16\n2.55\n\n\nNaive\n1.21\n78.89\n0.84\n0.16\n123.38"
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#performance-evaluation---nemenyi-test",
    "href": "talks/qff_london/slides/qff_london.html#performance-evaluation---nemenyi-test",
    "title": "Home",
    "section": "Performance evaluation - Nemenyi test",
    "text": "Performance evaluation - Nemenyi test\n\n\nFigure 2: Average ranks of forecasting methods with 95% confidence intervals based on the Nemenyi test for all metrics. Lower ranks indicate better performance."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#performance-evaluation---forecasting",
    "href": "talks/qff_london/slides/qff_london.html#performance-evaluation---forecasting",
    "title": "Home",
    "section": "Performance evaluation - forecasting",
    "text": "Performance evaluation - forecasting\n\n\nFigure 3: Forecasting metrics for each series type for the different forecasting methods."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#performance-evaluation---inventory",
    "href": "talks/qff_london/slides/qff_london.html#performance-evaluation---inventory",
    "title": "Home",
    "section": "Performance evaluation - inventory",
    "text": "Performance evaluation - inventory\n\n\nFigure 4: Inentory metrics for each series type for the different forecasting methods."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#way-forward",
    "href": "talks/qff_london/slides/qff_london.html#way-forward",
    "title": "Home",
    "section": "Way forward",
    "text": "Way forward\n\n\n\n\nDevelop a quantile-based inventory policy → Incorporate uncertainty directly into order decisions\n\nExtend empirical model with external covariates → Account for special events, disruptions, and policy shifts\n\nConduct lab experiment with real demand planners → Measure how model recommendations affect decision-making\n\n\n\nImage generated using ChatGPT."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#materials",
    "href": "talks/qff_london/slides/qff_london.html#materials",
    "title": "Home",
    "section": "Materials",
    "text": "Materials\n\n\nYou can find the slides here."
  },
  {
    "objectID": "talks/qff_london/slides/qff_london.html#section",
    "href": "talks/qff_london/slides/qff_london.html#section",
    "title": "Home",
    "section": "",
    "text": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains"
  },
  {
    "objectID": "talks/qff_london/index.html#context",
    "href": "talks/qff_london/index.html#context",
    "title": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains",
    "section": "Context",
    "text": "Context\nIn this presentation for the IIF UK Chapter: Quarterly Forecasting Forum seminar, we explore how traditional forecasting methods often fail to capture unmet demand in contraceptive supply chains—particularly when stockouts censor the data. Our work combines temporal kernel forecasting, conformal prediction, and inventory-aware simulation to estimate true demand beyond what the system records.\nWe show how “no demand” often just means “no stock,” and how our approach can support smarter, more equitable inventory decisions. Our next steps include expanding the policy framework, incorporating external covariates, and testing the model in both lab and field settings with real demand planners.\nBecause unmet demand isn’t invisible—our systems just fail to see it."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#outline",
    "href": "lab/quarto_website/slides/quarto_website.html#outline",
    "title": "Home",
    "section": "Outline",
    "text": "Outline\n\n\n\n\n\n\n\n\nConnect R Studio with GIT\n\n\n\n\n\n\nLets build the home page\n\n\n\n\n\n\nHow to style your website\n\n\n\n\n\n\nPages and listings\n\n\n\n\n\n\nLets publish your website\n\n\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#assumptions",
    "href": "lab/quarto_website/slides/quarto_website.html#assumptions",
    "title": "Home",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nYou are expected to be comfortable with R and Git.\nHave used Quarto to generate documents (e.g. HTML, PDF, MS Word etc.)\nAre comfortable editing plain text documents (e.g .qmd) in your IDE (e.g. RStudio, Visual Studio Code etc.)\nWant to walk away with your own personal website.\nWe won’t assume you have any HTML, CSS/SCSS or Git/GitHub experience.\n\n\n\n\n\n\nWhat we will cover?\n\nCreate the home page\nSimple styling such as font, colour, size\nListing and navigation\n\n\n\n\n\nWhat we will not cover\n\nAdvance styling using css and scss\nAdvance navigation options\nCustomise templates\nRun code blocks"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#materials",
    "href": "lab/quarto_website/slides/quarto_website.html#materials",
    "title": "Home",
    "section": "Materials",
    "text": "Materials\n\nYou can find the workshop materials https://chamara7h.github.io/lab/ or simply scan the QR code.\n\nNote: These materials are based on my learnings at;\n\nPosit PBC: Quarto websites video series.\nIntroduction to Git and GitHub for R Users by Nicola Rennie.\nPublish a Quarto project using GitHub Pages by Melissa Van Bussel."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#using-git-and-github-with-rstudio",
    "href": "lab/quarto_website/slides/quarto_website.html#using-git-and-github-with-rstudio",
    "title": "Home",
    "section": "Using Git and GitHub with RStudio",
    "text": "Using Git and GitHub with RStudio\n\nWhat you need:\n\nR, RStudio, and Git installed on your laptop\nA GitHub account\nGitHub credentials\n\nOptional (but strongly recommended):\n\nThe usethis R package\nHappy Git with R"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#lets-setup-git-in-r-studio---live-demo",
    "href": "lab/quarto_website/slides/quarto_website.html#lets-setup-git-in-r-studio---live-demo",
    "title": "Home",
    "section": "Lets setup Git in R Studio - Live demo",
    "text": "Lets setup Git in R Studio - Live demo\n\n\n\nTerminal\n\n# check git is installed\ngit --version\n\n\nThen,\n\n\nR script\n\n# from R: configure git (usethis)\nlibrary(usethis)\nuse_git_config(\n  user.name = \"your-username\",\n  user.email = \"you@example.org\"\n)\n\n# create a GitHub personal access token (opens browser)\ncreate_github_token()\n\n# store credentials locally (prompts for token paste)\nlibrary(gitcreds)\ngitcreds_set()"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\nCreate a GitHub account if you don’t already have one, and make sure you have Git installed on your laptop.\nInstall and load the usethis and gitcreds packages.\nConfigure git then run create_github_token(), and follow the instructions to generate a token.\nRun gitcreds_set() and paste in the token when prompted.\n\n\n\n\n\n\n−+\n05:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#start-with-a-template",
    "href": "lab/quarto_website/slides/quarto_website.html#start-with-a-template",
    "title": "Home",
    "section": "Start with a template",
    "text": "Start with a template\n\nStart somewhere logical (Mac and Windows OS):\n\n\nTerminal\n\ncd ~/Documents/website  \n\nOR (Windows OS)\n\n\nTerminal\n\ncd /d/GIT\n\nGet the website template:\n\n\nTerminal\n\nquarto use template chamara7h/quarto_website_template\n\nFollow the prompts:\n\n\nTerminal\n\nQuarto templates may execute code when documents are rendered. If you do not \ntrust the authors of the template, we recommend that you do not install or \nuse the template.\n? Do you trust the authors of this template (Y/n) › Y\n? Create a subdirectory for template? (Y/n) › Y\n? Directory name: › website"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-1",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-1",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\nGet the starter template.\nOpen index.qmd\nPreview index.qmd\n\n\n\n\n\n\n−+\n02:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#home-page-structure",
    "href": "lab/quarto_website/slides/quarto_website.html#home-page-structure",
    "title": "Home",
    "section": "Home page structure",
    "text": "Home page structure\n\n\n\nindex.qmd\n\n---\ntitle: \"Bruce Wayne\"\nsubtitle: \"I am Batman\"\nimage: profile.jpg\nabout: # https://quarto.org/docs/websites/website-about.html\n  template: jolla\n  links:\n    - icon: github\n      text: Github\n      href: https://github.com\n    - icon: linkedin\n      text: LinkedIn\n      href: https://linkedin.com\n---\n\nA little bit about me and my life.\n\n## Education\n\nUniversity of XYZ, City \\| Location \\| Sept 20XX - June 20XX\n\n## Experience\n\nWorkplace \\| Job title \\| April 20XX - present\n\n\nWebpages are like any other Quarto document:\n\nStart with a YAML header\nCan include code cells\nEverything else is markdown content"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#about-key-in-yaml",
    "href": "lab/quarto_website/slides/quarto_website.html#about-key-in-yaml",
    "title": "Home",
    "section": "About key in YAML",
    "text": "About key in YAML\n\n\nLanding/ home page controlled by the about key in YAML.\nContent and YAML values are combined using a template:\n\n---\nabout: # https://quarto.org/docs/websites/website-about.html\n  template: jolla\n  links:\n    - icon: github\n      text: Github\n      href: https://github.com\n    - icon: linkedin\n      text: LinkedIn\n      href: https://linkedin.com\n---\n\nDifferent templates: jolla, trestles, solana, marquee, or broadside"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#how-to-set-the-image",
    "href": "lab/quarto_website/slides/quarto_website.html#how-to-set-the-image",
    "title": "Home",
    "section": "How to set the image",
    "text": "How to set the image\n\nUsed in about template, and social cards.\n\n\n\n\nindex.qmd\n\n---\nimage: images/profile.jpg\nabout: # https://quarto.org/docs/websites/website-about.html\n  template: jolla\n  image-shape: round\n---\n\n\nwebsite/\n├── _quarto.yml\n├── images/\n│   └──  profile.jpg\n└── index.qmd\n\n\nUse a path relative to index.qmd\nImage shapes: rectangle, round and rounded"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-2",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-2",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\nEdit the content in index.qmd to reflect you.\nTry different templates and pick one.\nEdit the links to point at your own profiles, or add different links.\nReplace the image with your image.\nExperiment with image-shape.\n\n\n\n\n\n\n−+\n05:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#understand-the-website-structure",
    "href": "lab/quarto_website/slides/quarto_website.html#understand-the-website-structure",
    "title": "Home",
    "section": "Understand the website structure",
    "text": "Understand the website structure\n\nA minimal website has two files: index.qmd and _quarto.yml\n\nindex.qmd: Renders to index.html, your home page.\n_quarto.yml: Controls project and website properties.\n\n\nWhen rendered you will get a _site/ folder. This contains everything needed to serve the site."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#styling-using-yaml",
    "href": "lab/quarto_website/slides/quarto_website.html#styling-using-yaml",
    "title": "Home",
    "section": "Styling using YAML",
    "text": "Styling using YAML\n\nWith Quarto html output we have a number of ways to style the site.\nA number of basic options allows us to change colors, fonts, and SIZES.\n\n\n\n\n\nColours\n\nfontcolor: The main color for text on the site\nlinkcolor: Color of the links\nbackgroundcolor: Background color of the whole site\nmonobackgroundcolor: background for code chunks\n\n\n\n\n_quarto.yml\n\n---\nformat:\n  html:\n    backgroundcolor: \"#eeffee\"\n    fontcolor: \"darkgreen\"\n    linkcolor: \"black\"\n---\n\n\nNeed more colours: Figma colour pallet"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#styling-using-yaml-1",
    "href": "lab/quarto_website/slides/quarto_website.html#styling-using-yaml-1",
    "title": "Home",
    "section": "Styling using YAML",
    "text": "Styling using YAML\n\nWith Quarto html output we have a number of ways to style the site.\nA number of basic options allows us to change colors, fonts, and SIZES.\n\n\n\n\nFonts\n\nmainfont: The main font for text on the site\nmonofont: Font for code elements\n\n\n\n\n_quarto.yml\n\n---\nformat:\n  html:\n    backgroundcolor: \"#eeffee\"\n    fontcolor: \"darkgreen\"\n    linkcolor: \"black\"\n    mainfont: \"monospace\"\n---\n\n\n\ngeneric families include; serif, sans-serif, monospace, cursive, fantasy, system-ui, ui-serif, ui-sans-serif, ui-monospace, ui-rounded, and fangsong."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#styling-using-yaml-2",
    "href": "lab/quarto_website/slides/quarto_website.html#styling-using-yaml-2",
    "title": "Home",
    "section": "Styling using YAML",
    "text": "Styling using YAML\n\nWith Quarto html output we have a number of ways to style the site.\nA number of basic options allows us to change colors, fonts, and SIZES.\n\n\n\n\nSizes\n\nmax-width: Width of the main text area of the pages\nfontsize: Base font size for website\nlinestretch: Distance between lines of text\n\n\n\n\n_quarto.yml\n\n---\nformat:\n  html:\n    backgroundcolor: \"#eeffee\"\n    fontcolor: \"darkgreen\"\n    linkcolor: \"black\"\n    mainfont: \"monospace\"\n    fontsize: 20px\n---\n\n\n\n\nRemember!!! You can always do further styling using custom styles in CSS."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-3",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-3",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\nOpen up your _quarto.yml file and experiment with the colors.\nOpen up your _quarto.yml file and experiment with the fonts.\nOpen up your _quarto.yml file and experiment with the sizing.\n\n\n\n\n\n\n−+\n05:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#adding-pages",
    "href": "lab/quarto_website/slides/quarto_website.html#adding-pages",
    "title": "Home",
    "section": "Adding Pages",
    "text": "Adding Pages\n\nTwo decisions:\n\nStructure Where will it live in your website project?\nNavigation How will people discover it on your website?\n\nindex.html is special\n\n\n\nFile location\nindex.qmd\ntalks/index.qmd\n\nURL\n{ site url }\n{ site url }/talks\n\nindex.qmd (or .md, or .ipynb) -&gt; index.html\nindex.html acts like a default page for the site or directory.\n\n\nYou aren’t limited to .html. We can also use other file types as .pdf or .csv."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#navigation-bar",
    "href": "lab/quarto_website/slides/quarto_website.html#navigation-bar",
    "title": "Home",
    "section": "Navigation bar",
    "text": "Navigation bar\n\nIn _quarto.yml under website: navbar\n\n\nGive a path from site root\n\n\n_quarto.yml\n\nwebsite:\n  navbar:\n    right:\n      - text: Projects\n        href: projects/index.qmd\n      - text: Talks\n        href: talks/index.qmd\n\n\n\nOther customizations\n\n\n_quarto.yml\n\nwebsite:\n  navbar:\n    title: Data Lab for Social Good \n    logo: images/logo.png\n    search: true\n    left:\n      - text: Home\n        href: index.qmd\n        icon: house-door-fill\n    right:\n      - text: Projects\n        href: projects/index.qmd\n      - text: Talks\n        href: talks/index.qmd\n      tools:\n      - icon: github\n        href: https://github.com/\n      - icon: linkedin\n        href: https://www.linkedin.com/\n\n\n\n\nOptions for icons: Bootstrap Icons"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#primary-navigation",
    "href": "lab/quarto_website/slides/quarto_website.html#primary-navigation",
    "title": "Home",
    "section": "Primary navigation",
    "text": "Primary navigation\n\n\n\nTop navigation\n\n\n_quarto.yml\n\nwebsite:\n  navbar:\n    right:\n      - text: Projects\n        href: projects/index.qmd\n    tools:\n      - icon: github\n        href: https://github.com/\n\n\nAdd items to left, right and tools\n\n\nSide navigation\n\n\n_quarto.yml\n\nwebsite:\n  sidebar:\n    contents:\n      - text: Projects\n        href: projects/index.qmd\n    tools:\n      - icon: github\n        href: https://github.com/    \n\n\nAdd items to contents and tools"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#nested-navigation",
    "href": "lab/quarto_website/slides/quarto_website.html#nested-navigation",
    "title": "Home",
    "section": "Nested navigation",
    "text": "Nested navigation\n\n\n\nTop navigation\n\n\n_quarto.yml\n\nwebsite:\n  navbar:\n    left:\n      - index.qmd\n      - text: Work\n        menu: \n          - talks/index.qmd\n          - projects/index.qmd\n\n\nAdd a text item along with menu\n\n\nSide navigation\n\n\n_quarto.yml\n\nwebsite:\n  sidebar:\n    contents:\n      - index.qmd\n      - section: Work\n        contents: \n          - talks/index.qmd\n          - projects/index.qmd\n\n Add a section with its own contents\n\n\n\nRead more about page navigation"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#bonus-page-footer",
    "href": "lab/quarto_website/slides/quarto_website.html#bonus-page-footer",
    "title": "Home",
    "section": "Bonus: Page footer",
    "text": "Bonus: Page footer\n\n\n\n_quarto.yml\n\nwebsite:\n  page-footer: \n    left: \"Copyright 2025, DL4SG\" \n    right: \n      - icon: github\n        href: https://github.com/\n      - icon: twitter \n        href: https://twitter.com/"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-4",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-4",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\nCustomize your navigation bar title\nExperiment with the position of your links in the navbar: left, right or a mix\nAdd at least one item to tools in your navbar\nTry both top navigation and side navigation\n\n\n\n\n\n\n−+\n05:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#break",
    "href": "lab/quarto_website/slides/quarto_website.html#break",
    "title": "Home",
    "section": "Break",
    "text": "Break\n\n\n\n\n\n\nCtrl+Alt+Coffee ☕ — reboot in 10 mins.\n\n\n\n\n\n\n−+\n10:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#what-is-listing-and-why-we-need-it",
    "href": "lab/quarto_website/slides/quarto_website.html#what-is-listing-and-why-we-need-it",
    "title": "Home",
    "section": "What is listing and why we need it?",
    "text": "What is listing and why we need it?\n\n\n\nWhat?\n\nan automatically generated list of content\nstyled via a template, (built-in type, or custom template)\ncan be included on any page\n\n\n\n\nWhy?\n\nGreat for large collections\nGreat for collections that grow"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#lets-create-lisitng-for-blog-page",
    "href": "lab/quarto_website/slides/quarto_website.html#lets-create-lisitng-for-blog-page",
    "title": "Home",
    "section": "Lets create lisitng for blog page",
    "text": "Lets create lisitng for blog page\n\nblog/ has some folders with documents in them\nDemo:\n\nMake a new page blog/index.qmd\nMake it a listing:\n\n\n\n\nAs simple as:\n---\ntitle: My Blog\nlisting: default\n---\n\n\nExplicit default options:\n---\ntitle: My Blog\nlisting:\n  contents: /\n  type: default\n---\n\nExplore more listing types; default, grid or table"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-5",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-5",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\n(Repeat) Make a new page blog/index.qmd which is a listing\nExperiment with type: default, grid or table\n\n\n\n\n\n\n−+\n05:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#more-options-in-listings",
    "href": "lab/quarto_website/slides/quarto_website.html#more-options-in-listings",
    "title": "Home",
    "section": "More options in listings",
    "text": "More options in listings\n\nUse listings for projects page\n\nNote: contents can be a YAML file\n\n\n\n\n\n\nprojects/index.qmd\n\n---\ntitle: Projects\nlisting:\n  contents: projects.yml\n  type: grid\n  max-description-length: 250\n---\n\n\n\n\nproject/projects.yml\n\n- title: Predicting House Prices with Machine Learning\n  path: https://example.com/house-prices\n  # Photo by Breno Assis on Unsplash https://unsplash.com/photos/aerial-photography-of-rural-r3WAWU5Fi5Q \n  image: images/breno-assis-r3WAWU5Fi5Q-unsplash.jpg\n  description: &gt;\n    This project involves using machine learning algorithms to predict house prices based on\n    various features such as location, size, and amenities. It includes data cleaning,\n    feature engineering, and model selection.\n  categories: [Python, Machine Learning, Data Cleaning]\n  date: 2024-01-01\n  ...\n\nPath can be a relative path to a file in your site, or a URL\nYou can use Listing Fields, or create custom ones."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#simple-customizations",
    "href": "lab/quarto_website/slides/quarto_website.html#simple-customizations",
    "title": "Home",
    "section": "Simple customizations",
    "text": "Simple customizations\n\n\n\nSort and filtering options\n\n\n\nprojects/index.qmd\n\n---\ntitle: Projects\nlisting:\n  contents: projects.yml\n  type: grid\n  max-description-length: 250\n  sort-ui: false\n  sort: date desc\n  filter-ui: true\n---\n\n\n\nUse field-display-names to provide a different label for a field\n\n\nprojects/index.qmd\n\n---\ntitle: Projects\nlisting:\n  contents: projects.yml\n  type: grid\n  max-description-length: 250\n  sort-ui: false\n  sort: date desc\n  filter-ui: true\n  fields: [title, description, categories]\n  field-display-names: \n      title: 'Project'\n      categories: 'Skills'\n---"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#ymal-keys-for-a-blog-or-project-file",
    "href": "lab/quarto_website/slides/quarto_website.html#ymal-keys-for-a-blog-or-project-file",
    "title": "Home",
    "section": "YMAL keys for a blog or project file",
    "text": "YMAL keys for a blog or project file\n\nLets open blog/third-post/index.qmd\n\n\n\nAs simple as:\n---\ntitle: \"Second Post\"\ndescription: \"Post description for second post\"\nauthor: \"Bruce Wayne\"\ndate: \"5/23/2021\"\n---\n\n\nAdd categories and a cover image\n---\ntitle: \"Second Post\"\ndescription: \"Talk\"\nauthor: \"Bruce Wayne\"\ndate: \"5/23/2021\"\nimage: \"cover.jpg\"\ncategories:\n  - Forecasting\n  - Healthcare\n  - R\n  - Python\n  - Machine Learning\nlink-external-newwindow: true\n---"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#lets-add-custom-buttons",
    "href": "lab/quarto_website/slides/quarto_website.html#lets-add-custom-buttons",
    "title": "Home",
    "section": "Lets add custom buttons",
    "text": "Lets add custom buttons\n\n\nPaste this in styles.scss\n\n\n\nstyles.scss\n\n\n/* custom button */\n\nbutton {\n  background-color: transparent; /* No filling */\n  border: 2px solid rgb(55, 58, 60); /* Gray border */\n  color: rgb(55, 58, 60); /* Gray text */\n  padding: 5px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 12px;\n  margin: 4px 2px;\n  cursor: pointer;\n  border-radius: 5px; /* Rounded corners */\n}\n\n/* Hover effect */\nbutton:hover {\n  border-color: #0066b2; /* Blue border on hover */\n  color: #0066b2; /* Blue text on hover */\n}\n\n/* Hover effect for Journal article */\n#journal-article-btn:hover {\n  border-color: #0066b2; /* Blue border on hover */\n  color: #0066b2; /* Blue text on hover */\n}\n\n/* Hover effect for GitHub repo */\n#github-repo-btn:hover {\n  border-color: #00A86B; /* Green border on hover */\n  color: #00A86B; /* Green text on hover */\n}\n\n\nAdd text to the body and a link:\n\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\"&gt;\n\n&lt;button id=\"journal-article-btn\" \n        onclick=\"window.open('slides/intro_method_skills.html', '_blank')\"&gt;\n  &lt;i class=\"fas fa-bookmark\"&gt;&lt;/i&gt; Slides\n&lt;/button&gt;\n\n&lt;button id=\"github-repo-btn\" onclick=\"window.open('https://github.com/', '_blank');\"&gt;\n  &lt;i class=\"fab fa-github\"&gt;&lt;/i&gt; GitHub repo\n&lt;/button&gt;\nHeading\n\n\n Slides\n\n\n GitHub repo\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-6",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-6",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\nTest different customisations in listing\nEdit and customise yml in one of your blog posts\nAdd all the pages to your site navigation\n\n\n\n\n\n\n−+\n10:00\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#create-a-new-quarto-project",
    "href": "lab/quarto_website/slides/quarto_website.html#create-a-new-quarto-project",
    "title": "Home",
    "section": "Create a new Quarto project",
    "text": "Create a new Quarto project\n\n\nIn RStudio, go to File &gt; New Project &gt; New Directory &gt; Quarto Website.\nFor the directory name, use the URL you want for your website.\n\nIf it’s a personal site, use the format your-username.github.io. For example, if your username is “batman”, the directory name would be batman.github.io.\nIf the site is for a GitHub organization, use the format organization-name.github.io.\n\nChoose a location on your computer to save the project.\nEnsure that “Create a git repository” and “Use renv with this project” are both checked.\nClick “Create Project”."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#create-a-github-repository",
    "href": "lab/quarto_website/slides/quarto_website.html#create-a-github-repository",
    "title": "Home",
    "section": "Create a GitHub repository",
    "text": "Create a GitHub repository\n\nGo to GitHub and create a new repository.\nThe repository name must exactly match the directory name you created in RStudio (e.g., your-username.github.io).\nFor free accounts, you must set the repository to Public for GitHub Pages to work.\nClick “Create repository”.\nLink your local project folder to the new GitHub repository and push the initial files.\n\n\n\n\nTerminal/ Bash\n\n# push an exisitng repo\ngit remote add origin https://github.com/&lt;your user name&gt;.github.io\ngit branch -M main\n\n# push your initial changes\ngit status\ngit add .\ngit commit -m 'initial commit'\ngit push origin main"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#create-a-gh-pages-branch",
    "href": "lab/quarto_website/slides/quarto_website.html#create-a-gh-pages-branch",
    "title": "Home",
    "section": "Create a gh-pages Branch",
    "text": "Create a gh-pages Branch\n\n\nAfter pushing your project, go to your repository on GitHub.\nClick on the branch dropdown menu (which should say “main”) and select “View all branches”.\nClick “New branch”.\nName the new branch gh-pages and click “Create new branch”."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#set-up-the-github-actions-workflow",
    "href": "lab/quarto_website/slides/quarto_website.html#set-up-the-github-actions-workflow",
    "title": "Home",
    "section": "Set up the GitHub actions workflow",
    "text": "Set up the GitHub actions workflow\n\n\nIn your RStudio project, create a new folder named .github.\nInside the .github folder, create another folder named workflows.\nInside the workflows folder, create a new text file and name it publish.yml.\nGo to the Quarto documentation page about publishing to GitHub Pages. Find the example for a GitHub Action that uses renv (go here: https://quarto.org/docs/publishing/github-pages.html#example-knitr-with-renv).\nCopy the entire code block from that example.\nPaste this code into your publish.yml file and save it and commit changes."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#configure-github-pages-settings",
    "href": "lab/quarto_website/slides/quarto_website.html#configure-github-pages-settings",
    "title": "Home",
    "section": "Configure GitHub pages settings",
    "text": "Configure GitHub pages settings\n\n\nOn your GitHub repository page, go to Settings and then click Pages in the side menu.\nUnder the “Branch” section, use the dropdown menu to select the gh-pages branch as the source.\nClick Save.\nClick on the Actions tab at the top of your GitHub repository page.\nou will see that your recent commit has automatically started the workflow.\nOnce the action is complete, go back to Settings &gt; Pages.\nClick the “Visit site” button to see your live website at the URL you specified."
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-7",
    "href": "lab/quarto_website/slides/quarto_website.html#now-it-is-your-turn.-7",
    "title": "Home",
    "section": "Now it is your turn.",
    "text": "Now it is your turn.\n\n\n\n\n\n\nCreate the quarto website project folder\nCreate Github repo\nLink your local folder with the Github repo\nCreate the workflow\nPublish your website\n\n\n\n\n\n\n−+\n07:30\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html#resources-to-keep-learning",
    "href": "lab/quarto_website/slides/quarto_website.html#resources-to-keep-learning",
    "title": "Home",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\nhttps://quarto.org/\nhttps://github.com/quarto-dev/quarto-cli/discussions\nhttps://github.com/mcanouil/awesome-quarto"
  },
  {
    "objectID": "phd/test.html",
    "href": "phd/test.html",
    "title": "Forecasting for Health Supply Chains",
    "section": "",
    "text": "Forecasting for Health Supply Chains\n        My PhD research project is focused on improving demand forecasting for family planning to ensure contraceptives are always available when and where they are needed.\n    \n\n    \n        The Core Challenge: What Was Lost?\n        \n            In public health, \"zero demand\" doesn't mean \"zero need\"—it often just means the shelves were empty. When health facilities run out of stock, the true demand from the community goes unrecorded. This \"censored\" or \"lost\" demand data leads to flawed forecasts, which in turn causes chronic under-stocking and reinforces a cycle of supply failure. My research focuses on breaking this cycle.\n        \n    \n\n    \n        My Research Approach\n        \n            \n                1. Hybrid Forecasting\n                Develop a model combining modern machine learning with expert knowledge to create probabilistic forecasts that capture the full range of uncertainty in demand.\n                \n                     Won Best Student Presentation at ISF 2024, Dijon\n                \n            \n            →\n            \n                2. Estimate Lost Demand\n                Build the Truncated Conformal Kalman Filter (TCKF) to reconstruct true, unobserved demand from incomplete data caused by stockouts and service disruptions.\n            \n            →\n            \n                3. Optimize Inventory\n                Use improved forecasts and data-driven prediction intervals to inform smarter inventory policies that reduce stockouts while maintaining efficiency.\n            \n            \n        \n    \n\n    \n        Key Outputs & Publications\n        \n        \n            A Novel Hybrid Approach to Contraceptive Demand Forecasting\n            Published in International Journal of Production Research\n            \n                 Read Paper\n                 View Code\n            \n        \n\n        \n            Estimating censored demand in family planning supply chains: forecast accuracy, inventory implications, and public health outcomes\n            Preprint - Submitted for review\n            \n                 Read Paper\n                 View Code\n            \n        \n    \n\n    \n         Impact in Practice: EPSS Training\n        Research is most valuable when it's put into practice. As part of my PhD, I developed and delivered a two-day training workshop for practitioners at the Ethiopian Pharmaceutical Supply Service (EPSS), equipping them with modern forecasting skills using R and Python.\n        \n             Training Materials\n             Code & Labs\n        \n    \n    \n    \n        Supervision & Collaboration Team\n        \n            \n                \n                Prof. Bahman Rostami-Tabar\n                Lead Supervisor, Cardiff University\n            \n            \n                \n                Prof. Aris Syntetos\n                Co-supervisor, Cardiff University\n            \n            \n                \n                Dr. Federico Liberatore\n                Co-supervisor, Cardiff University\n            \n            \n                \n                Glenn Milano\n                Industry Collaborator, USAID\n            \n        \n    \n    \n    \n        Funding & Partners"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Home",
    "section": "",
    "text": "My Story\n  \n    \n      I’m a forecasting researcher passionate about using data science and machine learning to tackle real-world problems that matter. My journey started in Sri Lanka, where a foundation in transport and logistics sparked a lasting interest in time series forecasting and supply chain analytics.\n    \n    \n      Today, I’m a PhD candidate at the Data Lab for Social Good (Cardiff University), where I develop forecasting and decision-support solutions for global health and climate-related challenges. My work focuses on healthcare supply chains, climate-sensitive disease forecasting, and forecasting for social good, with a strong emphasis on real-world impact.\n    \n    \n      I collaborate closely with organizations such as USAID, the Ethiopian Pharmaceutical Supply Chain Agency, and the HISP Centre at the University of Oslo, translating research into operational tools that improve public health outcomes.\n    \n    \n      \n         Download CV\n      \n    \n  \n\n\n\n  PhD Research Showcase\n  \n    \n      \n      Research Focus\n      Developing novel forecasting and inventory management approaches for health commodity supply chains.\n    \n    \n      \n      Methodology\n      Integrating advanced probabilistic forecasting models with robust inventory frameworks.\n    \n    \n      \n      Global Health Impact\n      Enhancing public health outcomes and supporting data-driven decisions for tangible social good.\n    \n    \n      \n      Collaborations\n      Working with influential organizations like USAID, EPSA, and the HISP Center at the University of Oslo.\n    \n  \n\n\n\n  Funder & Collaborators\n  \n    This research is proudly funded by the Welsh Graduate School for the Social Sciences (WGSSS) and conducted in collaboration with leading global health organizations.\n  \n  \n      \n      \n      \n      \n  \n\n\n\n  Supervision Team\n  \n    \n      \n      \n        Prof. Bahman Rostami-Tabar (Lead)\n        Professor of Data-Driven Decision Science, Cardiff University\n      \n    \n    \n      \n      \n        Prof. Aris Syntetos (Co-supervisor)\n        Professor of Decision Science, Cardiff University\n      \n    \n    \n      \n      \n        Dr. Federico Liberatore (Co-supervisor)\n        Senior Lecturer in Data & Knowledge Engineering, Cardiff University\n      \n    \n    \n      \n      \n        Glenn Milano (Industry Collaborator)\n        Advisor for Demand Planning and Analytics, USAID"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#outline",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#outline",
    "title": "WHAT WAS LOST",
    "section": "Outline",
    "text": "Outline\n\n\n\n\n\n\n\n\nWhat was never COUNTED . . .\n\n\n\n\n\n\nThe fundamental question\n\n\n\n\n\n\nWhat we are going to do\n\n\n\n\n\n\nEmpirical evaluation\n\n\n\n\n\n\nWhat NEXT?\n\n\n\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#seen-the-unseen",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#seen-the-unseen",
    "title": "WHAT WAS LOST",
    "section": "Seen the UNSEEN",
    "text": "Seen the UNSEEN\n\nHuman story: What data misses\n\n\n\n\nNilu went to a pharmacy for Product A. It wasn’t in stock.\n\nThe system logs it as zero demand.\n\nBut the need was real. The system just missed it.\n\nThis creates broken trust and leads to create\nUNMET DEMAND.\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#seen-the-unseen-1",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#seen-the-unseen-1",
    "title": "WHAT WAS LOST",
    "section": "Seen the UNSEEN",
    "text": "Seen the UNSEEN\n\nAnalytical reality: Why this matters\n\n\n\n\nIn supply chains like this, Stockouts censor demand.\nObserved sales ≠ actual demand.\n\nInventory decisions based on this false signal?\nUnderstocking → more stockouts.\n\nForecasts don’t just underperform. They miss the whole story.\n\nA lost sale = a lost opportunity for care.\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#contraceptive-products-arent-easily-substituted",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#contraceptive-products-arent-easily-substituted",
    "title": "WHAT WAS LOST",
    "section": "Contraceptive products aren’t easily SUBSTITUTED",
    "text": "Contraceptive products aren’t easily SUBSTITUTED\n\n\n\n\n*Percent of women who will get pregnant within the first year of typical use."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#the-big-picture",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#the-big-picture",
    "title": "WHAT WAS LOST",
    "section": "The BIG PICTURE",
    "text": "The BIG PICTURE\n\n\n\n\n\nIn reality…\nThere are more than\n218 million women\nlike Nilu still have an unmet need for family planning.\nUltimately, this results in dropouts, unwanted pregnancies, and almost 7 million hospitalizations each year in developing countries.\n\n\n\n\nImage generated using 04-mini-high\nSources: USAID, 2020; PATH, 2019; Mukasa et al., 2017; Gilda et al., 2016"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#why-this-is-critical",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#why-this-is-critical",
    "title": "WHAT WAS LOST",
    "section": "Why this is critical",
    "text": "Why this is critical\n\n\n\n\n\n\n\nCensorship is structural — stockouts and interruptions are common in FPSC, not rare events.\n\n\n\n\nField insight — in Côte d’Ivoire and Ethiopia, demand planners repeatedly flagged stockouts as the key barrier.\n\n\n\n\nForecasting fails under censorship — observed sales understate true demand.\n\n\n\n\nThe literature split - prior research often separates forecasting from inventory decisions.\n\n\n\n\nResources are tightening — with USAID withdrawal, high service levels must be achieved efficiently.\n\n\n\n\n\n\nImage generated using 04-mini-high\nSources: Bijvank et al., 2011; Karimi et al., 2021; Thanos et al., 2022 ; Trapero et al., 2024"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#key-definitions",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#key-definitions",
    "title": "WHAT WAS LOST",
    "section": "Key definitions",
    "text": "Key definitions\n\n\nStockouts: Periods when demand is higher than available inventory, leading to censored observations of demand.\nInterruptions: Periods when no products are issued despite available stock, thus artificially recorded as zero demand.\nCensored Demand: Demand occurring during periods when products are unavailable (stockouts or interruptions), thus not fully observable.\nTrue Demand: Actual demand that would have occurred if sufficient stock was available or no service interruptions."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#bridging-forecasting-inventory-and-impact",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#bridging-forecasting-inventory-and-impact",
    "title": "WHAT WAS LOST",
    "section": "Bridging forecasting, inventory, and impact",
    "text": "Bridging forecasting, inventory, and impact\n\nHow can a demand forecasting model that explicitly handles censored demand due to stockouts and service interruptions improve inventory performance and public health outcomes in contraceptive supply chains?\n\n\n\n\n\n\n\n\nFigure 1: Censorship scenarios in family planning supply chains."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#how-we-can-fill-the-gaps",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#how-we-can-fill-the-gaps",
    "title": "WHAT WAS LOST",
    "section": "How we can fill the gaps",
    "text": "How we can fill the gaps\n\n\n\n\n\n\n\n\nRQ1: How accurately can a Tobit Kalman Filter with conformal prediction estimate true demand under censorship?\n\n\n\n\n\nRQ2: How does demand reconstruction improve inventory performance and healthcare impact compared to baseline planning methods?\n\n\n\n\nRQ3: How do planner-adjusted forecasts compare to model-based methods in balancing availability and inventory efficiency?\n\n\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#overview-of-the-experimental-framework",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#overview-of-the-experimental-framework",
    "title": "WHAT WAS LOST",
    "section": "Overview of the experimental framework",
    "text": "Overview of the experimental framework"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#truncated-conformal-kalman-filter-tckf",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#truncated-conformal-kalman-filter-tckf",
    "title": "WHAT WAS LOST",
    "section": "Truncated Conformal Kalman Filter (TCKF)",
    "text": "Truncated Conformal Kalman Filter (TCKF)\n\nState-Space Formulation\n\\[\nX_t = F X_{t-1} + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, Q_t)\n\\]\n\n\\(X_t = [\\ell_t, \\tau_t, \\gamma_t]^T\\): level, trend, seasonality\n\\(F\\): state transition matrix with seasonal decay and trend\n\n\nObservation Equation with Censorship\n\\[\ny_t =\n\\begin{cases}\nH X_t + \\nu_t & \\text{if uncensored} \\\\\n0 & \\text{if fully censored} \\\\\n\\min(H X_t, s_t) & \\text{if partially censored}\n\\end{cases}\n\\]\n\n\\(H = [1, 0, 1]\\): maps level and seasonality to observation\n\\(s_t\\): stock available at time \\(t\\)\n\n\nKalman Prediction Step\n\\[\n\\mu_t = H \\hat{X}_{t|t-1}, \\quad \\sigma_t^2 = H P_{t|t-1} H^T + R\n\\]"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#truncated-conformal-kalman-filter-tckf-1",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#truncated-conformal-kalman-filter-tckf-1",
    "title": "WHAT WAS LOST",
    "section": "Truncated Conformal Kalman Filter (TCKF)",
    "text": "Truncated Conformal Kalman Filter (TCKF)\n\nCensored Observation Update\n\\[\n\\hat{y}_t = \\mu_t + \\sigma_t \\cdot \\frac{\\phi(z_t)}{1 - \\Phi(z_t)}, \\quad z_t = \\frac{y_t - \\mu_t}{\\sigma_t}\n\\]\n\\[\n\\hat{X}_{t|t} = \\hat{X}_{t|t-1} + K_t (\\hat{y}_t - \\mu_t)\n\\]\n\nUses expectation of truncated Gaussian\nFor fully censored (\\(y_t = 0\\)), skip state update; propagate uncertainty\n\n\nConformal Prediction for Interval Estimation\n\\[\nD_t \\in [\\max(0, \\mu_t - q_\\alpha), \\mu_t + q_\\alpha]\n\\]\n\nResiduals from uncensored periods used to calibrate \\(q_\\alpha\\)\nEnsures valid coverage without assuming normality\n\n\nNote on Initialization: Initial state vector \\(X_0 = [\\ell_0, \\tau_0, \\gamma_0]^T\\) is extracted via STL decomposition from uncensored periods."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---forecast",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---forecast",
    "title": "WHAT WAS LOST",
    "section": "Performance evaluation - Forecast",
    "text": "Performance evaluation - Forecast\n\nPoint predictions\n\\[\nForecast\\ Value\\ Added = (1 -Rel\\ RMSE) \\times 100\\%\n\\] Values above 0 indicate better performance than TCKF.\n\\[\nRel\\ RMSE = \\frac{RMSE_{\\text{Method}}}{RMSE_{\\text{TCKF}}}\n\\]\n\\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{t=1}^{n} (\\hat{y}_t - y_t)^2}\n\\]\nProbabilistic predictions\nThe pinball loss at time \\(t\\) for quantile level \\(p\\) is defined as:\n\\[\nQ_{p,t} =\n\\begin{cases}\n2(1 - p)(\\hat{f}_{p,t} - y_t), & \\text{if } y_t &lt; \\hat{f}_{p,t} \\\\\n2p(y_t - \\hat{f}_{p,t}), & \\text{if } y_t \\geq \\hat{f}_{p,t}\n\\end{cases}\n\\]\n\n\\[\n\\text{Skill Score} = \\frac{\\text{Pinball Score}_{\\text{TCKF}} - \\text{Pinball Score}_{\\text{Method}}}{\\text{Pinball Score}_{\\text{TCKF}}} \\times 100\\%\n\\]\nValues above 0 indicate better performance than TCKF."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---inventory",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---inventory",
    "title": "WHAT WAS LOST",
    "section": "Performance evaluation - Inventory",
    "text": "Performance evaluation - Inventory\n\n\nCycle Service Level (CSL): proportion of periods in which stock was available\n\\[\n  CSL = \\frac{1}{N} \\sum_{t=1}^{N} \\mathbb{1}(in\\_stock_t = 1)\n\\]\nStock-Out Rate (SOR): proportion of periods experiencing a stockout\n\\[\n  SOR = \\frac{1}{N} \\sum_{t=1}^{N} \\mathbb{1}(stockout\\_ev_t = 1)\n\\]\nInventory Turnover (IT): average stock relative to the target level\n\\[\n  IT = \\frac{1}{N} \\sum_{t=1}^{N} \\frac{average\\_stock_t}{target\\_stock_t}, \\quad \\text{if } target\\_stock_t &gt; 0\n\\]"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---inventory-1",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---inventory-1",
    "title": "WHAT WAS LOST",
    "section": "Performance evaluation - Inventory",
    "text": "Performance evaluation - Inventory\n\nRelative CSL (Rel CSL):\n\\[\n  Rel\\ CSL =\n  \\begin{cases}\n  \\frac{CSL - CSL_{TCKF}}{CSL_{TCKF}}, & CSL_{TCKF} \\ne 0 \\\\\n  CSL - CSL_{TCKF}, & \\text{otherwise}\n  \\end{cases}\n\\]\nRelative SOR (Rel SOR):\n\\[\n  Rel\\ SOR =\n  \\begin{cases}\n  \\frac{SOR_{TCKF} - SOR}{SOR_{TCKF}}, & SOR_{TCKF} &gt; 0 \\\\\n  0, & SOR_{TCKF} = 0 \\land SOR = 0 \\\\\n  -SOR, & SOR_{TCKF} = 0 \\land SOR &gt; 0\n  \\end{cases}\n\\]\nRelative IT (Rel IT):\nLet \\(\\Delta = |IT_{TCKF} - 1|\\), then:\n\\[\n  Rel\\ IT =\n  \\begin{cases}\n  0, & IT = 0 \\land IT_{TCKF} = 0 \\\\\n  IT - 1, & IT_{TCKF} = 1 \\\\\n  1 - \\frac{|IT - 1|}{\\Delta}, & \\text{otherwise}\n  \\end{cases}\n\\]\n\nThe composite Inventory Value Added score aggregates these components:\n\\[\nIVA = w_{CSL} \\cdot Rel\\ CSL + w_{SOR} \\cdot Rel\\ SOR + w_{IT} \\cdot Rel\\ IT\n\\]\nValues above 0 indicate better performance than TCKF."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---healthcare-impact",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#performance-evaluation---healthcare-impact",
    "title": "WHAT WAS LOST",
    "section": "Performance evaluation - Healthcare impact",
    "text": "Performance evaluation - Healthcare impact\n\n\nStockout of one product can decrease mCPR by 6.5% points\n\\[\n\\Delta mCPR = 0.065 \\times M \\times (\\text{Stockout Rate}_{\\text{method}} - \\text{Stockout Rate}_{\\text{TCKF}})\n\\]\n\n\\[\n\\text{Users Lost} = \\Delta mCPR \\times \\text{WRA Population}\n\\]\nwhere \\(\\text{WRA Population}\\) is the number of women of reproductive age, and \\(M\\) is the number of distinct products considered.\n\n\nMaternal deaths averted: \\(\\frac{\\text{Users Lost}}{3153}\\)\nInfant deaths averted: \\(\\frac{\\text{Users Lost}}{251}\\)\nAbortions averted: \\(\\frac{\\text{Users Lost}}{6.46}\\)\nUnintended pregnancies averted: \\(\\frac{\\text{Users Lost}}{3.63}\\)\n\n\n\n\nSources: Rosen et al. (2018), Singh et al. (2009), Karim et al. (2008), Wang and Wang (2012), and Ross and Stover (2012)\nRHSC Impact Calculator: https://www.rhsupplies.org/activities-resources/tools/reducing-stockouts-impact-calculator"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#empirical-evaluation",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#empirical-evaluation",
    "title": "WHAT WAS LOST",
    "section": "Empirical evaluation",
    "text": "Empirical evaluation\n\n\nData source: Monthly LMIS records from Côte d’Ivoire (Jan 2016–Dec 2019)\nScope: 507 site–product time series covering 9 contraceptive methods (male & female condoms, emergency contraception, oral pills, injectables, implants, IUDs)\nTraining window: January 2016 – December 2018\nTest window: January 2019 – December 2019\nCross-validation: Rolling-origin evaluation—re-train each month on all prior data, forecast 1-month ahead across the test year"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#empirical-data-exploration",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#empirical-data-exploration",
    "title": "WHAT WAS LOST",
    "section": "Empirical data exploration",
    "text": "Empirical data exploration\n\n\n\n\n\n\n\n\n\n\n\n(a) Representative time series for each demand type; (b) Distribution of time series by trend and seasonality strength and; (c) Intermittency classification based on IDI and CV^2 thresholds.\n\n\n\n\n\n\nFigure 2: Timeseries characteristics of the realworld dataset used in the empirical evaluation."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#overall-forecasting-performance-across-models",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#overall-forecasting-performance-across-models",
    "title": "WHAT WAS LOST",
    "section": "Overall forecasting performance across models",
    "text": "Overall forecasting performance across models\n\n\n\n\nMethod\nMean FVA\nMean Skill Score (q80)\nMean Skill Score (q85)\nMean Skill Score (q90)\nMean Skill Score (q95)\nMean Skill Score (q97.5)\n\n\n\n\nTCKF\n0\n0\n0\n0\n0\n0\n\n\nETS\n-0.86\n-2.72\n-2.67\n-2.28\n-2.05\n-2.52\n\n\nSystem Generated\n-0.97\n-1.37\n-1.69\n-1.6\n-1.81\n-1.57\n\n\nMean\n-0.97\n-2.73\n-2.39\n-2.38\n-2.18\n-2.26\n\n\nTimeGPT\n-0.98\n-4.05\n-4.93\n-6.84\n-13.28\n-20.99\n\n\nCensored TimeGPT\n-1.01\n-3.94\n-4.71\n-6.53\n-12.79\n-20.18\n\n\nCensored Mean\n-1.02\n-2.69\n-2.38\n-2.59\n-2.4\n-2.63\n\n\nSyntetos-Boylan Approx\n-1.03\n-2.74\n-2.63\n-3.22\n-2.87\n-3.16\n\n\nARIMA\n-1.19\n-3.11\n-3.05\n-3.04\n-3.06\n-3.84\n\n\nCensored ARIMA\n-1.19\n-3.77\n-3.45\n-3.39\n-3.28\n-4.03\n\n\nLightGBM\n-1.36\n-3.39\n-2.97\n-2.93\n-2.96\n-3.14\n\n\nCensored LightGBM\n-1.36\n-3.54\n-2.96\n-2.93\n-3.17\n-3.42\n\n\nNaive\n-1.7\n-3.78\n-3.43\n-2.76\n-2.85\n-3.98\n\n\nCensored Linear Regression\n-2.3\n-3.95\n-4.53\n-5.8\n-6.47\n-7.52\n\n\nLinear Regression\n-2.37\n-3.55\n-4.18\n-5.12\n-5.57\n-6.88\n\n\nDemand Planner\n-4.63\n-13.53\n-11.38\n-9.11\n-6.45\n-5.01"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#significance-test---point-forecast",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#significance-test---point-forecast",
    "title": "WHAT WAS LOST",
    "section": "Significance test - point forecast",
    "text": "Significance test - point forecast\n\n\n\n\n\n\n\n\nFigure 3: Nemenyi post-hoc test with 95% confidence level on inverted FVA values from the empirical evaluation."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#overall-inventory-performance",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#overall-inventory-performance",
    "title": "WHAT WAS LOST",
    "section": "Overall inventory performance",
    "text": "Overall inventory performance\nExisting mean based order-up-to level\n\n\n\n\n\n\n\nMethod\nMean CSL\nMean Stockout Rate\nMean Inventory Turnover\nMean IVA\n\n\n\n\nDemand Planner\n0.991\n0.004\n3.641\n-2.931\n\n\nLightGBM\n0.966\n0.034\n1.271\n-0.066\n\n\nSystem Generated\n0.955\n0.023\n4.048\n-4.155\n\n\nCensored LightGBM\n0.95\n0.041\n1.652\n-0.851\n\n\nTCKF\n0.929\n0.055\n1.087\n0\n\n\nCensored TimeGPT\n0.927\n0.061\n1.516\n-0.459\n\n\nETS\n0.916\n0.065\n1.199\n-0.191\n\n\nTimeGPT\n0.915\n0.07\n1.714\n-0.855\n\n\nSyntetos-Boylan Approx\n0.913\n0.077\n1.003\n-0.091\n\n\nARIMA\n0.911\n0.075\n1.184\n-0.256\n\n\nMean\n0.896\n0.078\n1.507\n-0.614\n\n\nLinear Regression\n0.892\n0.083\n1.878\n-1.308\n\n\nCensored Mean\n0.891\n0.082\n1.236\n-0.314\n\n\nCensored Linear Regression\n0.883\n0.084\n1.841\n-1.267\n\n\nNaive\n0.877\n0.072\n2.423\n-1.923\n\n\nCensored ARIMA\n0.876\n0.106\n1.041\n-0.238\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n⚠️ High CSL often comes at the cost of overstocking\n📉 Low inventory does not always equal efficiency\n✅ TCKF achieves the most efficient trade-off"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#overall-inventory-performance-1",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#overall-inventory-performance-1",
    "title": "WHAT WAS LOST",
    "section": "Overall inventory performance",
    "text": "Overall inventory performance\nQuantile based order-up-to level\n\n\n\n\n\n\n\n\n\n\n\n(a) Achieved CSL; (b) stockout rate; (c) inventory turnover; (d) relative CSL vs. TCKF; (e) relative stockout rate vs. TCKF; (f) relative inventory turnover vs. TCKF\n\n\n\n\n\n\nFigure 4: Inventory performance under the quantile-based order-up-to level policy from the empirical evaluation."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#overall-inventory-performance-2",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#overall-inventory-performance-2",
    "title": "WHAT WAS LOST",
    "section": "Overall inventory performance",
    "text": "Overall inventory performance\nQuantile based order-up-to level\n\n\n\n\n\n\n\n\nFigure 5: Inventory Value Added (IVA) vs. TCKF. under the quantile-based order-up-to level policy from the empirical evaluation."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#healthcare-metrics",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#healthcare-metrics",
    "title": "WHAT WAS LOST",
    "section": "Healthcare metrics",
    "text": "Healthcare metrics\n\n\n\n\n\n\n\n\nFigure 6: mCRP loss the empirical evaluation, based on relative stockout loss compared to the TCKF."
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#illustrative-impact-why-forecast-quality-matters",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#illustrative-impact-why-forecast-quality-matters",
    "title": "WHAT WAS LOST",
    "section": "Illustrative impact: Why forecast quality matters",
    "text": "Illustrative impact: Why forecast quality matters\n\n\n\n\n\nIn Côte d’Ivoire, ~1.59 million women use modern contraceptives (Track20).\n\nReplacing TCKF with the current LMIS forecast under a q95 inventory policy would lead to:\n🔻 reduce 18,599 additional women losing access\n➕ save 5,124 unintended pregnancies\n➕ save 5,879 abortions\n➕ save 54 infant deaths\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#what-matters-linking-forecasts-inventory-health-impact",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#what-matters-linking-forecasts-inventory-health-impact",
    "title": "WHAT WAS LOST",
    "section": "What matters: linking forecasts, inventory & health impact",
    "text": "What matters: linking forecasts, inventory & health impact\n\n\n\n\n\n📌 Forecast accuracy ≠ forecast value\nMost models focus on error metrics, but ignore censorship, uncertainty, and how forecasts are used.\n\n📦 High service ≠ high performance\nDemand Planner & System Forecast look strong, but hide poor forecasts behind excess inventory.\n\n❤️ Forecast quality drives public health\nUnder lean policies (q80–q95), poor forecasts → more stockouts → more harm.\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "talks/wgsss_25/slides/wpgrc_25.html#key-contributions-implications",
    "href": "talks/wgsss_25/slides/wpgrc_25.html#key-contributions-implications",
    "title": "WHAT WAS LOST",
    "section": "Key contributions & implications",
    "text": "Key contributions & implications\n\n\n\n\n\n\n\n\nAddressing Censored Demand\n\n\nTCKF explicitly accounts for stockouts and service interruptions by reconstructing true demand.\n\n\n\n\n\n\nForecasts Aligned with Inventory\n\n\nOur study links forecasting with inventory decisions and public health outcomes.\n\n\n\n\n\n\nImproved Inventory Efficiency\n\n\nBy reducing stockouts without overstocking, TCKF enhances both service levels and inventory turnover.\n\n\n\n\n\n\nPractical Value for Planners & Donors\n\n\nTCKF enables risk-aware, evidence-based planning — particularly valuable under resource constraints, such as the phasing out of USAID support.\n\n\n\n\n\n\nReproducibility & Extendability\n\n\nThe full pipeline is openly implemented in R using both synthetic and empirical LMIS data, supporting reproducibility.\n\n\n\n\n\n\n\nImage generated using 04-mini-high"
  },
  {
    "objectID": "lab/quarto_website/test.html",
    "href": "lab/quarto_website/test.html",
    "title": "Quarto Websites: Create and Publish Your First Website",
    "section": "",
    "text": "Quarto Websites: Create and Publish Your First Website\n  A hands-on workshop for researchers, students, and practitioners.\n  Harsha Chamara Hewage | 2025-11-12\n  \n    \n       View Slides\n    \n    \n       GitHub Repo\n    \n  \n\n\n\n  Who is the Course For?\n  This course is designed for researchers, students, and practitioners, who wish to create a personal or project website using the Quarto framework.\n  It assumes familiarity with R and RStudio, but no prior experience with HTML, CSS, or GitHub Pages is required.\n\n\n\n  Learning Objectives\n  \n    \n      \n      Set up and connect Git with RStudio\n    \n    \n      \n      Build and preview a basic website using Quarto templates\n    \n    \n      \n      Customize website appearance using YAML\n    \n    \n      \n      Create multiple pages, listings, and navigation bars\n    \n    \n      \n      Publish the website using GitHub Pages and GitHub Actions\n    \n    \n      \n      Understand how websites can be used to share research and project outputs\n    \n  \n\n\n\n  Prerequisites (Before the Training)\n  \n    Install R, RStudio, RTools (for Windows users), Quarto, and Git.\n    Create a GitHub account.\n    Install required R packages: install.packages(c(\"usethis\", \"gitcreds\"))\n    Ensure you are comfortable using `.qmd` files, the R Console, the Terminal, and basic GitHub navigation.\n    Bring a photo of yourself (.jpg or .png) and prepare a short personal introduction (3–4 lines).\n  \n\n\n\n  Topics & Structure\n  \n\n\n\nSection\nTopic\nActivities\n\n\n\n\n1\nConnect RStudio with Git\nLive demo + setup + token config\n\n\n2\nBuild the Homepage\nUse quarto use template\n\n\n3\nStyling your Website\nModify _quarto.yml\n\n\n4\nPages and Navigation\nAdd `.qmd` pages and navbar\n\n\n5\nListings\nCreate blog/projects listings\n\n\n6\nPublish using GitHub Pages\nConfigure workflow + publish\n\n\n7\nWrap-up & Q&A\nShare published sites"
  },
  {
    "objectID": "lab/quarto_website/slides/quarto_website.html",
    "href": "lab/quarto_website/slides/quarto_website.html",
    "title": "Quarto Websites",
    "section": "",
    "text": "Image generated using GPT-5 Thinking Mini"
  },
  {
    "objectID": "lab/quarto_website/index.html#who-is-the-course-for",
    "href": "lab/quarto_website/index.html#who-is-the-course-for",
    "title": "Quarto Websites: Create and Publish Your First Website",
    "section": "Who is the Course For?",
    "text": "Who is the Course For?\nThis course is designed for researchers, students, and practitioners, who wish to create a personal or project website using the Quarto framework.\nIt assumes familiarity with R and RStudio, but no prior experience with HTML, CSS, or GitHub Pages is required."
  },
  {
    "objectID": "lab/quarto_website/index.html#learning-objectives",
    "href": "lab/quarto_website/index.html#learning-objectives",
    "title": "Quarto Websites: Create and Publish Your First Website",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this course, participants will be able to:\n\nSet up and connect Git with RStudio\nBuild and preview a basic website using Quarto templates\nCustomize website appearance using YAML\nCreate multiple pages, listings, and navigation bars\nPublish the website using GitHub Pages and GitHub Actions\nUnderstand how websites can be used to share research and project outputs"
  },
  {
    "objectID": "lab/quarto_website/index.html#prerequisites-before-the-training",
    "href": "lab/quarto_website/index.html#prerequisites-before-the-training",
    "title": "Quarto Websites: Create and Publish Your First Website",
    "section": "Prerequisites (Before the Training)",
    "text": "Prerequisites (Before the Training)\nPlease ensure the following are completed before attending:\n\nInstall R, RStudio, RTools (for Windows users), Quarto, and Git\nInstall required R packages:\n\n\nconsole\n\ninstall.packages(c(\"countdown\", \"usethis\", \"gitcreds\"))\n\nCreate a GitHub account\nEnsure you are comfortable using:\n\n.qmd files in RStudio\nThe R Console and Terminal tabs\nBasic navigation of GitHub repositories\n\nBring a photo of yourself (.jpg or .png) — this will be used on your homepage.\nPrepare a short personal introduction (3–4 lines) — this will be added as your website bio."
  },
  {
    "objectID": "lab/quarto_website/index.html#topics-structure",
    "href": "lab/quarto_website/index.html#topics-structure",
    "title": "Quarto Websites: Create and Publish Your First Website",
    "section": "Topics & structure",
    "text": "Topics & structure\n\n\n\n\n\n\n\n\nSection\nTopic\nActivities\n\n\n\n\n1\nConnect RStudio with Git\nLive demo + setup + token config\n\n\n2\nBuild the Homepage\nUse quarto use template\n\n\n3\nStyling your Website\nModify _quarto.yml\n\n\n4\nPages and Navigation\nAdd .qmd pages and navbar\n\n\n5\nListings\nCreate blog/projects listings\n\n\n6\nPublish using GitHub Pages\nConfigure workflow + publish\n\n\n7\nWrap-up & Q&A\nShare published sites"
  },
  {
    "objectID": "lab/quarto_website/index.html",
    "href": "lab/quarto_website/index.html",
    "title": "Quarto Websites: Create and Publish Your First Website",
    "section": "",
    "text": "Quarto Websites: Create and Publish Your First Website\n  A hands-on workshop for researchers, students, and practitioners.\n  Harsha Chamara Hewage | 2025-11-12\n  \n    \n       View Slides\n    \n    \n       GitHub Repo\n    \n  \n\n\n\n  Who is the Course For?\n  This course is designed for researchers, students, and practitioners, who wish to create a personal or project website using the Quarto framework.\n  It assumes familiarity with R and RStudio, but no prior experience with HTML, CSS, or GitHub Pages is required.\n\n\n\n  Learning Objectives\n  \n    \n      \n      Set up and connect Git with RStudio\n    \n    \n      \n      Build and preview a basic website using Quarto templates\n    \n    \n      \n      Customize website appearance using YAML\n    \n    \n      \n      Create multiple pages, listings, and navigation bars\n    \n    \n      \n      Publish the website using GitHub Pages and GitHub Actions\n    \n    \n      \n      Understand how websites can be used to share research and project outputs\n    \n  \n\n\n\n  Prerequisites (Before the Training)\n  \n    Install R, RStudio, RTools (for Windows users), Quarto, and Git.\n    Create a GitHub account.\n    Install required R packages: install.packages(c(\"usethis\", \"gitcreds\"))\n    Ensure you are comfortable using `.qmd` files, the R Console, the Terminal, and basic GitHub navigation.\n    Bring a photo of yourself (.jpg or .png) and prepare a short personal introduction (3–4 lines).\n  \n\n\n\n  Topics & Structure\n  \n\n\n\nSection\nTopic\nActivities\n\n\n\n\n1\nConnect RStudio with Git\nLive demo + setup + token config\n\n\n2\nBuild the Homepage\nUse quarto use template\n\n\n3\nStyling your Website\nModify _quarto.yml\n\n\n4\nPages and Navigation\nAdd `.qmd` pages and navbar\n\n\n5\nListings\nCreate blog/projects listings\n\n\n6\nPublish using GitHub Pages\nConfigure workflow + publish\n\n\n7\nWrap-up & Q&A\nShare published sites"
  },
  {
    "objectID": "lab/dl4sg_marcov/index.html",
    "href": "lab/dl4sg_marcov/index.html",
    "title": "Introduction to Markov Processes in Healthcare Supply Chains",
    "section": "",
    "text": "Introduction to Markov Processes in Healthcare Supply Chains\n  A hands-on workshop on stochastic modelling for logistical systems.\n  Harsha Chamara Hewage | 2025-05-15\n  \n    \n       Slides\n    \n     \n       Lab 1\n    \n     \n       Lab 2\n    \n    \n       GitHub Repo\n    \n  \n\n\n\n  Who is the Course For?\n  This course is intended for healthcare supply chain researchers, practitioners, and students who want to model uncertainty in logistical systems using Markov Chains. It assumes familiarity with basic probability and matrix manipulation in R.\n\n\n\n  Learning Objectives\n  \n    \n      \n      Understand the structure and assumptions of discrete-time Markov chains (DTMCs)\n    \n    \n      \n      Apply transition matrices to simulate system evolution over time\n    \n    \n      \n      Compute and interpret steady-state distributions\n    \n    \n      \n      Model brand switching and service reliability in healthcare supply chains\n    \n    \n      \n      Simulate and visualize long-run outcomes in R\n    \n    \n      \n      Link model insights to supply chain policy decisions\n    \n  \n\n\n\n  Prerequisites\n  \n    Comfortable with basic probability (random variables, distributions)\n    Familiarity with R and tidyverse for matrix operations and plotting\n    No prior knowledge of Markov chains is assumed\n  \n\n\n\n  Course Topics\n  \n    \n        Section 1: Foundations of Markov Chains\n        \n            State-based systems and probabilistic transitions\n            The Markov property and memoryless dynamics\n            Transition probability matrices and system trajectories\n        \n    \n    \n        Section 2: Steady-State Analysis\n        \n            n-step transitions and convergence\n            Existence and uniqueness of steady-state distributions\n            Interpreting long-run behaviour in real systems\n        \n    \n    \n        Section 3: Brand Switching Case Study\n        \n            Promote-local strategy for paracetamol brands\n            Simulate switching behaviour and market share convergence\n            Solve steady-state equations using R\n        \n    \n    \n        Section 4: Applied Simulation in R\n        \n            Vector-matrix calculations\n            Transition matrix exponentiation\n            Plotting and comparing convergence paths"
  },
  {
    "objectID": "lab/intro_python/index.html",
    "href": "lab/intro_python/index.html",
    "title": "A basic introduction to Python",
    "section": "",
    "text": "A basic introduction to Python\n  A practical, hands-on tutorial for new learners.\n  Harsha Chamara Hewage | 2025-03-03\n  \n    \n       View Tutorials\n    \n  \n\n\n\n  Who is the course for?\n  This course is primarily aimed at learners who require a practical introduction to Python. It assumes no previous experience using Python.\n\n\n\n  Learning Objectives\n  \n    Familiarize yourself with Python and an IDE (Jupyter Lab or VS Code).\n    Understand the basics of working with Python, including how to write and execute code.\n    Use three different ways to work with Python: Python Shell, Python Scripts, and Jupyter Lab.\n    Learn about Python’s basic data types and structures.\n    Install and import required libraries.\n    Find resources for help when coding in Python.\n  \n\n\n\n  Prerequisites\n  \n    No prior knowledge of Python is assumed.\n    Install Python and GitHub Desktop.\n  \n\n\n\n  Course Topics\n  \n    \n        Section 1: Python and IDE Essentials\n        \n            Introduction to Python & Installation\n            Setting up an Integrated Development Environment (IDE)\n            Overview of Jupyter Lab, VS Code, Google Colab, and Anaconda\n        \n    \n    \n        Section 2: Working with Jupyter Lab\n        \n            Understanding the basic syntax\n            Writing and executing code in Python\n            Setting up and organizing a Python project"
  },
  {
    "objectID": "lab/intro_method_skills/index.html",
    "href": "lab/intro_method_skills/index.html",
    "title": "Methodological Skills for Research",
    "section": "",
    "text": "Methodological Skills for Research\n  Lecture - University of Moratuwa Sri Lanka\n  Harsha Halgamuwe Hewage | 2025-04-11\n  \n    \n       View Slides\n    \n  \n\n\n\n  Context\n  Understanding research methodologies is essential for conducting rigorous, credible, and context-appropriate research in fields like transport, logistics, and beyond. This lecture introduces key philosophical foundations, paradigms, and methodological choices that shape how research questions are framed and investigated. It supports students in developing critical thinking and research design skills, which are foundational for academic projects and evidence-based decision-making in logistics and supply chain management.\n\n\n\n  Target Audience\n  \n    Undergraduate students studying Transport and Logistics Management\n    Anyone interested in gaining a practical understanding of research paradigms, methodologies, and methods\n    Students preparing to develop their undergraduate dissertations or research proposals\n  \n\n\n\n  Learning Outcomes\n  By the end of this lecture, students will be able to:\n  \n    Distinguish between ontology, epistemology, axiology, methodology, and methods\n    Understand and compare major research paradigms: Positivism, Interpretivism, Critical Realism, Pragmatism, and Post-Structuralism\n    Recognize how paradigms influence research design choices\n    Identify suitable methodologies and data collection methods for different research problems\n    Apply these concepts to real-world examples in transport and logistics contexts\n    Critically reflect on their own positionality and philosophical assumptions in research\n  \n\n\n\n  Prerequisites\n  \n    Curiosity about research and how knowledge is constructed\n    No prior experience with research philosophy is required\n    Basic understanding of research goals (e.g., solving real-world problems, generating new insights)"
  },
  {
    "objectID": "lab/intro_python/materials/test.html#univariate-energy-score-calculation",
    "href": "lab/intro_python/materials/test.html#univariate-energy-score-calculation",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Univariate Energy Score Calculation",
    "text": "Univariate Energy Score Calculation\nFor a single time series (one product), the Energy Score (ES) is given by:\n\\[\n    ES(F, y) = \\mathbb{E}_F \\left[ \\|X - y\\| \\right] - \\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X' \\| \\right]\n\\]\nwhere:\n\n\\(F\\) is the forecasted probability distribution.\n\\(y\\) is the actual observed value.\n\\(X\\) and \\(X'\\) are independent samples from the forecasted distribution.\n\\(\\|\\cdot\\|\\) represents the absolute distance (since we are dealing with univariate values).\nThe first term, \\(\\mathbb{E}_F \\left[ \\|X - y\\| \\right]\\) measures how far the forecasted values are from the observed demand.\nThe second term, \\(\\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X' \\| \\right]\\) accounts for the spread of the forecasted distribution.\n\nLets take an example from demand for a pharmaceutical product, suppose:\nForecasted samples for Product 1 follow a normal distribution: \\(X \\sim \\mathcal{N}(550, 60^2)\\)\nObserved demand: \\(y = 600\\)\n\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of forecasted demands, with the red dashed line indicating the actual observed demand.\nStep-by-Step Calculation\nCompute the first term:  \\[\n\\mathbb{E}_F \\left[ \\|X - y\\| \\right]\n\\]\nTake the average absolute difference between each forecasted sample and the observed demand (600):\n\\[\n\\frac{1}{m} \\sum_{i=1}^{m} |X_i - 600|\n\\]\nCompute the second term:  \\[\n\\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X'\\| \\right]\n\\]\nTake all possible pairs of forecasted samples and compute their absolute differences:\n\\[\n\\frac{1}{2m^2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} |X_i - X_j|\n\\]\nCompute the Energy Score:\nSubtract the two terms to get the final Energy Score.\nWe can compute the ES using scoringRules package in R (read more).\n\nlibrary(scoringRules)\n\nset.seed(123)\nforecast_samples_product1 &lt;- rnorm(1000, mean = 550, sd = 60)  # Forecasted demand\nactual_demand_product1 &lt;- 600  # Observed demand\n\n# Compute the Energy Score for Product 1\nes_univariate &lt;- es_sample(y = actual_demand_product1, dat = matrix(forecast_samples_product1, nrow = 1))\nprint(paste(\"Univariate Energy Score:\", round(es_univariate, 4)))\n\n[1] \"Univariate Energy Score: 29.457\""
  },
  {
    "objectID": "lab/intro_python/materials/test.html#multivariate-energy-score-calculation",
    "href": "lab/intro_python/materials/test.html#multivariate-energy-score-calculation",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Multivariate Energy Score Calculation",
    "text": "Multivariate Energy Score Calculation\nFor multiple time series (joint demand forecasting), the multivariate Energy Score is:\n\\[\nES(F, y) = \\mathbb{E}_F \\left[ \\|X - y\\| \\right] - \\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X'\\| \\right]\n\\]\nwhere:\n\n( X ) and ( X’ ) are now vectors representing forecasted values for both Product 1 and Product 2.\n( y ) is now a vector containing the actual observed values for both products.\n( || ) is the Euclidean distance (not just absolute value) because we now have multiple dimensions.\n\nSuppose we have two products now;\nForecasted distributions: \\[\n\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 550 \\\\ 320 \\end{bmatrix}, \\begin{bmatrix} 3600 & 1500 \\\\ 1500 & 2500 \\end{bmatrix} \\right)\n\\]\nObserved demand vector: \\[\ny = \\begin{bmatrix} 600 \\\\ 280 \\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nStep-by-Step Calculation\nCompute the first term:\n\\[\n\\mathbb{E}_F \\left[ \\|X - y\\| \\right]\n\\]\nCompute the Euclidean distance between each forecast sample and the actual observed demand vector:\n\\[\n\\frac{1}{m} \\sum_{i=1}^{m} \\sqrt{(X_{i,1} - y_1)^2 + (X_{i,2} - y_2)^2}\n\\]\nCompute the second term:\n\\[\n\\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X'\\| \\right]\n\\]\nCompute pairwise Euclidean distances between all forecasted sample pairs:\n\\[\n\\frac{1}{2m^2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\sqrt{(X_{i,1} - X_{j,1})^2 + (X_{i,2} - X_{j,2})^2}\n\\]\nCompute the Multivariate Energy Score:\nSubtract the two terms to get the final Multivariate Energy Score.\n\nlibrary(MASS)\n\n# Define number of dimensions and samples\nd &lt;- 2  # Number of products\nm &lt;- 1000  # Number of forecast samples\n\n# Define mean vector and covariance matrix\nmu &lt;- c(550, 320)  \nSigma &lt;- matrix(c(3600, 1500, \n                  1500, 2500), nrow = d, ncol = d)\n\n# Generate forecast samples\ndemand_forecast_multi &lt;- t(mvrnorm(m, mu = mu, Sigma = Sigma))  \n\n# Define observed demand vector\nactual_demand_multi &lt;- c(600, 280)\n\n# Compute the Multivariate Energy Score\nes_multivariate &lt;- es_sample(y = actual_demand_multi, dat = demand_forecast_multi)\n\n# Print result\nprint(paste(\"Multivariate Energy Score:\", round(es_multivariate, 4)))\n\n[1] \"Multivariate Energy Score: 44.319\""
  },
  {
    "objectID": "lab/energy_score/index.html#univariate-energy-score-calculation",
    "href": "lab/energy_score/index.html#univariate-energy-score-calculation",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Univariate Energy Score Calculation",
    "text": "Univariate Energy Score Calculation\nFor a single time series (one product), the Energy Score (ES) is given by:\n\\[\n    ES(F, y) = \\mathbb{E}_F \\left[ \\|X - y\\| \\right] - \\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X' \\| \\right]\n\\]\nwhere:\n\n\\(F\\) is the forecasted probability distribution.\n\\(y\\) is the actual observed value.\n\\(X\\) and \\(X'\\) are independent samples from the forecasted distribution.\n\\(\\|\\cdot\\|\\) represents the absolute distance (since we are dealing with univariate values).\nThe first term, \\(\\mathbb{E}_F \\left[ \\|X - y\\| \\right]\\) measures how far the forecasted values are from the observed demand.\nThe second term, \\(\\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X' \\| \\right]\\) accounts for the spread of the forecasted distribution.\n\nLets take an example from demand for a pharmaceutical product, suppose:\nForecasted samples for Product 1 follow a normal distribution: \\(X \\sim \\mathcal{N}(550, 60^2)\\)\nObserved demand: \\(y = 600\\)\n\n\n\n\n\n\n\n\n\nThe histogram above shows the distribution of forecasted demands, with the red dashed line indicating the actual observed demand.\nStep-by-Step Calculation\nCompute the first term:  \\[\n\\mathbb{E}_F \\left[ \\|X - y\\| \\right]\n\\]\nTake the average absolute difference between each forecasted sample and the observed demand (600):\n\\[\n\\frac{1}{m} \\sum_{i=1}^{m} |X_i - 600|\n\\]\nCompute the second term:  \\[\n\\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X'\\| \\right]\n\\]\nTake all possible pairs of forecasted samples and compute their absolute differences:\n\\[\n\\frac{1}{2m^2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} |X_i - X_j|\n\\]\nCompute the Energy Score:\nSubtract the two terms to get the final Energy Score.\nWe can compute the ES using scoringRules package in R (read more).\n\nlibrary(scoringRules)\n\nset.seed(123)\nforecast_samples_product1 &lt;- rnorm(1000, mean = 550, sd = 60)  # Forecasted demand\nactual_demand_product1 &lt;- 600  # Observed demand\n\n# Compute the Energy Score for Product 1\nes_univariate &lt;- es_sample(y = actual_demand_product1, dat = matrix(forecast_samples_product1, nrow = 1))\nprint(paste(\"Univariate Energy Score:\", round(es_univariate, 4)))\n\n[1] \"Univariate Energy Score: 29.457\""
  },
  {
    "objectID": "lab/energy_score/index.html#multivariate-energy-score-calculation",
    "href": "lab/energy_score/index.html#multivariate-energy-score-calculation",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Multivariate Energy Score Calculation",
    "text": "Multivariate Energy Score Calculation\nFor multiple time series (joint demand forecasting), the multivariate Energy Score is:\n\\[\nES(F, y) = \\mathbb{E}_F \\left[ \\|X - y\\| \\right] - \\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X'\\| \\right]\n\\]\nwhere:\n\n( X ) and ( X’ ) are now vectors representing forecasted values for both Product 1 and Product 2.\n( y ) is now a vector containing the actual observed values for both products.\n( || ) is the Euclidean distance (not just absolute value) because we now have multiple dimensions.\n\nSuppose we have two products now;\nForecasted distributions: \\[\n\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 550 \\\\ 320 \\end{bmatrix}, \\begin{bmatrix} 3600 & 1500 \\\\ 1500 & 2500 \\end{bmatrix} \\right)\n\\]\nObserved demand vector: \\[\ny = \\begin{bmatrix} 600 \\\\ 280 \\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\nStep-by-Step Calculation\nCompute the first term:\n\\[\n\\mathbb{E}_F \\left[ \\|X - y\\| \\right]\n\\]\nCompute the Euclidean distance between each forecast sample and the actual observed demand vector:\n\\[\n\\frac{1}{m} \\sum_{i=1}^{m} \\sqrt{(X_{i,1} - y_1)^2 + (X_{i,2} - y_2)^2}\n\\]\nCompute the second term:\n\\[\n\\frac{1}{2} \\mathbb{E}_F \\left[ \\|X - X'\\| \\right]\n\\]\nCompute pairwise Euclidean distances between all forecasted sample pairs:\n\\[\n\\frac{1}{2m^2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\sqrt{(X_{i,1} - X_{j,1})^2 + (X_{i,2} - X_{j,2})^2}\n\\]\nCompute the Multivariate Energy Score:\nSubtract the two terms to get the final Multivariate Energy Score.\n\nlibrary(MASS)\n\n# Define number of dimensions and samples\nd &lt;- 2  # Number of products\nm &lt;- 1000  # Number of forecast samples\n\n# Define mean vector and covariance matrix\nmu &lt;- c(550, 320)  \nSigma &lt;- matrix(c(3600, 1500, \n                  1500, 2500), nrow = d, ncol = d)\n\n# Generate forecast samples\ndemand_forecast_multi &lt;- t(mvrnorm(m, mu = mu, Sigma = Sigma))  \n\n# Define observed demand vector\nactual_demand_multi &lt;- c(600, 280)\n\n# Compute the Multivariate Energy Score\nes_multivariate &lt;- es_sample(y = actual_demand_multi, dat = demand_forecast_multi)\n\n# Print result\nprint(paste(\"Multivariate Energy Score:\", round(es_multivariate, 4)))\n\n[1] \"Multivariate Energy Score: 44.319\""
  },
  {
    "objectID": "lab/energy_score/index.html#interpretation",
    "href": "lab/energy_score/index.html#interpretation",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Interpretation",
    "text": "Interpretation\nA lower Energy Score indicates a better probabilistic forecast. Comparing different forecasting models using the Energy Score helps in selecting the model that best represents the underlying demand uncertainty."
  },
  {
    "objectID": "lab/energy_score/index.html#univariate-energy-score",
    "href": "lab/energy_score/index.html#univariate-energy-score",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Univariate Energy Score:",
    "text": "Univariate Energy Score:\n\nIf you compute es_sample(y, dat) for a single time series (one product), it returns one scalar value representing the accuracy of that probabilistic forecast compared to the observed value.\nExample: If forecasting demand for Product 1 only, it outputs one Energy Score."
  },
  {
    "objectID": "lab/energy_score/index.html#multivariate-energy-score",
    "href": "lab/energy_score/index.html#multivariate-energy-score",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Multivariate Energy Score:",
    "text": "Multivariate Energy Score:\n\nWhen forecasting multiple time series jointly (e.g., multiple products), es_sample(y, dat) computes one single Energy Score that evaluates the overall accuracy of the joint probabilistic forecast.\nThe forecast considers both marginal distributions (each product’s demand) and their dependency structure (correlation between products)."
  },
  {
    "objectID": "lab/energy_score/index.html#key-difference",
    "href": "lab/energy_score/index.html#key-difference",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Key Difference:",
    "text": "Key Difference:\n\n\n\n\n\n\n\n\nType\nForecasted Elements\nEnergy Score Output\n\n\n\n\nUnivariate\nOne product (single time series)\nOne Energy Score\n\n\nMultivariate\nMultiple products (joint time series)\nOne Energy Score for all series"
  },
  {
    "objectID": "lab/energy_score/index.html#why-one-value-for-multivariate",
    "href": "lab/energy_score/index.html#why-one-value-for-multivariate",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Why One Value for Multivariate?",
    "text": "Why One Value for Multivariate?\n\nThe Energy Score is a proper scoring rule that measures the accuracy of the entire probabilistic distribution of multiple time series.\nIt captures how well the forecasted joint distribution matches the observed values across all series."
  },
  {
    "objectID": "lab/energy_score/index.html#why-euclidean-distance",
    "href": "lab/energy_score/index.html#why-euclidean-distance",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Why Euclidean Distance?",
    "text": "Why Euclidean Distance?\n\nIn the univariate case, we use absolute difference \\(|X - y|\\) since we are dealing with a single variable.\nIn the multivariate case, we use Euclidean distance \\(\\|X - y\\|\\) because we now have multiple variables (e.g., multiple products with correlated forecasts).\n\nWhen evaluating multiple products together, we must assess how far off the entire forecast vector is from the actual vector. Euclidean distance considers both the magnitude of the forecast errors and their combined effect across multiple dimensions.\n\nCaptures Correlated Forecast Errors.\nMeasures Overall Forecast Accuracy Across Multiple Products.\n\nIf Product 1 and Product 2 have correlated demand patterns, Euclidean distance captures how well the joint forecast aligns with the observed reality. A simple sum of absolute differences (like in the univariate case) would ignore these relationships."
  },
  {
    "objectID": "lab/energy_score/index.html#why-do-we-have-frac12-in-the-second-term-of-the-energy-score-formula",
    "href": "lab/energy_score/index.html#why-do-we-have-frac12-in-the-second-term-of-the-energy-score-formula",
    "title": "Evaluating Probabilistic Forecasts with Energy Score",
    "section": "Why Do We Have \\(\\frac{1}{2}\\) in the Second Term of the Energy Score Formula?",
    "text": "Why Do We Have \\(\\frac{1}{2}\\) in the Second Term of the Energy Score Formula?\nDouble Counting in Pairwise Distances\nThe second term involves taking all pairwise distances between forecasted samples:\n\\[\n\\mathbb{E}_F \\left[ \\|X - X'\\| \\right] = \\frac{1}{m^2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\|X_i - X_j\\|\n\\]\nwhere:\n\n\\(m\\) is the number of forecast samples.\nEach pair \\((X_i, X_j)\\) is counted twice because we compute distances between all pairs \\(X_i\\) and \\(X_j\\).\nThus, to avoid over-weighting this term, we include \\(\\frac{1}{2}\\) to correct for double counting.\n\nMaintaining Proper Scale of the Score\n\nThe Energy Score is designed to measure how concentrated the forecast distribution is around the actual observed value.\nWithout the \\(\\frac{1}{2}\\), the second term would be too large, making the score too negative, distorting the interpretation."
  },
  {
    "objectID": "talks/wgsss_25/index.html#context",
    "href": "talks/wgsss_25/index.html#context",
    "title": "WHAT WAS LOST: Estimating censored demand in family planning supply chains with the Truncated Conformal Kalman Filter",
    "section": "Context",
    "text": "Context\nAt the 5th Welsh Postgraduate Research Cluster Workshop in Swansea University, we present a new lens on forecasting for public health supply chains, where demand signals are often distorted by stockouts and service interruptions. When shelves are empty, demand doesn’t disappear, it goes unrecorded.\nWe propose a Truncated Conformal Kalman Filter (TCKF) that reconstructs censored demand while generating uncertainty-aware forecasts, directly usable in inventory planning. Going beyond accuracy metrics, we evaluate how forecasts translate into inventory efficiency and ultimately public health outcomes.\nUsing synthetic and real-world data from Côte d’Ivoire, we show how better forecasting doesn’t just improve service levels, it prevents stockouts, reduces unmet need, and saves lives. Our work offers a reproducible framework that links forecasting with reorder decisions and health impact—because what gets forecasted, gets funded and delivered."
  },
  {
    "objectID": "talks/dl4sg_seminar/index.html",
    "href": "talks/dl4sg_seminar/index.html",
    "title": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains",
    "section": "",
    "text": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains\n  Data Lab for Social Good Seminar\n  Harsha Halgamuwe Hewage | 2025-05-20\n  \n    \n       View Slides\n    \n  \n\n\n\n  Context\n  In this presentation for the Data Lab for Social Good seminar, we explore how traditional forecasting methods often fail to capture unmet demand in contraceptive supply chains—particularly when stockouts censor the data. Our work combines tobit kalman filter, conformal prediction, and inventory-aware simulation to estimate true demand beyond what the system records.\n  We show how \"no demand\" often just means \"no stock,\" and how our approach can support smarter, more equitable inventory decisions. Our next steps include expanding the policy framework, incorporating external covariates, and testing the model in both lab and field settings with real demand planners.\n  Because unmet demand isn’t invisible—our systems just fail to see it."
  },
  {
    "objectID": "talks/qff_london/index.html",
    "href": "talks/qff_london/index.html",
    "title": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains",
    "section": "",
    "text": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains\n  IIF UK Chapter: Quarterly Forecasting Forum\n  Harsha Halgamuwe Hewage | 2025-05-23\n  \n    \n       View Slides\n    \n  \n\n\n\n  Context\n  In this presentation for the IIF UK Chapter: Quarterly Forecasting Forum seminar, we explore how traditional forecasting methods often fail to capture unmet demand in contraceptive supply chains—particularly when stockouts censor the data. Our work combines tobit kalman filter, conformal prediction, and inventory-aware simulation to estimate true demand beyond what the system records.\n  We show how \"no demand\" often just means \"no stock,\" and how our approach can support smarter, more equitable inventory decisions. Our next steps include expanding the policy framework, incorporating external covariates, and testing the model in both lab and field settings with real demand planners.\n  Because unmet demand isn’t invisible—our systems just fail to see it."
  },
  {
    "objectID": "talks/euro_leeds/index.html",
    "href": "talks/euro_leeds/index.html",
    "title": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains",
    "section": "",
    "text": "WHAT WAS LOST: tracing unmet demand in contraceptive supply chains\n  EURO 2025 Leeds, UK\n  Harsha Halgamuwe Hewage | 2025-06-24\n  \n    \n       View Slides\n    \n  \n\n\n\n  Context\n  At the EURO 2025 conference in Leeds, we present our work on reconstructing true demand in family planning supply chains—where stockouts routinely censor observed data and distort decision-making. Standard forecasting tools treat these absences as lack of demand, leading to understocking and reinforcing supply failure.\n  We introduce a novel approach that integrates a Truncated Conformal Kalman Filter (TCKF) with simulation-based inventory evaluation. By correcting for both partial and full censorship and layering conformal prediction for distribution-free uncertainty, we recover latent demand more accurately and translate it into better ordering policies.\n  Through synthetic experiments and real data application, we show how ignoring censored demand underestimates both need and risk. Our results point toward a scalable framework for inventory management in fragile public health systems—where demand isn’t lost, just buried under broken assumptions.\n  Because “zero demand” doesn’t mean “zero need”—it often just means the shelves were empty."
  },
  {
    "objectID": "talks/wgsss_25/index.html",
    "href": "talks/wgsss_25/index.html",
    "title": "WHAT WAS LOST: Estimating censored demand in family planning supply chains with the Truncated Conformal Kalman Filter",
    "section": "",
    "text": "WHAT WAS LOST: Estimating censored demand in family planning supply chains with the Truncated Conformal Kalman Filter\n  5th Welsh Postgraduate Research Cluster Workshop in Economy, Enterprise and Productivity\n  Harsha Halgamuwe Hewage | 2025-09-16\n  \n    \n       View Slides\n    \n  \n\n\n\n  Context\n  At the 5th Welsh Postgraduate Research Cluster Workshop in Swansea University, we present a new lens on forecasting for public health supply chains, where demand signals are often distorted by stockouts and service interruptions. When shelves are empty, demand doesn’t disappear, it goes unrecorded.\n  We propose a Truncated Conformal Kalman Filter (TCKF) that reconstructs censored demand while generating uncertainty-aware forecasts, directly usable in inventory planning. Going beyond accuracy metrics, we evaluate how forecasts translate into inventory efficiency and ultimately public health outcomes.\n  Using synthetic and real-world data from Côte d’Ivoire, we show how better forecasting doesn’t just improve service levels, it prevents stockouts, reduces unmet need, and saves lives. Our work offers a reproducible framework that links forecasting with reorder decisions and health impact—because what gets forecasted, gets funded and delivered."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "This page displays a live feed of my publications directly from my Google Scholar profile."
  },
  {
    "objectID": "talks/dhis2_webinar/index.html",
    "href": "talks/dhis2_webinar/index.html",
    "title": "From Cases to Consumption: Evaluating CHAP/DHIS2 Model Portability for Health Supply Chains",
    "section": "",
    "text": "From Cases to Consumption: Evaluating CHAP/DHIS2 Model Portability for Health Supply Chains\n  Webinar: DHIS2 Climate and Health Tools for Planning and Monitoring Immunization Programmes.\n  Harsha Halgamuwe Hewage | 2025-10-29\n  \n    \n       View Slides\n    \n  \n\n\n\n  Context\n  At the Webinar: DHIS2 Climate and Health Tools for Planning and Monitoring Immunization Programmes, we address a critical gap between disease forecasting and supply chain logistics.\n  Platforms like DHIS2 and CHAP are increasingly powerful at forecasting disease cases, often leveraging climate data to predict outbreaks. However, forecasting the product consumption needed to respond (e.g., vaccines, antimalarials) remains a persistent challenge. Consumption data is notoriously incomplete, inconsistent, and distorted by stockouts, making it difficult to plan effectively.\n  This presentation investigates model portability: can the robust, high-performing models built for cases be effectively repurposed to forecast consumption? We evaluate when to reuse, when to fine-tune (e.g., with climate or campaign data as external regressors), and when to retrain models from scratch.\n  Our work provides a practical framework that links statistical forecasts to inventory decisions. By understanding which model to trust, health systems can move from reactive ordering to predictive logistics, ensuring life-saving commodities are available to meet climate-driven health needs."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html",
    "title": "",
    "section": "",
    "text": "Data Lab for Social Good, Cardiff University in collaboration with HISP Centre, University of Oslo\n\n\n2025/10/29\n\n\n\n\n\n\n\n\n\n\n\n\nOutline"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-1",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-1",
    "title": "",
    "section": "",
    "text": "Outline"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-2",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-2",
    "title": "",
    "section": "",
    "text": "The Problem: A tale of two data streams\n\nHealth supply chains are struggling with forecasting at sub-national/ facility level.\n\nOperational realities: Incomplete records, irregular orders, frequent manual adjustments.\nThis masks true demand and leads to a cycle of…\nPersistent, critical stockouts 📉.\n\n\nBut… we have a success story.\nPlatforms like DHIS2 and CHAP have robust, high-performing models for forecasting disease cases (morbidity)."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#the-fundemental-question",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#the-fundemental-question",
    "title": "",
    "section": "The fundemental question",
    "text": "The fundemental question\n\n✅ We’re good at forecasting cases (e.g., malaria).\n\n❌ We’re struggling with forecasting consumption (e.g., antimalarials).\n \n\nCan we leverage the CHAP/DHIS2 morbidity models to forecast product consumption in supply chains?"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#the-challenge-why-isnt-this-easy",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#the-challenge-why-isnt-this-easy",
    "title": "",
    "section": "The challenge: Why isn’t this easy?",
    "text": "The challenge: Why isn’t this easy?\n\nCase data and consumption data are different.\n\n\n\n\n\n Different Data Structures\nConsumption data is “messier”.\n\nMissing entries (e.g., “0” = no consumption, or “0” = data not entered?)\nIntermittency (infrequent demand).\nInconsistent recording.\n\n\n\n\n Different Metadata\nConsumption is affected by logistics.\n\nLead times.\nProcurement cycles.\nExisting stock levels and stockouts."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-3",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-3",
    "title": "",
    "section": "",
    "text": "The Problem: A tale of two data streams\n\nHealth supply chains are struggling with forecasting at sub-national/ facility level.\n\nOperational realities: Incomplete records, irregular orders, frequent manual adjustments.\nThis masks true demand and leads to a cycle of…\nPersistent, critical stockouts 📉.\n\n\n\nBut… we have a success story.\nPlatforms like DHIS2 and CHAP have robust, high-performing models for forecasting disease cases (morbidity)."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#what-we-are-going-to-do",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#what-we-are-going-to-do",
    "title": "",
    "section": "What we are going to do",
    "text": "What we are going to do\n\n\n\n\n\n\n\nWe’ve organized our work into three interconnected Work Packages (WPs).\n\nWP1: Portability (Test reuse/transfer. Create “if/then” rules.)\n\\(\\downarrow\\)\nWP2: Forecast \\(\\rightarrow\\) Inventory (Translate forecasts into real-world inventory policy.)\n\\(\\downarrow\\)\nWP3: Hierarchical Coherence (Test aggregation for national procurement.)"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#our-current-plan",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#our-current-plan",
    "title": "",
    "section": "Our current plan",
    "text": "Our current plan\nOur immediate focus is on WP1 (Portability) and linking to WP2 (Inventory Simulation)."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-4",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-4",
    "title": "",
    "section": "",
    "text": "Key takeaways\n\n The Idea: Reusing morbidity models for supply chains is a promising but non-trivial opportunity.\n\n Our Contribution: We’re building an evidence-based, practical guide to show when and how to do this.\n\n The Impact: This work directly links forecasting models to operational decisions, aiming to reduce stockouts and improve procurement."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-1",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-1",
    "title": "Home",
    "section": "",
    "text": "Three-month Overseas Institutional Visit (OIV) at the HISP Centre, University of Oslo, Norway\n\n\n\nCollaborated with:\n– DHIS2 core development team\n– Climate and health modelling team (CHAP)\n\n\n\n\nFocus of the collaboration:\n– Understanding how disease forecasts are produced, validated, and deployed in DHIS2\n– Model reuse across domains is attractive, but largely untested"
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-3",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-3",
    "title": "Home",
    "section": "",
    "text": "The Problem: A tale of two data streams\n\nHealth supply chains are struggling with forecasting at sub-national/ facility level.\n\nOperational realities: Incomplete records, irregular orders, frequent manual adjustments.\nThis masks true demand and leads to a cycle of persistent, critical stockouts.\n\n\n\nBut… we have a success story.\n\nPlatforms like DHIS2 and CHAP have robust, high-performing models for forecasting disease cases (morbidity)."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-4",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-4",
    "title": "Home",
    "section": "",
    "text": "The Problem: A tale of two data streams\n\nHealth supply chains are struggling with forecasting at sub-national/ facility level.\n\nOperational realities: Incomplete records, irregular orders, frequent manual adjustments.\nThis masks true demand and leads to a cycle of persistent, critical stockouts.\n\n\n\nBut… we have a success story.\n\nPlatforms like DHIS2 and CHAP have robust, high-performing models for forecasting disease cases (morbidity)."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#the-fundemental-question",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#the-fundemental-question",
    "title": "Home",
    "section": "The fundemental question",
    "text": "The fundemental question\n\n✅ We’re good at forecasting cases (e.g., malaria).\n\n❌ We’re struggling with forecasting consumption (e.g., antimalarials) at sub-national level.\n\n\nCan the same CHAP morbidity models and pipelines be ported to health commodity demand and inventory decisions?\n\n\n\n\nPortability lens\n\nSame platform (DHIS2/CHAP)\nSame model pipelines\nNew target (consumption)"
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#the-challenge-why-isnt-this-easy",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#the-challenge-why-isnt-this-easy",
    "title": "Home",
    "section": "The challenge: Why isn’t this easy?",
    "text": "The challenge: Why isn’t this easy?\n\nCase data and consumption data are different.\n\n\n\n\n\n Different Data Structures\nConsumption data is “messier”.\n\nMissing entries (e.g., “0” = no consumption, or “0” = data not entered?)\nIntermittency (infrequent demand).\nInconsistent recording.\n\n\n\n\n Different Metadata\nConsumption is affected by logistics.\n\nLead times.\nProcurement cycles.\nExisting stock levels and stockouts."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-6",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-6",
    "title": "Home",
    "section": "",
    "text": "What we are going to do\n\n\n Apply the same CHAP morbidity models (no extra tuning) to product demand (malaria commodities, vaccines).\n\n\n\n Link forecasts to operations: evaluate both forecast accuracy and inventory impact.\n\n\n\n Build a portability matrix: when can we transfer a model?\n\n\n\n Design an implementation workflow for DHIS2/CHAP.\n\n\n\n Provide R scripts, CHAP YAML configs and sample datasets for full reproducibility."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#our-experimental-workflow",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#our-experimental-workflow",
    "title": "Home",
    "section": "Our experimental workflow",
    "text": "Our experimental workflow"
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#overview-of-the-candidate-models",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#overview-of-the-candidate-models",
    "title": "Home",
    "section": "Overview of the candidate models",
    "text": "Overview of the candidate models\n\n\n\n\n\nMethod\nCovariates\n\n\n\n\nAuto EWARS\nPopulation, rainfall, teamperature\n\n\nINLA Baseline\nNone\n\n\nNaïve\nNone\n\n\nsNaive\nNone\n\n\nMean\nNone\n\n\nETS\nNone\n\n\nARIMA\nNone\n\n\nARIMA Climate\nRainfall, temperature\n\n\nARIMA Madagaskar\nRainfall (Lag 3), temperature (Lag 3)\n\n\nLinear Regression\nTrend, seasonality\n\n\nLinear Regression Climate\nTrend, seasonality, rainfall, teamperature\n\n\nLightGBM\nPercentage of zero values, lags of target variable (lag 6 - 12), rainfall, temperature, month, healthcare facility code, product code\n\n\nXGBoost\nPercentage of zero values, lags of target variable (lag 6 - 12), rainfall, temperature, month, healthcare facility code, product code\n\n\nRandom Forest\nPercentage of zero values, lags of target variable (lag 6 - 12), rainfall, temperature, month, healthcare facility code, product code"
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#data-exploration",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#data-exploration",
    "title": "Home",
    "section": "Data exploration",
    "text": "Data exploration\n\n\nFigure 1: Classification of all morbidity and consumption series using the ln(CV²) and ln(IDI) map. Each point represents a monthly series plotted on the logarithmic CV²–IDI scale."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-model-rankings",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-model-rankings",
    "title": "Home",
    "section": "Average forecast model rankings",
    "text": "Average forecast model rankings\n\n\nFigure 2: Average point forecast (MASE) ranks. Methods on the y-axis are ordered from best (top) to worst (bottom) based on their overall average rank across all domains. Lower ranks indicate better performance."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#point-forecast-accuracy-distribution",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#point-forecast-accuracy-distribution",
    "title": "Home",
    "section": "Point forecast accuracy distribution",
    "text": "Point forecast accuracy distribution\n\n\nFigure 4: Distribution of MASE across morbidity and supply chain datasets. Methods are sorted by median error, with the best-performing methods positioned at the bottom of each facet."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#probabilistic-forecast-accuracy-distribution",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#probabilistic-forecast-accuracy-distribution",
    "title": "Home",
    "section": "Probabilistic forecast accuracy distribution",
    "text": "Probabilistic forecast accuracy distribution\n\n\nFigure 5: Distribution of quantile loss (q90) across morbidity and supply chain datasets. Methods are sorted by median error, with the best-performing methods positioned at the bottom of each facet. The x-axis employs an inverse hyperbolic sine (\\(asinh\\)) transformation to visualize extreme outliers."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#overall-inventory-performance",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#overall-inventory-performance",
    "title": "Home",
    "section": "Overall inventory performance",
    "text": "Overall inventory performance\n\n\nFigure 4: Inventory performance across malaria-product and vaccine-product simulations using quantile-based order-up-to policies (q80–q97.5)."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#execution-scalability-of-all-forecasting-methods",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#execution-scalability-of-all-forecasting-methods",
    "title": "Home",
    "section": "Execution scalability of all forecasting methods",
    "text": "Execution scalability of all forecasting methods\n\n\nFigure 6: Model portability metrics for all candidate forecasting methods, relative to the Mean method. Relative MASE and QL(0.9) are reported separately for malaria and vaccine products; values below 1 indicate improvement over the Mean method."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#proposed-end-to-end-forecasting-to-inventory-architecture",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#proposed-end-to-end-forecasting-to-inventory-architecture",
    "title": "Home",
    "section": "Proposed end-to-end forecasting-to-inventory architecture",
    "text": "Proposed end-to-end forecasting-to-inventory architecture\n \n\n\nSolid arrows represent data flow, and dashed arrows represent information feedback for learning."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-9",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-9",
    "title": "Home",
    "section": "",
    "text": "Way forward\n\n\n Build a harmonised data foundation for supply chain forecasting.\n\n\n\n Conduct a controlled pilot using Auto ARIMA / Auto ETS.\n\n\n\n Implement quantile-based, uncertainty-driven inventory policies.\n\n\n\n Evaluate the pilot instance against real operational outcomes.\n\n\n\n Introduce preference learning and automated model selection.\n\n\n\n Automate contextual knowledge capture in the logistics stream."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#any-questions-or-thoughts",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#any-questions-or-thoughts",
    "title": "Home",
    "section": "Any questions or thoughts? 💬",
    "text": "Any questions or thoughts? 💬\n\n\n\n\n\n\n\n\n\nFrom Cases to Consumption: Evaluating the portability of climate informed forecasting methods for public health supply chains"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-5",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-5",
    "title": "",
    "section": "",
    "text": "Our research questions\n\nTo solve this, we are investigating three key questions:\n\n\n Portability: When can we transfer a model? (Morbidity \\(\\rightarrow\\) Consumption? One product \\(\\rightarrow\\) Another?)\n\n\n\n Decision Translation: How do we turn a statistical forecast into a better inventory order? (Forecast \\(\\rightarrow\\) Action)\n\n\n\n Hierarchical Coherence: Do site-level forecasts reliably add up for district and national planning?"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#data-exploration",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#data-exploration",
    "title": "",
    "section": "Data exploration",
    "text": "Data exploration\nWe used number of dengue cases in Laos to test the forecasting models.\n\n\n\n\n\n\n\n\nFigure 1: Monthly dengue cases by location."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#ovreall-forecast-performance",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#ovreall-forecast-performance",
    "title": "",
    "section": "Ovreall forecast performance",
    "text": "Ovreall forecast performance\nBest values in each column are highlighted in bold.\n\n\n\n\nModel\nMASE\nQuantile Loss (q10)\nQuantile Loss (q50)\nQuantile Loss (q90)\n\n\n\n\nRandom Forest\n0.864\n107.360\n383.058\n607.857\n\n\nARIMA Madagaskar\n0.902\n165.565\n360.123\n310.765\n\n\nAuto ARIMA Reg\n0.913\n173.929\n367.735\n322.289\n\n\nAuto ARIMA\n0.950\n176.069\n376.163\n309.962\n\n\nLinear Regression\n1.008\n129.647\n416.129\n278.839\n\n\nLightGBM\n1.009\n96.771\n452.020\n752.016\n\n\nLinear Regression Reg\n1.035\n127.927\n391.573\n284.118\n\n\nAuto EWARS\n1.041\n125.371\n421.408\n296.665\n\n\nxgBoost\n1.057\n103.980\n485.837\n817.878\n\n\nINLA baseline\n1.082\n126.139\n432.163\n239.886\n\n\nMean\n1.090\n138.155\n461.150\n347.705\n\n\nETS\n1.099\n124.722\n428.761\n299.443\n\n\nsNaive\n1.217\n138.155\n510.311\n276.443\n\n\nNaïve\n1.221\n143.645\n506.282\n319.259"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#ovreall-point-forecast-performance-across-forecast-origins",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#ovreall-point-forecast-performance-across-forecast-origins",
    "title": "",
    "section": "Ovreall point forecast performance across forecast origins",
    "text": "Ovreall point forecast performance across forecast origins\n\n\n\n\n\n\n\n\nFigure 2: Overall MASE accross forecast origins"
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#ovreall-point-forecast-performance-across-each-location",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#ovreall-point-forecast-performance-across-each-location",
    "title": "",
    "section": "Ovreall point forecast performance across each location",
    "text": "Ovreall point forecast performance across each location\n\n\n\n\n\n\n\n\nFigure 3: Overall MASE accross forecast origins for each location."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-8",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-8",
    "title": "",
    "section": "",
    "text": "Way forward\n\n Leverage CHAP/DHIS2 based models for supply chain data.\n\n Evaluate forecast perfromance based on time series structure, region and across products.\n\n Run order up-to-level based inventory simulations.\n\n Evaluate how forecast performance translate into inventory decisions."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-9",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#section-9",
    "title": "",
    "section": "",
    "text": "Key takeaways\n\n The Idea: Reusing morbidity models for supply chains is a promising but non-trivial opportunity.\n\n\n Our Contribution: We’re building an evidence-based, practical guide for when and how to repurpose morbidity models for consumption forecasting, leveraging CHAP’s modelling infrastructure and DHIS2-linked data.\n\n\n\n The Impact: By aligning forecasts with operational decisions, this work aims to improve stock availability, and inform sourcing/procurement decisions.\n\n\n\n Compatibility: CHAP/DHIS2’s external model interface makes it possible to integrate these forecasting tools seamlessly, no new platform required."
  },
  {
    "objectID": "talks/dhis2_webinar/slides/dhis2_webinar.html#any-questions-or-thoughts",
    "href": "talks/dhis2_webinar/slides/dhis2_webinar.html#any-questions-or-thoughts",
    "title": "",
    "section": "Any questions or thoughts? 💬",
    "text": "Any questions or thoughts? 💬"
  },
  {
    "objectID": "talks/lomsac_2026/index.html",
    "href": "talks/lomsac_2026/index.html",
    "title": "From Cases to Consumption: Evaluating Forecasting Model Portability for Public-Health Supply Chains",
    "section": "",
    "text": "From Cases to Consumption: Evaluating Forecasting Model Portability for Public-Health Supply Chains\n  Conference: LOM Section Annual Conference (LOMSAC) 2026, Cardiff Business School.\n  Harsha Halgamuwe Hewage | 2026-01-15\n  \n    \n       View Slides\n    \n  \n\n\n\n  Context\n\n  \n    This work is motivated by a persistent disconnect in public health analytics between\n    what is forecast and what is procured.\n    Platforms such as :contentReference[oaicite:0]{index=0} and the\n    :contentReference[oaicite:1]{index=1}\n    have substantially advanced the forecasting of disease incidence, often incorporating\n    climate and environmental signals to provide early warning of outbreaks.\n  \n\n  \n    Yet, operational decisions do not act on cases alone.\n    Health systems must translate predicted morbidity into product requirements such as\n    vaccines, antimalarials, and diagnostics.\n    At this point, forecasting performance often deteriorates.\n    Consumption data are incomplete, irregular, and frequently censored by stockouts,\n    reporting delays, and supply interruptions, making direct modelling of demand both\n    unreliable and inconsistent across settings.\n  \n\n  \n    This presentation examines forecasting model portability across this divide.\n    We ask whether models developed and validated for disease cases can be meaningfully\n    transferred to forecast health commodity consumption, and under what conditions this\n    transfer fails.\n    Using routine surveillance and logistics data, we compare three strategies:\n    direct reuse of case-based models, targeted fine-tuning using supply-chain covariates\n    such as campaign timing or climate signals, and full retraining on consumption data.\n  \n\n  \n    The contribution is practical rather than purely methodological.\n    We provide evidence on when portability is viable, when it introduces systematic bias,\n    and how forecasting choices propagate into inventory decisions.\n    By linking predictive performance to downstream ordering and stock availability,\n    the work supports a shift from reactive replenishment to anticipatory logistics.\n    This is particularly relevant for climate-sensitive diseases, where early warning only\n    delivers value if commodities arrive before demand materialises."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-2",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-2",
    "title": "Home",
    "section": "",
    "text": "Outline\n\n\n\nBackground\n\n\n\n\n\n\nThe fundamental question\n\n\n\n\n\n\nWhat we are going to do\n\n\n\n\n\n\nWhat did we find\n\n\n\n\n\n\nWhat NEXT?"
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-5",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-5",
    "title": "Home",
    "section": "",
    "text": "What is DHIS2 and CHAP?\n\n\n DHIS2 (District Health Information Software 2) is a free, open-source, web-based health information platform.\n\n\n\n It is the world’s largest Health Management Information System (HMIS), used by Ministries of Health in over 80 countries.\n\n\n\n CHAP stands for the Climate Health Analytics Platform.\n\n\n\n CHAP is a predictive modelling layer integrated within DHIS2, designed to support climate-informed disease forecasting and early warning."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-7",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-7",
    "title": "Home",
    "section": "",
    "text": "What we are going to do\n\n\n Apply the same CHAP morbidity models (no extra tuning) to product demand (malaria commodities, vaccines).\n\n\n\n Link forecasts to operations: evaluate both forecast accuracy and inventory impact.\n\n\n\n Build a portability matrix: when can we transfer a model?\n\n\n\n Design an implementation workflow for DHIS2/CHAP.\n\n\n\n Provide R scripts, CHAP YAML configs and sample datasets for full reproducibility."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#section-10",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#section-10",
    "title": "Home",
    "section": "",
    "text": "Way forward\n\n\n DHIS2 community webinar (90+ participants across industry, NGOs, and implementers)\n\n\n\n Technical briefing to the DHIS2 development team and country representatives\n\n\n\n Open release of code, YAML configurations, and deployable DHIS2/CHAP pipelines\n\n\n\n Planned hands-on practitioner demo using the existing DHIS2 ecosystem\n\n\n\n Co-delivered one-week, in-person forecasting training (Kigali, February)\n\n\n\n Long-term collaboration between DL4SG (Cardiff) and HISP (Oslo)"
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#data-sources",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#data-sources",
    "title": "Home",
    "section": "Data sources",
    "text": "Data sources\n\n\n\n\n\n\n\n\n\n\n\n\n\nData source\nDomain\nGeography / Sites\nCoverage period\nGranularity\n\n\n\n\nDHIS2\nDengue cases\nBrazil (23 locations)\n2001 Jan – 2017 Dec\nMonthly, subnational\n\n\n\n\nLaos (7 locations)\n2000 Jul – 2013 Jun\nMonthly, subnational\n\n\n\n\nParaguay (16 locations)\n2012 Mar – 2017 Dec\nMonthly, subnational\n\n\n\n\nVietnam (19 locations)\n2000 Jul – 2017 Jun\nMonthly, subnational\n\n\nDHIS2\nMalaria incidence\nRwanda (30 locations)\n2015 Jan – 2024 Dec\nMonthly, subnational\n\n\nDHIS2 / LMIS\nMalaria commodities\nLaos (11 products, 18 facilities)\n2019 Feb – 2021 Jun\nMonthly, subnational\n\n\nDHIS2 / LMIS\nVaccine consumption\nLaos (21 products, 10 facilities)\n2021 Feb – 2024 Jan\nMonthly, subnational\n\n\nDHIS2 Climate App\nClimate covariates\nAll study settings\nStudy-specific\nMonthly, subnational"
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-model-rankings-1",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-model-rankings-1",
    "title": "Home",
    "section": "Average forecast model rankings",
    "text": "Average forecast model rankings\n\n\nFigure 3: Average probabilistic forecast (quantile loss) ranks. Methods on the y-axis are ordered from best (top) to worst (bottom) based on their overall average rank across all domains. Lower ranks indicate better performance."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-method-rankings",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-method-rankings",
    "title": "Home",
    "section": "Average forecast method rankings",
    "text": "Average forecast method rankings\n\n\nFigure 2: Average point forecast (MASE) ranks. Methods on the y-axis are ordered from best (top) to worst (bottom) based on their overall average rank across all domains. Lower ranks indicate better performance."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-method-rankings-1",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#average-forecast-method-rankings-1",
    "title": "Home",
    "section": "Average forecast method rankings",
    "text": "Average forecast method rankings\n\n\nFigure 3: Average probabilistic forecast (quantile loss) ranks. Methods on the y-axis are ordered from best (top) to worst (bottom) based on their overall average rank across all domains. Lower ranks indicate better performance."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#overall-inventory-performance---malaria-products",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#overall-inventory-performance---malaria-products",
    "title": "Home",
    "section": "Overall inventory performance - Malaria products",
    "text": "Overall inventory performance - Malaria products\n\n\nFigure 4: Inventory performance across malaria-product simulations using quantile-based order-up-to policies (q80–q97.5)."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#overall-inventory-performance---vaccine",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#overall-inventory-performance---vaccine",
    "title": "Home",
    "section": "Overall inventory performance - Vaccine",
    "text": "Overall inventory performance - Vaccine\n\n\nFigure 5: Inventory performance across vaccine-product simulations using quantile-based order-up-to policies (q80–q97.5)."
  },
  {
    "objectID": "talks/lomsac_2026/slides/lomsac_2026.html#research-impact-and-engagement",
    "href": "talks/lomsac_2026/slides/lomsac_2026.html#research-impact-and-engagement",
    "title": "Home",
    "section": "Research impact and engagement",
    "text": "Research impact and engagement\n\n\n DHIS2 community webinar (90+ participants across industry, NGOs, and implementers)\n\n\n\n Technical briefing to the DHIS2 development team and country representatives\n\n\n\n Open release of code, YAML configurations, and deployable DHIS2/CHAP pipelines\n\n\n\n Planned hands-on practitioner demo using the existing DHIS2 ecosystem\n\n\n\n Co-delivered one-week, in-person forecasting training (Kigali, February)\n\n\n\n Long-term collaboration between DL4SG (Cardiff) and HISP (Oslo)"
  }
]